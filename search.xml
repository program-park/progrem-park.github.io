<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Python 爬虫基础之 urllib 库的深入使用详解</title>
      <link href="/2022/08/30/reptile_1/"/>
      <url>/2022/08/30/reptile_1/</url>
      
        <content type="html"><![CDATA[<h1 id="1-urllib库的使用"><a class="header-anchor" href="#1-urllib库的使用">¶</a>1. urllib库的使用</h1><h2 id="1-1-获取数据"><a class="header-anchor" href="#1-1-获取数据">¶</a>1.1 获取数据</h2><p>  爬虫的定义我这里就不多说了，直接进入正题，如何利用<code>urllib</code>库去使用爬虫。模拟浏览器向服务器发送请求：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://www.某du.com&#x27;</span>)</span><br></pre></td></tr></table></figure><p>  下面的方法能获取响应的页面源码，但是需要注意的是<code>read()</code>返回的是字节形式的二进制数据，所以需要<code>decode()</code>解码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure><p>  返回的<code>reponse</code>的类型为<code>HTTPResponse</code>，该返回值常用的方法如下：</p><ul><li><strong>read()</strong> ：字节形式读取二进制数据，<code>read(Number)</code>返回指定字节的数据；</li><li><strong>readline()</strong> ：读取一行；</li><li><strong>readlines()</strong> ：一行一行读取所有；</li><li><strong>getcode()</strong> ：获取响应状态码；</li><li><strong>geturl()</strong> ：获取请求的URL；</li><li><strong>getheaders()</strong> ：获取headers；</li></ul><h2 id="1-2-下载"><a class="header-anchor" href="#1-2-下载">¶</a>1.2 下载</h2><p>  除了获取数据，我们也能通过爬虫下载网页资源：</p><ul><li>下载网页：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.urlretrieve(<span class="string">&#x27;http://www.某du.com&#x27;</span>,<span class="string">&#x27;baidu.html&#x27;</span>)</span><br></pre></td></tr></table></figure></li><li>下载图片：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.urlretrieve(<span class="string">&#x27;https://xxxxx.cdn.bcebos.com/pic/fcfaaf51f3deb48f8f9616a7fd1f3a292cf578cf?x-bce-process=image/resize,m_lfit,h_500,limit_1/format,f_auto&#x27;</span>,<span class="string">&#x27;迪丽热巴.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure></li><li>下载视频：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.urlretrieve(<span class="string">&#x27;https://vd2.xxxxxxxx.com/mda-magpzuurpki42hbv/v1-cae/sc/mda-magpzuurpki42hbv.mp4?v_from_s=hkapp-haokan-hbe&amp;auth_key=1661150989-0-0-5951edef259d3b8ceac0327beca694ac&amp;bcevod_channel=searchbox_feed&amp;pd=1&amp;cd=0&amp;pt=3&amp;logid=1189836669&amp;vid=12518682849229018276&amp;abtest=103579_1-103742_4&amp;klogid=1189836669&#x27;</span>,<span class="string">&#x27;迪丽热巴.mp4&#x27;</span>)</span><br></pre></td></tr></table></figure></li></ul><h1 id="2-请求对象request的定制"><a class="header-anchor" href="#2-请求对象request的定制">¶</a>2. 请求对象request的定制</h1><blockquote><p><strong>User-Agent，中文名为用户代理，简称 UA，它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本，浏览器内核、浏览器渲染引擎、浏览器语言、浏览器插件等</strong></p></blockquote><p>  URL 的组成有以下几个部分：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">http/httpswwww.某du.com80/443swd=迪丽热巴#</span><br><span class="line">协议主机端口号路径参数锚点</span><br></pre></td></tr></table></figure><p>  上面的例子我们是请求的<code>http</code>接口，能够正常爬取，但是请求<code>https</code>某度网址，会遇到<code>UA</code>反爬，也是很常见的一种反爬机制：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">url = <span class="string">&#x27;https://www.某du.com&#x27;</span></span><br><span class="line">response = urllib.request.urlopen(url)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/61ec838df1da40768026e08d52e440f9.png#pic_center" alt="在这里插入图片描述"></p><p>  所以在发送请求时，要带上我们的<code>UA</code>，<code>UA</code>的获取方法如下：</p><p><img src="https://img-blog.csdnimg.cn/543aca0fee964b35803f1d67dcc9d123.png#pic_center" alt="在这里插入图片描述"><br>  带上<code>UA</code>爬虫代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">&#x27;https://www.某du.com&#x27;</span></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure><p>  因为<code>urlopen()</code>方法不支持字典变量，所以我们可以将信息放入定制的<code>Request</code>对象中，再将<code>Request</code>对象放入<code>urlopen()</code>中。这里需要注意的是，<code>Request()</code>方法的参数必须使用关键字传参，查看<code>Request</code>源码，可以发现，第二个参数是<code>data=None</code>，所以不指定的话，默认会认为传入的<code>UA</code>是<code>data</code>参数：</p><p><img src="https://img-blog.csdnimg.cn/7fa42a5a989f440e87de55c361dabd7c.png#pic_center" alt="在这里插入图片描述"></p><h1 id="3-编解码"><a class="header-anchor" href="#3-编解码">¶</a>3. 编解码</h1><blockquote><p><strong>编码集的演变：由于计算机是美国人发明的，因此，最早只有127个字符被编码到计算机里，也就是大小写英文字母、数字和一些符号，这个编码表被称为ASCII编码，比如大写字母A的编码是65，小写字母z的编码是122。但是要处理中文显然一个字节是不够的，至少需要两个字节，而且还不能和ASCII编码冲突，所以，中国制定了GB2312编码，用来把中文编进去。你可以想得到的是，全世界有上百种语言，日本把日文编到Shift_JIS里，韩国把韩文编到Euc‐kr里，各国有各国的标准，就会不可避免地出现冲突，结果就是，在多语言混合的文本中，显示出来会有乱码。<br>因此，Unicode应运而生。Unicode把所有语言都统一到一套编码里，这样就不会再有乱码问题了。Unicode标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要4个字节）。现代操作系统和大多数编程语言都直接支持Unicode。</strong></p></blockquote><h2 id="3-1-get请求"><a class="header-anchor" href="#3-1-get请求">¶</a>3.1 get请求</h2><h3 id="3-1-1-quote"><a class="header-anchor" href="#3-1-1-quote">¶</a>3.1.1 quote()</h3><p>  上面说了 URL 的组成和<code>UA</code>的作用，那么现在有一个需求：获取<code>https://www.某du.com/s?wd=迪丽热巴</code>的网页源码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">&#x27;https://www.某du.com/s?wd=迪丽热巴&#x27;</span></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure><p>  直接运行上面的代码，会发现报错：</p><p><img src="https://img-blog.csdnimg.cn/3c0a4978b000456f924614abf9b54397.png#pic_center" alt="在这里插入图片描述"><br>  这是因为编码的问题，我们需要将<code>迪丽热巴</code>四个字变成<code>unicode</code>编码的格式，而这需要依赖<code>urllib.parse</code>的<code>quote()</code>方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">name = urllib.parse.quote(<span class="string">&#x27;迪丽热巴&#x27;</span>)</span><br><span class="line">url = <span class="string">&#x27;https://www.某du.com/s?wd=&#x27;</span> + name</span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure><h3 id="3-1-2-urlencode"><a class="header-anchor" href="#3-1-2-urlencode">¶</a>3.1.2 urlencode()</h3><p>  既然有将一个参数变成<code>unicode</code>编码格式的方法，那么肯定有将多个参数同时变为<code>unicode</code>编码格式的方法，毕竟我们平时用的接口不可能只有一个参数，这个方法就是<code>urlencode()</code>，它的参数类型是字典，应用场景：URL 同时有多个参数时，将多个参数变成<code>unicode</code>编码的格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;wd&#x27;</span> : <span class="string">&#x27;迪丽热巴&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sex&#x27;</span> : <span class="string">&#x27;女&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">a = urllib.parse.urlencode(data)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure><p>  运行结果如下，将多个参数同时转为了<code>unicode</code>编码格式并用<code>&amp;</code>分割：</p><p><img src="https://img-blog.csdnimg.cn/1cf0a9b4c89f41a0af9fd79d688b86b3.png#pic_center" alt="在这里插入图片描述"></p><h2 id="3-2-post请求"><a class="header-anchor" href="#3-2-post请求">¶</a>3.2 post请求</h2><p>  上面说的案例都是<code>get</code>请求接口，但其实我们遇到的更多的还是<code>post</code>请求接口，在请求时要带上参数值，以某度翻译为例，找到发送单词请求数据的 URL：</p><p><img src="https://img-blog.csdnimg.cn/8ee72c85d4b84760adf3b9f6b93ea64d.png#pic_center" alt="在这里插入图片描述"><br>  请求地址为<code>https://fanyi.某du.com/sug</code>，且以<code>post</code>方式发送。携带的参数为<code>&#123;kw:copy&#125;</code>，返回的数据为一组<code>json</code>数据，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">url = <span class="string">&#x27;https://fanyi.某du.com/sug&#x27;</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;kw&#x27;</span> : <span class="string">&#x27;copy&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># post请求的参数必须进行编码，编码后必须调用encode()</span></span><br><span class="line">data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">request = urllib.request.Request(url=url, data=data, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">obj = json.loads(content)</span><br><span class="line"><span class="built_in">print</span>(obj)</span><br></pre></td></tr></table></figure><p>  <strong>注意：</strong></p><ul><li><code>get</code>请求方式的参数必须编码，参数是拼接到 URL 后面，编码之后不需要调用<code>encode()</code>方法；</li><li><code>post</code>请求方式的参数必须编码，参数是放在请求对象定制的方法中，编码之后需要调用<code>encode()</code>方法；</li></ul><h2 id="3-3-案例：某度详细翻译"><a class="header-anchor" href="#3-3-案例：某度详细翻译">¶</a>3.3 案例：某度详细翻译</h2><p>  前面的案例用过了某度翻译的<code>post</code>的接口，但其实某度翻译还有一个某度详细翻译的接口，返回的数据更加详细：</p><p><img src="https://img-blog.csdnimg.cn/8dce122e647c402f944de43e30ac1218.png#pic_center" alt="在这里插入图片描述"><br>  该接口需要的参数如下：</p><p><img src="https://img-blog.csdnimg.cn/ce1b08d764d8456d894ee209edea159b.png#pic_center" alt="在这里插入图片描述"><br>  爬虫代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">url = <span class="string">&#x27;https://fanyi.某du.com/v2transapi?from=en&amp;to=zh&#x27;</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;from&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;to&#x27;</span>: <span class="string">&#x27;zh&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;query&#x27;</span>: <span class="string">&#x27;copy&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;simple_means_flag&#x27;</span>: <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sign&#x27;</span>: <span class="string">&#x27;479144.240793&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;token&#x27;</span>: <span class="string">&#x27;68566c3678a1d2d18424ef21037ad1cd&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;domain&#x27;</span>: <span class="string">&#x27;common&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># post请求的参数必须进行编码，编码后必须调用encode()</span></span><br><span class="line">data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment"># 请求对象的定制</span></span><br><span class="line">request = urllib.request.Request(url=url, data=data, headers=headers)</span><br><span class="line"><span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"><span class="comment"># 获取响应的数据</span></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">obj = json.loads(content)</span><br><span class="line"><span class="built_in">print</span>(obj)</span><br></pre></td></tr></table></figure><p>  运行代码，遇到报错：</p><p><img src="https://img-blog.csdnimg.cn/2211079b1ba8467781063cf59c13d1ed.png#pic_center" alt="在这里插入图片描述"><br>  这是因为请求头中的数据没有全部放到<code>headers</code>中：</p><p><img src="https://img-blog.csdnimg.cn/ee3b495d8f2a42309f06deab8ae57c22.png#pic_center" alt="在这里插入图片描述"><br>  将请求头的数据全部放入<code>headers</code>即可，这里我使用的<code>Sublime Text</code>修改的数据格式，否则手动改还是有些费事：</p><p><img src="https://img-blog.csdnimg.cn/3b2de1a4a30045299ac3148938eae1a3.png#pic_center" alt="在这里插入图片描述"><br>  加入请求头数据后的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">url = <span class="string">&#x27;https://fanyi.某du.com/v2transapi?from=en&amp;to=zh&#x27;</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;from&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;to&#x27;</span>: <span class="string">&#x27;zh&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;query&#x27;</span>: <span class="string">&#x27;copy&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;simple_means_flag&#x27;</span>: <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sign&#x27;</span>: <span class="string">&#x27;479144.240793&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;token&#x27;</span>: <span class="string">&#x27;68566c3678a1d2d18424ef21037ad1cd&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;domain&#x27;</span>: <span class="string">&#x27;common&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;Accept&#x27;</span>:<span class="string">&#x27;*/*&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Accept-Encoding&#x27;</span>:<span class="string">&#x27;gzip, deflate, br&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Accept-Language&#x27;</span>:<span class="string">&#x27;zh-CN,zh;q=0.9&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Acs-Token&#x27;</span>:<span class="string">&#x27;1661151781547_1661235691501_0f9CZMIZ5F98s+p/e4nt/8nbrZ+AlxwEaULsxFVtWLyXY5q1G9TLC9A94gcj+3pNbT9maF1j+iqywD1ReX5S2xJ4XVhHgh2AoNNoG3kKPIvkKt1xg5LC/q465CS7LldtxQiHZH7aVD3hI98PAkb/BP/WJL94eJnMUR5z3TWWHWHznmhHc3DBZphXD7bvfieuSSSTrSfQJKh1gZAAED7Qd+FkTJPyTmx71HOYgDoKXgL8at5CVbstnRlGybS6HKSaJ+q9fAy8R9pup4Op9+6xuNh2sFR9axAE+92cP8gcQLqUd7B745Bv7Bbjv1odwUqbFJUcsLMyu1qH0yi46MQlbQ==&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Connection&#x27;</span>:<span class="string">&#x27;keep-alive&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Content-Length&#x27;</span>:<span class="string">&#x27;116&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Content-Type&#x27;</span>:<span class="string">&#x27;application/x-www-form-urlencoded; charset=UTF-8&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Cookie&#x27;</span>:<span class="string">&#x27;BIDUPSID=EDEDD8AC90775AB7BB2E2A9BA887CB6A; PSTM=1643847626; BAIDUID=EDEDD8AC90775AB76211B82417CB627A:FG=1; __yjs_duid=1_3f010fc33690405f6d48831d8577999f1644021279727; FANYI_WORD_SWITCH=1; REALTIME_TRANS_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; APPGUIDE_10_0_2=1; BDUSS=VhuaU9TRWVWZmY2ZjhvTURUWkxsYnV6VGNrdDhuVnpLeGdiU3BWazZlYk5RQmRqRVFBQUFBJCQAAAAAAAAAAAEAAAA5xXOd0KHQob70z8LKvwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM2z72LNs-9iV; BDUSS_BFESS=VhuaU9TRWVWZmY2ZjhvTURUWkxsYnV6VGNrdDhuVnpLeGdiU3BWazZlYk5RQmRqRVFBQUFBJCQAAAAAAAAAAAEAAAA5xXOd0KHQob70z8LKvwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM2z72LNs-9iV; ZFY=fiXfZrdJz5x8NkMm:AiKmd1ufNDjZkiFrzNjDBw7nRpY:C; BAIDUID_BFESS=EDEDD8AC90775AB76211B82417CB627A:FG=1; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; ariaDefaultTheme=undefined; RT=&quot;z=1&amp;dm=baidu.com&amp;si=1ehud9ig578&amp;ss=l74cz1z6&amp;sl=k&amp;tt=co6&amp;bcn=https%3A%2F%2Ffclog.baidu.com%2Flog%2Fweirwood%3Ftype%3Dperf&amp;ld=d781&amp;ul=f7iu&amp;hd=f7jy&quot;; BA_HECTOR=2l2kal8180842k0k842ga5kc1hg8dvk16; BDRCVFR[feWj1Vr5u3D]=I67x6TjHwwYf0; delPer=0; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1660717558,1660788138,1661134918,1661223741; PSINO=1; H_PS_PSSID=36559_36463_36641_36982_36885_34812_36917_36779_37137_26350_37089_37194; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1661235691&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Host&#x27;</span>:<span class="string">&#x27;fanyi.baidu.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Origin&#x27;</span>:<span class="string">&#x27;https://fanyi.baidu.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>:<span class="string">&#x27;https://fanyi.baidu.com/?aldtype=16047&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua&#x27;</span>:<span class="string">&#x27;&quot;Chromium&quot;;v=&quot;104&quot;, &quot; Not A;Brand&quot;;v=&quot;99&quot;, &quot;Google Chrome&quot;;v=&quot;104&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-mobile&#x27;</span>:<span class="string">&#x27;?0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-platform&#x27;</span>:<span class="string">&#x27;&quot;Windows&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Dest&#x27;</span>:<span class="string">&#x27;empty&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Mode&#x27;</span>:<span class="string">&#x27;cors&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Site&#x27;</span>:<span class="string">&#x27;same-origin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;X-Requested-With&#x27;</span>:<span class="string">&#x27;XMLHttpRequest&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># post请求的参数必须进行编码，编码后必须调用encode()</span></span><br><span class="line">data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment"># 请求对象的定制</span></span><br><span class="line">request = urllib.request.Request(url=url, data=data, headers=headers)</span><br><span class="line"><span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"><span class="comment"># 获取响应的数据</span></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">obj = json.loads(content)</span><br><span class="line"><span class="built_in">print</span>(obj)</span><br></pre></td></tr></table></figure><p>  执行后发现还是报错：</p><p><img src="https://img-blog.csdnimg.cn/2622c1e1a3d24e229ca9ffb4ee1b5c51.png#pic_center" alt="在这里插入图片描述"><br>  这是因为接收的编码格式不支持<code>utf-8</code>，将<code>'Accept-Encoding':'gzip, deflate, br'</code>注释即可，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">url = <span class="string">&#x27;https://fanyi.某du.com/v2transapi?from=en&amp;to=zh&#x27;</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;from&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;to&#x27;</span>: <span class="string">&#x27;zh&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;query&#x27;</span>: <span class="string">&#x27;copy&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;simple_means_flag&#x27;</span>: <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sign&#x27;</span>: <span class="string">&#x27;479144.240793&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;token&#x27;</span>: <span class="string">&#x27;68566c3678a1d2d18424ef21037ad1cd&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;domain&#x27;</span>: <span class="string">&#x27;common&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;Accept&#x27;</span>:<span class="string">&#x27;*/*&#x27;</span>,</span><br><span class="line">    <span class="comment"># &#x27;Accept-Encoding&#x27;:&#x27;gzip, deflate, br&#x27;,</span></span><br><span class="line">    <span class="string">&#x27;Accept-Language&#x27;</span>:<span class="string">&#x27;zh-CN,zh;q=0.9&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Acs-Token&#x27;</span>:<span class="string">&#x27;1661151781547_1661235691501_0f9CZMIZ5F98s+p/e4nt/8nbrZ+AlxwEaULsxFVtWLyXY5q1G9TLC9A94gcj+3pNbT9maF1j+iqywD1ReX5S2xJ4XVhHgh2AoNNoG3kKPIvkKt1xg5LC/q465CS7LldtxQiHZH7aVD3hI98PAkb/BP/WJL94eJnMUR5z3TWWHWHznmhHc3DBZphXD7bvfieuSSSTrSfQJKh1gZAAED7Qd+FkTJPyTmx71HOYgDoKXgL8at5CVbstnRlGybS6HKSaJ+q9fAy8R9pup4Op9+6xuNh2sFR9axAE+92cP8gcQLqUd7B745Bv7Bbjv1odwUqbFJUcsLMyu1qH0yi46MQlbQ==&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Connection&#x27;</span>:<span class="string">&#x27;keep-alive&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Content-Length&#x27;</span>:<span class="string">&#x27;116&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Content-Type&#x27;</span>:<span class="string">&#x27;application/x-www-form-urlencoded; charset=UTF-8&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Cookie&#x27;</span>:<span class="string">&#x27;BIDUPSID=EDEDD8AC90775AB7BB2E2A9BA887CB6A; PSTM=1643847626; BAIDUID=EDEDD8AC90775AB76211B82417CB627A:FG=1; __yjs_duid=1_3f010fc33690405f6d48831d8577999f1644021279727; FANYI_WORD_SWITCH=1; REALTIME_TRANS_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; APPGUIDE_10_0_2=1; BDUSS=VhuaU9TRWVWZmY2ZjhvTURUWkxsYnV6VGNrdDhuVnpLeGdiU3BWazZlYk5RQmRqRVFBQUFBJCQAAAAAAAAAAAEAAAA5xXOd0KHQob70z8LKvwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM2z72LNs-9iV; BDUSS_BFESS=VhuaU9TRWVWZmY2ZjhvTURUWkxsYnV6VGNrdDhuVnpLeGdiU3BWazZlYk5RQmRqRVFBQUFBJCQAAAAAAAAAAAEAAAA5xXOd0KHQob70z8LKvwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM2z72LNs-9iV; ZFY=fiXfZrdJz5x8NkMm:AiKmd1ufNDjZkiFrzNjDBw7nRpY:C; BAIDUID_BFESS=EDEDD8AC90775AB76211B82417CB627A:FG=1; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; ariaDefaultTheme=undefined; RT=&quot;z=1&amp;dm=baidu.com&amp;si=1ehud9ig578&amp;ss=l74cz1z6&amp;sl=k&amp;tt=co6&amp;bcn=https%3A%2F%2Ffclog.baidu.com%2Flog%2Fweirwood%3Ftype%3Dperf&amp;ld=d781&amp;ul=f7iu&amp;hd=f7jy&quot;; BA_HECTOR=2l2kal8180842k0k842ga5kc1hg8dvk16; BDRCVFR[feWj1Vr5u3D]=I67x6TjHwwYf0; delPer=0; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1660717558,1660788138,1661134918,1661223741; PSINO=1; H_PS_PSSID=36559_36463_36641_36982_36885_34812_36917_36779_37137_26350_37089_37194; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1661235691&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Host&#x27;</span>:<span class="string">&#x27;fanyi.baidu.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Origin&#x27;</span>:<span class="string">&#x27;https://fanyi.baidu.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>:<span class="string">&#x27;https://fanyi.baidu.com/?aldtype=16047&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua&#x27;</span>:<span class="string">&#x27;&quot;Chromium&quot;;v=&quot;104&quot;, &quot; Not A;Brand&quot;;v=&quot;99&quot;, &quot;Google Chrome&quot;;v=&quot;104&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-mobile&#x27;</span>:<span class="string">&#x27;?0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-platform&#x27;</span>:<span class="string">&#x27;&quot;Windows&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Dest&#x27;</span>:<span class="string">&#x27;empty&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Mode&#x27;</span>:<span class="string">&#x27;cors&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Site&#x27;</span>:<span class="string">&#x27;same-origin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;X-Requested-With&#x27;</span>:<span class="string">&#x27;XMLHttpRequest&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># post请求的参数必须进行编码，编码后必须调用encode()</span></span><br><span class="line">data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment"># 请求对象的定制</span></span><br><span class="line">request = urllib.request.Request(url=url, data=data, headers=headers)</span><br><span class="line"><span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"><span class="comment"># 获取响应的数据</span></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">obj = json.loads(content)</span><br><span class="line"><span class="built_in">print</span>(obj)</span><br></pre></td></tr></table></figure><p>  需要注意的是，在整个请求标头中，最重要的就是<code>cookie</code>，事实上将请求标头中除去<code>cookie</code>外所有的行全部注释，爬虫也能正常运行。</p><blockquote><p><strong>cookie 一般是登录后产生（post），用来保持登录状态的，一般登录一次，下一次访问该网站下的其他网址时就不需要登录了，这就是由于cookie的作用。cookie 就是给无状态的 HTTP/HTTPS 协议添加了一种保持之前状态的功能，这样下次处理信息的时候就不用重新获取信息了。<br>cookie 还可以来判断是否是爬虫程序，因为一般的爬虫程序中并不会携带 cookie，有些比较严格的网站，不登录也需要携带 cookie 访问，也就是说 cookie 的应用场景并不仅仅只有登录后才需要。</strong></p></blockquote><h1 id="4-Ajax"><a class="header-anchor" href="#4-Ajax">¶</a>4. Ajax</h1><blockquote><p><strong>Ajax 即 “Asynchronous Javascript And XML”（异步 JavaScript 和 XML），是指⼀种创建交互式网页应用的网页开发技术。用于创建快速动态网页，在无需重新加载整个网页的情况下，能够更新部分网页的技术。</strong><br><strong>通过在后台与服务器进行少量数据交换，Ajax 可以使网页实现异步更新，这意味着可以在不重新加载整个网页的情况下，对网页的某个部分进行更新，而传统的网页（不使用 Ajax）如果需要更新内容，必须重载整个页面。</strong></p></blockquote><h2 id="4-1-Ajax的get请求"><a class="header-anchor" href="#4-1-Ajax的get请求">¶</a>4.1 Ajax的get请求</h2><p>  以爬取某瓣网电影前十页数据为案例，某瓣网的分页就是用<code>Ajax</code>实现的。先完成一个小目标，爬取第一页数据，找到某瓣网数据接口：</p><p><img src="https://img-blog.csdnimg.cn/0709326ec6a24a4fb6c10f402c8024b1.png#pic_center" alt="在这里插入图片描述"><br>  爬取首页数据并写入到文件中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">&#x27;https://movie.某ban.com/j/chart/top_list?type=20&amp;interval_id=100%3A90&amp;action=&amp;start=0&amp;limit=20&#x27;</span></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">file = <span class="built_in">open</span>(<span class="string">&#x27;douban.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">file.write(content)</span><br></pre></td></tr></table></figure><p>  查看文件内容，快捷键<code>ctrl+alt+L</code>自动对齐，已爬取到第一页的数据：</p><p><img src="https://img-blog.csdnimg.cn/f7e72165a07746de9d36ea5a3a993a59.png#pic_center" alt="在这里插入图片描述"><br>  下面就是爬取前十页的数据了。通过观察，找到下面三个接口，这三个接口是豆瓣网电影前三页的接口，可以明显看到这三个接口的规律，接口是根据<code>start=(page-1)*20</code>来区分页数的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://movie.某ban.com/j/chart/top_list?type=20&amp;interval_id=100%3A90&amp;action=&amp;start=0&amp;limit=20</span><br><span class="line">https://movie.某ban.com/j/chart/top_list?type=20&amp;interval_id=100%3A90&amp;action=&amp;start=20&amp;limit=20</span><br><span class="line">https://movie.某ban.com/j/chart/top_list?type=20&amp;interval_id=100%3A90&amp;action=&amp;start=40&amp;limit=20</span><br></pre></td></tr></table></figure><p>  知道了接口规律，那么接下来就是爬取前十页数据了，代码已封装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回request对象</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_request</span>(<span class="params">page</span>):</span><br><span class="line">    base_url = <span class="string">&#x27;https://movie.某ban.com/j/chart/top_list?type=20&amp;interval_id=100%3A90&amp;action=&amp;&#x27;</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">&#x27;start&#x27;</span>: (page - <span class="number">1</span>) * <span class="number">20</span>,</span><br><span class="line">        <span class="string">&#x27;limit&#x27;</span>: <span class="number">20</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 将data转换为unicode编码格式</span></span><br><span class="line">    data = urllib.parse.urlencode(data)</span><br><span class="line">    url = base_url + data</span><br><span class="line">    <span class="comment"># 定制request</span></span><br><span class="line">    request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取响应数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">request</span>):</span><br><span class="line">    <span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存文件到本地</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_load</span>(<span class="params">page, content</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;douban_&#x27;</span> + <span class="built_in">str</span>(page) + <span class="string">&#x27;.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span>  file:</span><br><span class="line">        file.write(content)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入起始页码：&#x27;</span>))</span><br><span class="line">    end_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入结束页码：&#x27;</span>))</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start_page, end_page + <span class="number">1</span>):</span><br><span class="line">        request = create_request(page)</span><br><span class="line">        content = get_content(request)</span><br><span class="line">        down_load(page, content)</span><br></pre></td></tr></table></figure><p>  运行代码可以得到对应页面的数据文件，效果我就不放了。</p><h2 id="4-2-Ajax的post请求"><a class="header-anchor" href="#4-2-Ajax的post请求">¶</a>4.2 Ajax的post请求</h2><p>  下面以爬取某德基餐厅信息的案例来演示，如何爬取<code>Ajax</code>的<code>post</code>请求。先登录某德基官网，找到对应的<code>post</code>接口。（图片违规，不让我放上来。。。。。。。）<br>  然后通过对比前三页接口的不同，找到不同页码接口之间的规律：</p><p><img src="https://img-blog.csdnimg.cn/8640788300ea4e79a414fe24d307b957.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/4199a65f97234d908ac5716dcf8165d9.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/eaa886b1d3ab4a24bc437d84336f4de0.png#pic_center" alt="在这里插入图片描述"><br>  可以明显发现，该接口是通过<code>pageIndex</code>参数控制页码，<code>pageSize</code>控制每页的数量，<code>cname</code>控制地区。下面就直接放代码了，难点不多：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回request对象</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_request</span>(<span class="params">page</span>):</span><br><span class="line">    base_url = <span class="string">&#x27;http://www.某fc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname&#x27;</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">&#x27;cname&#x27;</span>: <span class="string">&#x27;北京&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pid&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pageIndex&#x27;</span>: page,</span><br><span class="line">        <span class="string">&#x27;pageSize&#x27;</span>: <span class="string">&#x27;10&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 将data转换为unicode编码格式，post接口需要用encode()编码</span></span><br><span class="line">    data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="comment"># 定制请求对象</span></span><br><span class="line">    request = urllib.request.Request(url=base_url, data=data, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取响应数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">request</span>):</span><br><span class="line">    <span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存文件到本地</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_load</span>(<span class="params">page, content</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;某fc_&#x27;</span> + <span class="built_in">str</span>(page) + <span class="string">&#x27;.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        file.write(content)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入起始页码：&#x27;</span>))</span><br><span class="line">    end_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入结束页码：&#x27;</span>))</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start_page, end_page + <span class="number">1</span>):</span><br><span class="line">        request = create_request(page)</span><br><span class="line">        content = get_content(request)</span><br><span class="line">        down_load(page, content)</span><br></pre></td></tr></table></figure><h1 id="5-捕获异常"><a class="header-anchor" href="#5-捕获异常">¶</a>5. 捕获异常</h1><p>  如果我们想让自己的爬虫代码更加健壮的话，也是需要捕获异常的，这里就介绍两个异常类，<code>HTTPError</code>和<code>URLError</code>，<code>HTTPError</code>类是<code>URLError</code>类的子类。<br>  当然实际平时爬虫的使用中，捕获异常使用的并不多，所以我这里就不做过多的介绍了，直接拿上一个某德基的代码改一下放这里：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回request对象</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_request</span>(<span class="params">page</span>):</span><br><span class="line">    base_url = <span class="string">&#x27;http://www.某fc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname&#x27;</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">&#x27;cname&#x27;</span>: <span class="string">&#x27;北京&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pid&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pageIndex&#x27;</span>: page,</span><br><span class="line">        <span class="string">&#x27;pageSize&#x27;</span>: <span class="string">&#x27;10&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 将data转换为unicode编码格式，post接口需要用encode()编码</span></span><br><span class="line">    data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="comment"># 定制请求对象</span></span><br><span class="line">    request = urllib.request.Request(url=base_url, data=data, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取响应数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">request</span>):</span><br><span class="line">    <span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存文件到本地</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_load</span>(<span class="params">page, content</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;某fc_&#x27;</span> + <span class="built_in">str</span>(page) + <span class="string">&#x27;.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        file.write(content)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        start_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入起始页码：&#x27;</span>))</span><br><span class="line">        end_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入结束页码：&#x27;</span>))</span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start_page, end_page + <span class="number">1</span>):</span><br><span class="line">            request = create_request(page)</span><br><span class="line">            content = get_content(request)</span><br><span class="line">            down_load(page, content)</span><br><span class="line">    <span class="keyword">except</span> urllib.error.HTTPError:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;系统正在升级&#x27;</span>)</span><br><span class="line">    <span class="keyword">except</span> urllib.error.URLError:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;您访问的域名不存在&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="6-cookie登录"><a class="header-anchor" href="#6-cookie登录">¶</a>6. cookie登录</h1><p>  在很多网站中，和上面案例中的网站有一个不一样的地方，就是未登录状态和登录状态所展示出来的数据是不一样的，这里就讲一下，如果想采集登录之后才展示出来的数据，应该怎么去写代码。这里就以某博<code>https://某bo.cn/</code>为例，采集个人主页的信息，首先按照上面讲过的代码进行尝试（因为涉及个人隐私，所以代码中的链接 uid 和后续的 cookie 我都会做随机修改处理，各位以自己的为准即可）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://某bo.cn/6328349562/info&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure><p>  运行代码会发现编码不对：</p><p><img src="https://img-blog.csdnimg.cn/895ef61125804162b2917a0b79b1e2a8.png#pic_center" alt="在这里插入图片描述"><br>  而查看个人信息网页的源代码，编码格式确实是<code>utf-8</code>，之所以编码不对是因为我们没有登录信息，请求个人信息主页时会自动跳转到登录页面，而登录页面的编码格式却是<code>gb2312</code>，这也是一个小的爬虫手段，所以我们只需要改一下编码格式为<code>gb2312</code>即可。可是随之而来的问题是，我们这样获取的是登录页面的信息，而我们实际想要的是个人信息主页，应该怎么去绕过登录页面直接到达个人信息主页呢，这就需要一个重要的东西，就是<code>cookie</code>，在个人信息主页，打开检查，点击网络的<code>info</code>，将请求头的东西都复制出来：</p><p><img src="https://img-blog.csdnimg.cn/fb2d4676c7854df5851b88ca9dd27fef.jpeg#pic_center" alt="在这里插入图片描述"><br>  前面带有冒号的可以不要，放入代码的<code>headers</code>中，可以用前面说过的正则方法，方便快捷。注意编码格式需要改回<code>utf-8</code>，因为这里不需要再去登录页了，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://某bo.cn/6347449799/info&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;accept&#x27;</span>:<span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9&#x27;</span>,</span><br><span class="line">    <span class="comment"># &#x27;accept-encoding&#x27;:&#x27;gzip, deflate, br&#x27;,</span></span><br><span class="line">    <span class="string">&#x27;accept-language&#x27;</span>:<span class="string">&#x27;zh-CN,zh;q=0.9&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;cache-control&#x27;</span>:<span class="string">&#x27;max-age=0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;cookie&#x27;</span>:<span class="string">&#x27;_T_WM=f1c3ebd01cf7dde23704b362ixs82v8q; SUB=_2A25OA26gDeRhGeBN71UV9CfLwjWIHXVtDHLorDV6PUNbktANLUjfkmdQ7XiRIiAWGcim-RPUJFDBLv9yPaBCUSFB; SUBP=0033WrSXqPxfM725Ws9jqgMF26451P9D9WFzL6FUia2PJzLiF9NnWlEC5JpX5KzhUgL.Foq0ShNESD.N2K.2dKCoIE7LxK-L1KBLB-qLxKBLB.BLBKWaUJYLxKBLBonL12BLxKqL1MDUVG6Efeh2centt; SSOLoginState=1661038641; ALF=1664046284; MLOGIN=1; M_WEIBOCN_PARAMS=luicode%3D20084629&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;referer&#x27;</span>:<span class="string">&#x27;https://weibo.cn/&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua&#x27;</span>:<span class="string">&#x27;&quot;Chromium&quot;;v=&quot;104&quot;, &quot; Not A;Brand&quot;;v=&quot;99&quot;, &quot;Google Chrome&quot;;v=&quot;104&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-mobile&#x27;</span>:<span class="string">&#x27;?0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-platform&#x27;</span>:<span class="string">&#x27;&quot;Windows&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-dest&#x27;</span>:<span class="string">&#x27;document&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-mode&#x27;</span>:<span class="string">&#x27;navigate&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-site&#x27;</span>:<span class="string">&#x27;same-origin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-user&#x27;</span>:<span class="string">&#x27;?1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;upgrade-insecure-requests&#x27;</span>:<span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;user-agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure><p>  如此便可获取到个人主页的信息，效果图我就不放了，懒。</p><h1 id="7-Handler处理器"><a class="header-anchor" href="#7-Handler处理器">¶</a>7. Handler处理器</h1><p>  随着业务逻辑逐渐复杂，请求对象的定制已经满足不了我们的需求，长期爬取一个网站，会被网站识别为爬虫，对我们的设备进行封 IP 操作，所以我们就需要动态<code>cookie</code>和代理来解决这个问题，而代理就是以<code>Handler</code>为基础，这里还是先用百度的简单案例，讲一下如何使用<code>Handler</code>处理器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.某du.com&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line"><span class="comment"># 获取handler对象</span></span><br><span class="line">handler = urllib.request.HTTPHandler()</span><br><span class="line"><span class="comment"># 获取opener对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"><span class="comment"># 调用open方法</span></span><br><span class="line">response = opener.<span class="built_in">open</span>(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure><h1 id="8-代理的基本使用"><a class="header-anchor" href="#8-代理的基本使用">¶</a>8. 代理的基本使用</h1><p>  当我们使用爬虫长期、高频的访问某个网站，就会被识别为爬虫，封禁设备 IP，从而导致无法访问该网站，而代理，就是为了隐藏我们的真实 IP，防止被封禁。当然代理不仅仅是这一个用途，像我们打游戏时开的加速器就是代理，访问外网的资源也需要使用代理，以及一些私密单位的内部资源都需要代理来进行访问，代理也能用来提高访问速度，通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将其保存到缓冲区中，当其他用户再访问相同的信息时， 则直接由缓冲区中取出信息，传给用户，以提高访问速度。<br>  那么下面我就来演示一下，如何用代理爬取网站信息。首先我们上某度网站，某度<code>ip</code>，可以直接显示出我们的本机<code>ip</code>：</p><p><img src="https://img-blog.csdnimg.cn/59b4986709cc4f889549bc04de53d276.png#pic_center" alt="在这里插入图片描述"><br>  我们先使用之前请求对象定制的方法，来爬取该网站信息看是否能正常获取到：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.某du.com/s?wd=ip&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;代理.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(content)</span><br></pre></td></tr></table></figure><p>  打开<code>代理.html</code>文件，可以看到和我们正常访问一模一样。<br>  然后换成<code>Hanlder</code>处理器，使用代理访问，我这里用的是<code>快代理</code>网站的免费代理，连接不太稳定，可能换几个代理才有一个能用的：</p><p><img src="https://img-blog.csdnimg.cn/6f487c24ae1546b586f9a265b5078166.png#pic_center" alt="在这里插入图片描述"><br>  代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.某du.com/s?wd=ip&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line"><span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line"><span class="comment"># response = urllib.request.urlopen(request)</span></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;223.82.60.202:8060&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 获取handler对象</span></span><br><span class="line">handler = urllib.request.ProxyHandler(proxies=proxies)</span><br><span class="line"><span class="comment"># 获取opener对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"><span class="comment"># 调用open方法</span></span><br><span class="line">response = opener.<span class="built_in">open</span>(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;代理.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(content)</span><br></pre></td></tr></table></figure><p>  效果图如下：</p><p><img src="https://img-blog.csdnimg.cn/5cac1e1a3da04b3aafeeb83aea0b86a6.png#pic_center" alt="在这里插入图片描述"><br>  当然在我们实际生产中，只用一个代理也是不现实的，也会面临被封<code>ip</code>的风险，所有就有了<code>代理池</code>的概念。</p><blockquote><p><strong>代理池就是由代理IP组成的池子, 它可以提供多个稳定可用的代理IP。</strong></p></blockquote><p>  我们在做爬虫的时候, 最常见一种反爬手段就是<code>ip</code>反爬，也就是当同一个<code>ip</code>高频的访问这个网站次数过多，就会限制这个<code>ip</code>访问。所以就需要使用代理来隐藏我们的真实<code>ip</code>，同时为了保证我们的代理能长期稳定的使用，就需要随机使用代理池里的代理地址，最简单的代理池实现方式就是将代理地址都放到一个列表里，创建一个随机数，每次请求都随机的从代理池中取一个代理使用，我这里就不贴代码了，就是一个很简单的功能。<br>  这篇博客就说这些简单的爬虫基础吧，已经万字长文了，后面再更新深入的爬虫相关知识。</p><h1 id="参考文献"><a class="header-anchor" href="#参考文献">¶</a>参考文献</h1><p>  【1】<a href="https://www.bilibili.com/video/BV1Db4y1m7Ho?p=51">https://www.bilibili.com/video/BV1Db4y1m7Ho?p=51</a><br>  【2】<a href="https://blog.csdn.net/itcast_cn/article/details/123678415">https://blog.csdn.net/itcast_cn/article/details/123678415</a><br>  【3】<a href="https://www.runoob.com/python3/python-urllib.html">https://www.runoob.com/python3/python-urllib.html</a><br>  【4】<a href="https://blog.csdn.net/m0_46473590/article/details/118328217">https://blog.csdn.net/m0_46473590/article/details/118328217</a><br>  【5】<a href="https://blog.csdn.net/Y_peak/article/details/120068358">https://blog.csdn.net/Y_peak/article/details/120068358</a><br>  【6】<a href="https://www.weixueyuan.net/a/739.html">https://www.weixueyuan.net/a/739.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
