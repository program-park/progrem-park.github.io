<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Python 爬虫基础之 urllib 库的深入使用详解 | 程序园</title><meta name="keywords" content="Python,爬虫"><meta name="author" content="一位木带感情的码农"><meta name="copyright" content="一位木带感情的码农"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="以 Python 为基础的爬虫教程，围绕最基本的 urllib 库展开讲解，附加多个案例同步实操练习。">
<meta property="og:type" content="article">
<meta property="og:title" content="Python 爬虫基础之 urllib 库的深入使用详解">
<meta property="og:url" content="https://program-park.github.io/2022/08/30/reptile_1/index.html">
<meta property="og:site_name" content="程序园">
<meta property="og:description" content="以 Python 为基础的爬虫教程，围绕最基本的 urllib 库展开讲解，附加多个案例同步实操练习。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://program-park.github.io/img/reptile/1.png">
<meta property="article:published_time" content="2022-08-30T09:15:02.000Z">
<meta property="article:modified_time" content="2022-09-08T07:28:27.844Z">
<meta property="article:author" content="一位木带感情的码农">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://program-park.github.io/img/reptile/1.png"><link rel="shortcut icon" href="/img/favicon_logo/favicon.png"><link rel="canonical" href="https://program-park.github.io/2022/08/30/reptile_1/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":350},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":30,"languages":{"author":"作者: 一位木带感情的码农","link":"链接: ","source":"来源: 程序园","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Python 爬虫基础之 urllib 库的深入使用详解',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-08 15:28:27'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatat_img.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">200</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">35</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/reptile/1.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">程序园</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Python 爬虫基础之 urllib 库的深入使用详解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-08-30T09:15:02.000Z" title="发表于 2022-08-30 17:15:02">2022-08-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-08T07:28:27.844Z" title="更新于 2022-09-08 15:28:27">2022-09-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>30分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Python 爬虫基础之 urllib 库的深入使用详解"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p><strong>本文章中所有内容仅供学习交流使用，不用于其他任何目的，严禁用于商业用途和非法用途，否则由此产生的一切后果均与作者无关。</strong></p>
</blockquote>
<h1 id="1-urllib库的使用">1. urllib库的使用</h1>
<h2 id="1-1-获取数据">1.1 获取数据</h2>
<p>  爬虫的定义我这里就不多说了，直接进入正题，如何利用<code>urllib</code>库去使用爬虫。模拟浏览器向服务器发送请求：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://www.某du.com&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>  下面的方法能获取响应的页面源码，但是需要注意的是<code>read()</code>返回的是字节形式的二进制数据，所以需要<code>decode()</code>解码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>  返回的<code>reponse</code>的类型为<code>HTTPResponse</code>，该返回值常用的方法如下：</p>
<ul>
<li><strong>read()</strong> ：字节形式读取二进制数据，<code>read(Number)</code>返回指定字节的数据；</li>
<li><strong>readline()</strong> ：读取一行；</li>
<li><strong>readlines()</strong> ：一行一行读取所有；</li>
<li><strong>getcode()</strong> ：获取响应状态码；</li>
<li><strong>geturl()</strong> ：获取请求的URL；</li>
<li><strong>getheaders()</strong> ：获取headers；</li>
</ul>
<h2 id="1-2-下载">1.2 下载</h2>
<p>  除了获取数据，我们也能通过爬虫下载网页资源：</p>
<ul>
<li>下载网页：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.urlretrieve(<span class="string">&#x27;http://www.某du.com&#x27;</span>,<span class="string">&#x27;baidu.html&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>下载图片：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.urlretrieve(<span class="string">&#x27;https://xxxxx.cdn.bcebos.com/pic/fcfaaf51f3deb48f8f9616a7fd1f3a292cf578cf?x-bce-process=image/resize,m_lfit,h_500,limit_1/format,f_auto&#x27;</span>,<span class="string">&#x27;迪丽热巴.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>下载视频：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.urlretrieve(<span class="string">&#x27;https://vd2.xxxxxxxx.com/mda-magpzuurpki42hbv/v1-cae/sc/mda-magpzuurpki42hbv.mp4?v_from_s=hkapp-haokan-hbe&amp;auth_key=1661150989-0-0-5951edef259d3b8ceac0327beca694ac&amp;bcevod_channel=searchbox_feed&amp;pd=1&amp;cd=0&amp;pt=3&amp;logid=1189836669&amp;vid=12518682849229018276&amp;abtest=103579_1-103742_4&amp;klogid=1189836669&#x27;</span>,<span class="string">&#x27;迪丽热巴.mp4&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="2-请求对象request的定制">2. 请求对象request的定制</h1>
<blockquote>
<p><strong>User-Agent，中文名为用户代理，简称 UA，它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本，浏览器内核、浏览器渲染引擎、浏览器语言、浏览器插件等</strong></p>
</blockquote>
<p>  URL 的组成有以下几个部分：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">http/https	wwww.某du.com	80/443	s	wd=迪丽热巴	#</span><br><span class="line">协议			主机				端口号	路径	参数			锚点</span><br></pre></td></tr></table></figure>
<p>  上面的例子我们是请求的<code>http</code>接口，能够正常爬取，但是请求<code>https</code>某度网址，会遇到<code>UA</code>反爬，也是很常见的一种反爬机制：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">url = <span class="string">&#x27;https://www.某du.com&#x27;</span></span><br><span class="line">response = urllib.request.urlopen(url)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/61ec838df1da40768026e08d52e440f9.png#pic_center" alt="在这里插入图片描述"></p>
<p>  所以在发送请求时，要带上我们的<code>UA</code>，<code>UA</code>的获取方法如下：</p>
<p><img src="https://img-blog.csdnimg.cn/543aca0fee964b35803f1d67dcc9d123.png#pic_center" alt="在这里插入图片描述"><br>
  带上<code>UA</code>爬虫代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">&#x27;https://www.某du.com&#x27;</span></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>
<p>  因为<code>urlopen()</code>方法不支持字典变量，所以我们可以将信息放入定制的<code>Request</code>对象中，再将<code>Request</code>对象放入<code>urlopen()</code>中。这里需要注意的是，<code>Request()</code>方法的参数必须使用关键字传参，查看<code>Request</code>源码，可以发现，第二个参数是<code>data=None</code>，所以不指定的话，默认会认为传入的<code>UA</code>是<code>data</code>参数：</p>
<p><img src="https://img-blog.csdnimg.cn/7fa42a5a989f440e87de55c361dabd7c.png#pic_center" alt="在这里插入图片描述"></p>
<h1 id="3-编解码">3. 编解码</h1>
<blockquote>
<p><strong>编码集的演变：由于计算机是美国人发明的，因此，最早只有127个字符被编码到计算机里，也就是大小写英文字母、数字和一些符号，这个编码表被称为ASCII编码，比如大写字母A的编码是65，小写字母z的编码是122。但是要处理中文显然一个字节是不够的，至少需要两个字节，而且还不能和ASCII编码冲突，所以，中国制定了GB2312编码，用来把中文编进去。你可以想得到的是，全世界有上百种语言，日本把日文编到Shift_JIS里，韩国把韩文编到Euc‐kr里，各国有各国的标准，就会不可避免地出现冲突，结果就是，在多语言混合的文本中，显示出来会有乱码。<br>
因此，Unicode应运而生。Unicode把所有语言都统一到一套编码里，这样就不会再有乱码问题了。Unicode标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要4个字节）。现代操作系统和大多数编程语言都直接支持Unicode。</strong></p>
</blockquote>
<h2 id="3-1-get请求">3.1 get请求</h2>
<h3 id="3-1-1-quote">3.1.1 quote()</h3>
<p>  上面说了 URL 的组成和<code>UA</code>的作用，那么现在有一个需求：获取<code>https://www.某du.com/s?wd=迪丽热巴</code>的网页源码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">&#x27;https://www.某du.com/s?wd=迪丽热巴&#x27;</span></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>
<p>  直接运行上面的代码，会发现报错：</p>
<p><img src="https://img-blog.csdnimg.cn/3c0a4978b000456f924614abf9b54397.png#pic_center" alt="在这里插入图片描述"><br>
  这是因为编码的问题，我们需要将<code>迪丽热巴</code>四个字变成<code>unicode</code>编码的格式，而这需要依赖<code>urllib.parse</code>的<code>quote()</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">name = urllib.parse.quote(<span class="string">&#x27;迪丽热巴&#x27;</span>)</span><br><span class="line">url = <span class="string">&#x27;https://www.某du.com/s?wd=&#x27;</span> + name</span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>
<h3 id="3-1-2-urlencode">3.1.2 urlencode()</h3>
<p>  既然有将一个参数变成<code>unicode</code>编码格式的方法，那么肯定有将多个参数同时变为<code>unicode</code>编码格式的方法，毕竟我们平时用的接口不可能只有一个参数，这个方法就是<code>urlencode()</code>，它的参数类型是字典，应用场景：URL 同时有多个参数时，将多个参数变成<code>unicode</code>编码的格式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;wd&#x27;</span> : <span class="string">&#x27;迪丽热巴&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sex&#x27;</span> : <span class="string">&#x27;女&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">a = urllib.parse.urlencode(data)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>
<p>  运行结果如下，将多个参数同时转为了<code>unicode</code>编码格式并用<code>&amp;</code>分割：</p>
<p><img src="https://img-blog.csdnimg.cn/1cf0a9b4c89f41a0af9fd79d688b86b3.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="3-2-post请求">3.2 post请求</h2>
<p>  上面说的案例都是<code>get</code>请求接口，但其实我们遇到的更多的还是<code>post</code>请求接口，在请求时要带上参数值，以某度翻译为例，找到发送单词请求数据的 URL：</p>
<p><img src="https://img-blog.csdnimg.cn/8ee72c85d4b84760adf3b9f6b93ea64d.png#pic_center" alt="在这里插入图片描述"><br>
  请求地址为<code>https://fanyi.某du.com/sug</code>，且以<code>post</code>方式发送。携带的参数为<code>&#123;kw:copy&#125;</code>，返回的数据为一组<code>json</code>数据，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">url = <span class="string">&#x27;https://fanyi.某du.com/sug&#x27;</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;kw&#x27;</span> : <span class="string">&#x27;copy&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># post请求的参数必须进行编码，编码后必须调用encode()</span></span><br><span class="line">data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">request = urllib.request.Request(url=url, data=data, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">obj = json.loads(content)</span><br><span class="line"><span class="built_in">print</span>(obj)</span><br></pre></td></tr></table></figure>
<p>  <strong>注意：</strong></p>
<ul>
<li><code>get</code>请求方式的参数必须编码，参数是拼接到 URL 后面，编码之后不需要调用<code>encode()</code>方法；</li>
<li><code>post</code>请求方式的参数必须编码，参数是放在请求对象定制的方法中，编码之后需要调用<code>encode()</code>方法；</li>
</ul>
<h2 id="3-3-案例：某度详细翻译">3.3 案例：某度详细翻译</h2>
<p>  前面的案例用过了某度翻译的<code>post</code>的接口，但其实某度翻译还有一个某度详细翻译的接口，返回的数据更加详细：</p>
<p><img src="https://img-blog.csdnimg.cn/8dce122e647c402f944de43e30ac1218.png#pic_center" alt="在这里插入图片描述"><br>
  该接口需要的参数如下：</p>
<p><img src="https://img-blog.csdnimg.cn/ce1b08d764d8456d894ee209edea159b.png#pic_center" alt="在这里插入图片描述"><br>
  爬虫代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">url = <span class="string">&#x27;https://fanyi.某du.com/v2transapi?from=en&amp;to=zh&#x27;</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;from&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;to&#x27;</span>: <span class="string">&#x27;zh&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;query&#x27;</span>: <span class="string">&#x27;copy&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;simple_means_flag&#x27;</span>: <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sign&#x27;</span>: <span class="string">&#x27;479144.240793&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;token&#x27;</span>: <span class="string">&#x27;68566c3678a1d2d18424ef21037ad1cd&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;domain&#x27;</span>: <span class="string">&#x27;common&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># post请求的参数必须进行编码，编码后必须调用encode()</span></span><br><span class="line">data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment"># 请求对象的定制</span></span><br><span class="line">request = urllib.request.Request(url=url, data=data, headers=headers)</span><br><span class="line"><span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"><span class="comment"># 获取响应的数据</span></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">obj = json.loads(content)</span><br><span class="line"><span class="built_in">print</span>(obj)</span><br></pre></td></tr></table></figure>
<p>  运行代码，遇到报错：</p>
<p><img src="https://img-blog.csdnimg.cn/2211079b1ba8467781063cf59c13d1ed.png#pic_center" alt="在这里插入图片描述"><br>
  这是因为请求头中的数据没有全部放到<code>headers</code>中：</p>
<p><img src="https://img-blog.csdnimg.cn/ee3b495d8f2a42309f06deab8ae57c22.png#pic_center" alt="在这里插入图片描述"><br>
  将请求头的数据全部放入<code>headers</code>即可，这里我使用的<code>Sublime Text</code>修改的数据格式，否则手动改还是有些费事：</p>
<p><img src="https://img-blog.csdnimg.cn/3b2de1a4a30045299ac3148938eae1a3.png#pic_center" alt="在这里插入图片描述"><br>
  加入请求头数据后的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">url = <span class="string">&#x27;https://fanyi.某du.com/v2transapi?from=en&amp;to=zh&#x27;</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;from&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;to&#x27;</span>: <span class="string">&#x27;zh&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;query&#x27;</span>: <span class="string">&#x27;copy&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;simple_means_flag&#x27;</span>: <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sign&#x27;</span>: <span class="string">&#x27;479144.240793&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;token&#x27;</span>: <span class="string">&#x27;68566c3678a1d2d18424ef21037ad1cd&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;domain&#x27;</span>: <span class="string">&#x27;common&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;Accept&#x27;</span>:<span class="string">&#x27;*/*&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Accept-Encoding&#x27;</span>:<span class="string">&#x27;gzip, deflate, br&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Accept-Language&#x27;</span>:<span class="string">&#x27;zh-CN,zh;q=0.9&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Acs-Token&#x27;</span>:<span class="string">&#x27;1661151781547_1661235691501_0f9CZMIZ5F98s+p/e4nt/8nbrZ+AlxwEaULsxFVtWLyXY5q1G9TLC9A94gcj+3pNbT9maF1j+iqywD1ReX5S2xJ4XVhHgh2AoNNoG3kKPIvkKt1xg5LC/q465CS7LldtxQiHZH7aVD3hI98PAkb/BP/WJL94eJnMUR5z3TWWHWHznmhHc3DBZphXD7bvfieuSSSTrSfQJKh1gZAAED7Qd+FkTJPyTmx71HOYgDoKXgL8at5CVbstnRlGybS6HKSaJ+q9fAy8R9pup4Op9+6xuNh2sFR9axAE+92cP8gcQLqUd7B745Bv7Bbjv1odwUqbFJUcsLMyu1qH0yi46MQlbQ==&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Connection&#x27;</span>:<span class="string">&#x27;keep-alive&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Content-Length&#x27;</span>:<span class="string">&#x27;116&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Content-Type&#x27;</span>:<span class="string">&#x27;application/x-www-form-urlencoded; charset=UTF-8&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Cookie&#x27;</span>:<span class="string">&#x27;BIDUPSID=EDEDD8AC90775AB7BB2E2A9BA887CB6A; PSTM=1643847626; BAIDUID=EDEDD8AC90775AB76211B82417CB627A:FG=1; __yjs_duid=1_3f010fc33690405f6d48831d8577999f1644021279727; FANYI_WORD_SWITCH=1; REALTIME_TRANS_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; APPGUIDE_10_0_2=1; BDUSS=VhuaU9TRWVWZmY2ZjhvTURUWkxsYnV6VGNrdDhuVnpLeGdiU3BWazZlYk5RQmRqRVFBQUFBJCQAAAAAAAAAAAEAAAA5xXOd0KHQob70z8LKvwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM2z72LNs-9iV; BDUSS_BFESS=VhuaU9TRWVWZmY2ZjhvTURUWkxsYnV6VGNrdDhuVnpLeGdiU3BWazZlYk5RQmRqRVFBQUFBJCQAAAAAAAAAAAEAAAA5xXOd0KHQob70z8LKvwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM2z72LNs-9iV; ZFY=fiXfZrdJz5x8NkMm:AiKmd1ufNDjZkiFrzNjDBw7nRpY:C; BAIDUID_BFESS=EDEDD8AC90775AB76211B82417CB627A:FG=1; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; ariaDefaultTheme=undefined; RT=&quot;z=1&amp;dm=baidu.com&amp;si=1ehud9ig578&amp;ss=l74cz1z6&amp;sl=k&amp;tt=co6&amp;bcn=https%3A%2F%2Ffclog.baidu.com%2Flog%2Fweirwood%3Ftype%3Dperf&amp;ld=d781&amp;ul=f7iu&amp;hd=f7jy&quot;; BA_HECTOR=2l2kal8180842k0k842ga5kc1hg8dvk16; BDRCVFR[feWj1Vr5u3D]=I67x6TjHwwYf0; delPer=0; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1660717558,1660788138,1661134918,1661223741; PSINO=1; H_PS_PSSID=36559_36463_36641_36982_36885_34812_36917_36779_37137_26350_37089_37194; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1661235691&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Host&#x27;</span>:<span class="string">&#x27;fanyi.baidu.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Origin&#x27;</span>:<span class="string">&#x27;https://fanyi.baidu.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>:<span class="string">&#x27;https://fanyi.baidu.com/?aldtype=16047&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua&#x27;</span>:<span class="string">&#x27;&quot;Chromium&quot;;v=&quot;104&quot;, &quot; Not A;Brand&quot;;v=&quot;99&quot;, &quot;Google Chrome&quot;;v=&quot;104&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-mobile&#x27;</span>:<span class="string">&#x27;?0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-platform&#x27;</span>:<span class="string">&#x27;&quot;Windows&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Dest&#x27;</span>:<span class="string">&#x27;empty&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Mode&#x27;</span>:<span class="string">&#x27;cors&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Site&#x27;</span>:<span class="string">&#x27;same-origin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;X-Requested-With&#x27;</span>:<span class="string">&#x27;XMLHttpRequest&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># post请求的参数必须进行编码，编码后必须调用encode()</span></span><br><span class="line">data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment"># 请求对象的定制</span></span><br><span class="line">request = urllib.request.Request(url=url, data=data, headers=headers)</span><br><span class="line"><span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"><span class="comment"># 获取响应的数据</span></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">obj = json.loads(content)</span><br><span class="line"><span class="built_in">print</span>(obj)</span><br></pre></td></tr></table></figure>
<p>  执行后发现还是报错：</p>
<p><img src="https://img-blog.csdnimg.cn/2622c1e1a3d24e229ca9ffb4ee1b5c51.png#pic_center" alt="在这里插入图片描述"><br>
  这是因为接收的编码格式不支持<code>utf-8</code>，将<code>'Accept-Encoding':'gzip, deflate, br'</code>注释即可，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">url = <span class="string">&#x27;https://fanyi.某du.com/v2transapi?from=en&amp;to=zh&#x27;</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;from&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;to&#x27;</span>: <span class="string">&#x27;zh&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;query&#x27;</span>: <span class="string">&#x27;copy&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;simple_means_flag&#x27;</span>: <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sign&#x27;</span>: <span class="string">&#x27;479144.240793&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;token&#x27;</span>: <span class="string">&#x27;68566c3678a1d2d18424ef21037ad1cd&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;domain&#x27;</span>: <span class="string">&#x27;common&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;Accept&#x27;</span>:<span class="string">&#x27;*/*&#x27;</span>,</span><br><span class="line">    <span class="comment"># &#x27;Accept-Encoding&#x27;:&#x27;gzip, deflate, br&#x27;,</span></span><br><span class="line">    <span class="string">&#x27;Accept-Language&#x27;</span>:<span class="string">&#x27;zh-CN,zh;q=0.9&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Acs-Token&#x27;</span>:<span class="string">&#x27;1661151781547_1661235691501_0f9CZMIZ5F98s+p/e4nt/8nbrZ+AlxwEaULsxFVtWLyXY5q1G9TLC9A94gcj+3pNbT9maF1j+iqywD1ReX5S2xJ4XVhHgh2AoNNoG3kKPIvkKt1xg5LC/q465CS7LldtxQiHZH7aVD3hI98PAkb/BP/WJL94eJnMUR5z3TWWHWHznmhHc3DBZphXD7bvfieuSSSTrSfQJKh1gZAAED7Qd+FkTJPyTmx71HOYgDoKXgL8at5CVbstnRlGybS6HKSaJ+q9fAy8R9pup4Op9+6xuNh2sFR9axAE+92cP8gcQLqUd7B745Bv7Bbjv1odwUqbFJUcsLMyu1qH0yi46MQlbQ==&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Connection&#x27;</span>:<span class="string">&#x27;keep-alive&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Content-Length&#x27;</span>:<span class="string">&#x27;116&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Content-Type&#x27;</span>:<span class="string">&#x27;application/x-www-form-urlencoded; charset=UTF-8&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Cookie&#x27;</span>:<span class="string">&#x27;BIDUPSID=EDEDD8AC90775AB7BB2E2A9BA887CB6A; PSTM=1643847626; BAIDUID=EDEDD8AC90775AB76211B82417CB627A:FG=1; __yjs_duid=1_3f010fc33690405f6d48831d8577999f1644021279727; FANYI_WORD_SWITCH=1; REALTIME_TRANS_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; APPGUIDE_10_0_2=1; BDUSS=VhuaU9TRWVWZmY2ZjhvTURUWkxsYnV6VGNrdDhuVnpLeGdiU3BWazZlYk5RQmRqRVFBQUFBJCQAAAAAAAAAAAEAAAA5xXOd0KHQob70z8LKvwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM2z72LNs-9iV; BDUSS_BFESS=VhuaU9TRWVWZmY2ZjhvTURUWkxsYnV6VGNrdDhuVnpLeGdiU3BWazZlYk5RQmRqRVFBQUFBJCQAAAAAAAAAAAEAAAA5xXOd0KHQob70z8LKvwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM2z72LNs-9iV; ZFY=fiXfZrdJz5x8NkMm:AiKmd1ufNDjZkiFrzNjDBw7nRpY:C; BAIDUID_BFESS=EDEDD8AC90775AB76211B82417CB627A:FG=1; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; ariaDefaultTheme=undefined; RT=&quot;z=1&amp;dm=baidu.com&amp;si=1ehud9ig578&amp;ss=l74cz1z6&amp;sl=k&amp;tt=co6&amp;bcn=https%3A%2F%2Ffclog.baidu.com%2Flog%2Fweirwood%3Ftype%3Dperf&amp;ld=d781&amp;ul=f7iu&amp;hd=f7jy&quot;; BA_HECTOR=2l2kal8180842k0k842ga5kc1hg8dvk16; BDRCVFR[feWj1Vr5u3D]=I67x6TjHwwYf0; delPer=0; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1660717558,1660788138,1661134918,1661223741; PSINO=1; H_PS_PSSID=36559_36463_36641_36982_36885_34812_36917_36779_37137_26350_37089_37194; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1661235691&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Host&#x27;</span>:<span class="string">&#x27;fanyi.baidu.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Origin&#x27;</span>:<span class="string">&#x27;https://fanyi.baidu.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>:<span class="string">&#x27;https://fanyi.baidu.com/?aldtype=16047&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua&#x27;</span>:<span class="string">&#x27;&quot;Chromium&quot;;v=&quot;104&quot;, &quot; Not A;Brand&quot;;v=&quot;99&quot;, &quot;Google Chrome&quot;;v=&quot;104&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-mobile&#x27;</span>:<span class="string">&#x27;?0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-platform&#x27;</span>:<span class="string">&#x27;&quot;Windows&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Dest&#x27;</span>:<span class="string">&#x27;empty&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Mode&#x27;</span>:<span class="string">&#x27;cors&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sec-Fetch-Site&#x27;</span>:<span class="string">&#x27;same-origin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;X-Requested-With&#x27;</span>:<span class="string">&#x27;XMLHttpRequest&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># post请求的参数必须进行编码，编码后必须调用encode()</span></span><br><span class="line">data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment"># 请求对象的定制</span></span><br><span class="line">request = urllib.request.Request(url=url, data=data, headers=headers)</span><br><span class="line"><span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"><span class="comment"># 获取响应的数据</span></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">obj = json.loads(content)</span><br><span class="line"><span class="built_in">print</span>(obj)</span><br></pre></td></tr></table></figure>
<p>  需要注意的是，在整个请求标头中，最重要的就是<code>cookie</code>，事实上将请求标头中除去<code>cookie</code>外所有的行全部注释，爬虫也能正常运行。</p>
<blockquote>
<p><strong>cookie 一般是登录后产生（post），用来保持登录状态的，一般登录一次，下一次访问该网站下的其他网址时就不需要登录了，这就是由于cookie的作用。cookie 就是给无状态的 HTTP/HTTPS 协议添加了一种保持之前状态的功能，这样下次处理信息的时候就不用重新获取信息了。<br>
cookie 还可以来判断是否是爬虫程序，因为一般的爬虫程序中并不会携带 cookie，有些比较严格的网站，不登录也需要携带 cookie 访问，也就是说 cookie 的应用场景并不仅仅只有登录后才需要。</strong></p>
</blockquote>
<h1 id="4-Ajax">4. Ajax</h1>
<blockquote>
<p><strong>Ajax 即 “Asynchronous Javascript And XML”（异步 JavaScript 和 XML），是指⼀种创建交互式网页应用的网页开发技术。用于创建快速动态网页，在无需重新加载整个网页的情况下，能够更新部分网页的技术。</strong><br>
<strong>通过在后台与服务器进行少量数据交换，Ajax 可以使网页实现异步更新，这意味着可以在不重新加载整个网页的情况下，对网页的某个部分进行更新，而传统的网页（不使用 Ajax）如果需要更新内容，必须重载整个页面。</strong></p>
</blockquote>
<h2 id="4-1-Ajax的get请求">4.1 Ajax的get请求</h2>
<p>  以爬取某瓣网电影前十页数据为案例，某瓣网的分页就是用<code>Ajax</code>实现的。先完成一个小目标，爬取第一页数据，找到某瓣网数据接口：</p>
<p><img src="https://img-blog.csdnimg.cn/0709326ec6a24a4fb6c10f402c8024b1.png#pic_center" alt="在这里插入图片描述"><br>
  爬取首页数据并写入到文件中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">&#x27;https://movie.某ban.com/j/chart/top_list?type=20&amp;interval_id=100%3A90&amp;action=&amp;start=0&amp;limit=20&#x27;</span></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">file = <span class="built_in">open</span>(<span class="string">&#x27;douban.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">file.write(content)</span><br></pre></td></tr></table></figure>
<p>  查看文件内容，快捷键<code>ctrl+alt+L</code>自动对齐，已爬取到第一页的数据：</p>
<p><img src="https://img-blog.csdnimg.cn/f7e72165a07746de9d36ea5a3a993a59.png#pic_center" alt="在这里插入图片描述"><br>
  下面就是爬取前十页的数据了。通过观察，找到下面三个接口，这三个接口是豆瓣网电影前三页的接口，可以明显看到这三个接口的规律，接口是根据<code>start=(page-1)*20</code>来区分页数的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://movie.某ban.com/j/chart/top_list?type=20&amp;interval_id=100%3A90&amp;action=&amp;start=0&amp;limit=20</span><br><span class="line">https://movie.某ban.com/j/chart/top_list?type=20&amp;interval_id=100%3A90&amp;action=&amp;start=20&amp;limit=20</span><br><span class="line">https://movie.某ban.com/j/chart/top_list?type=20&amp;interval_id=100%3A90&amp;action=&amp;start=40&amp;limit=20</span><br></pre></td></tr></table></figure>
<p>  知道了接口规律，那么接下来就是爬取前十页数据了，代码已封装：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回request对象</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_request</span>(<span class="params">page</span>):</span><br><span class="line">    base_url = <span class="string">&#x27;https://movie.某ban.com/j/chart/top_list?type=20&amp;interval_id=100%3A90&amp;action=&amp;&#x27;</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">&#x27;start&#x27;</span>: (page - <span class="number">1</span>) * <span class="number">20</span>,</span><br><span class="line">        <span class="string">&#x27;limit&#x27;</span>: <span class="number">20</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 将data转换为unicode编码格式</span></span><br><span class="line">    data = urllib.parse.urlencode(data)</span><br><span class="line">    url = base_url + data</span><br><span class="line">    <span class="comment"># 定制request</span></span><br><span class="line">    request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取响应数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">request</span>):</span><br><span class="line">    <span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存文件到本地</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_load</span>(<span class="params">page, content</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;douban_&#x27;</span> + <span class="built_in">str</span>(page) + <span class="string">&#x27;.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span>  file:</span><br><span class="line">        file.write(content)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入起始页码：&#x27;</span>))</span><br><span class="line">    end_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入结束页码：&#x27;</span>))</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start_page, end_page + <span class="number">1</span>):</span><br><span class="line">        request = create_request(page)</span><br><span class="line">        content = get_content(request)</span><br><span class="line">        down_load(page, content)</span><br></pre></td></tr></table></figure>
<p>  运行代码可以得到对应页面的数据文件，效果我就不放了。</p>
<h2 id="4-2-Ajax的post请求">4.2 Ajax的post请求</h2>
<p>  下面以爬取某德基餐厅信息的案例来演示，如何爬取<code>Ajax</code>的<code>post</code>请求。先登录某德基官网，找到对应的<code>post</code>接口。（图片违规，不让我放上来。。。。。。。）<br>
  然后通过对比前三页接口的不同，找到不同页码接口之间的规律：</p>
<p><img src="https://img-blog.csdnimg.cn/8640788300ea4e79a414fe24d307b957.png#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/4199a65f97234d908ac5716dcf8165d9.png#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/eaa886b1d3ab4a24bc437d84336f4de0.png#pic_center" alt="在这里插入图片描述"><br>
  可以明显发现，该接口是通过<code>pageIndex</code>参数控制页码，<code>pageSize</code>控制每页的数量，<code>cname</code>控制地区。下面就直接放代码了，难点不多：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回request对象</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_request</span>(<span class="params">page</span>):</span><br><span class="line">    base_url = <span class="string">&#x27;http://www.某fc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname&#x27;</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">&#x27;cname&#x27;</span>: <span class="string">&#x27;北京&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pid&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pageIndex&#x27;</span>: page,</span><br><span class="line">        <span class="string">&#x27;pageSize&#x27;</span>: <span class="string">&#x27;10&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 将data转换为unicode编码格式，post接口需要用encode()编码</span></span><br><span class="line">    data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="comment"># 定制请求对象</span></span><br><span class="line">    request = urllib.request.Request(url=base_url, data=data, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取响应数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">request</span>):</span><br><span class="line">    <span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存文件到本地</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_load</span>(<span class="params">page, content</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;某fc_&#x27;</span> + <span class="built_in">str</span>(page) + <span class="string">&#x27;.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        file.write(content)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入起始页码：&#x27;</span>))</span><br><span class="line">    end_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入结束页码：&#x27;</span>))</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start_page, end_page + <span class="number">1</span>):</span><br><span class="line">        request = create_request(page)</span><br><span class="line">        content = get_content(request)</span><br><span class="line">        down_load(page, content)</span><br></pre></td></tr></table></figure>
<h1 id="5-捕获异常">5. 捕获异常</h1>
<p>  如果我们想让自己的爬虫代码更加健壮的话，也是需要捕获异常的，这里就介绍两个异常类，<code>HTTPError</code>和<code>URLError</code>，<code>HTTPError</code>类是<code>URLError</code>类的子类。<br>
  当然实际平时爬虫的使用中，捕获异常使用的并不多，所以我这里就不做过多的介绍了，直接拿上一个某德基的代码改一下放这里：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回request对象</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_request</span>(<span class="params">page</span>):</span><br><span class="line">    base_url = <span class="string">&#x27;http://www.某fc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname&#x27;</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">&#x27;cname&#x27;</span>: <span class="string">&#x27;北京&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pid&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pageIndex&#x27;</span>: page,</span><br><span class="line">        <span class="string">&#x27;pageSize&#x27;</span>: <span class="string">&#x27;10&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 将data转换为unicode编码格式，post接口需要用encode()编码</span></span><br><span class="line">    data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="comment"># 定制请求对象</span></span><br><span class="line">    request = urllib.request.Request(url=base_url, data=data, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取响应数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">request</span>):</span><br><span class="line">    <span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存文件到本地</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_load</span>(<span class="params">page, content</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;某fc_&#x27;</span> + <span class="built_in">str</span>(page) + <span class="string">&#x27;.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        file.write(content)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        start_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入起始页码：&#x27;</span>))</span><br><span class="line">        end_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入结束页码：&#x27;</span>))</span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start_page, end_page + <span class="number">1</span>):</span><br><span class="line">            request = create_request(page)</span><br><span class="line">            content = get_content(request)</span><br><span class="line">            down_load(page, content)</span><br><span class="line">    <span class="keyword">except</span> urllib.error.HTTPError:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;系统正在升级&#x27;</span>)</span><br><span class="line">    <span class="keyword">except</span> urllib.error.URLError:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;您访问的域名不存在&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="6-cookie登录">6. cookie登录</h1>
<p>  在很多网站中，和上面案例中的网站有一个不一样的地方，就是未登录状态和登录状态所展示出来的数据是不一样的，这里就讲一下，如果想采集登录之后才展示出来的数据，应该怎么去写代码。这里就以某博<code>https://某bo.cn/</code>为例，采集个人主页的信息，首先按照上面讲过的代码进行尝试（因为涉及个人隐私，所以代码中的链接 uid 和后续的 cookie 我都会做随机修改处理，各位以自己的为准即可）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://某bo.cn/6328349562/info&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>
<p>  运行代码会发现编码不对：</p>
<p><img src="https://img-blog.csdnimg.cn/895ef61125804162b2917a0b79b1e2a8.png#pic_center" alt="在这里插入图片描述"><br>
  而查看个人信息网页的源代码，编码格式确实是<code>utf-8</code>，之所以编码不对是因为我们没有登录信息，请求个人信息主页时会自动跳转到登录页面，而登录页面的编码格式却是<code>gb2312</code>，这也是一个小的爬虫手段，所以我们只需要改一下编码格式为<code>gb2312</code>即可。可是随之而来的问题是，我们这样获取的是登录页面的信息，而我们实际想要的是个人信息主页，应该怎么去绕过登录页面直接到达个人信息主页呢，这就需要一个重要的东西，就是<code>cookie</code>，在个人信息主页，打开检查，点击网络的<code>info</code>，将请求头的东西都复制出来：</p>
<p><img src="https://img-blog.csdnimg.cn/fb2d4676c7854df5851b88ca9dd27fef.jpeg#pic_center" alt="在这里插入图片描述"><br>
  前面带有冒号的可以不要，放入代码的<code>headers</code>中，可以用前面说过的正则方法，方便快捷。注意编码格式需要改回<code>utf-8</code>，因为这里不需要再去登录页了，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://某bo.cn/6347449799/info&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;accept&#x27;</span>:<span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9&#x27;</span>,</span><br><span class="line">    <span class="comment"># &#x27;accept-encoding&#x27;:&#x27;gzip, deflate, br&#x27;,</span></span><br><span class="line">    <span class="string">&#x27;accept-language&#x27;</span>:<span class="string">&#x27;zh-CN,zh;q=0.9&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;cache-control&#x27;</span>:<span class="string">&#x27;max-age=0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;cookie&#x27;</span>:<span class="string">&#x27;_T_WM=f1c3ebd01cf7dde23704b362ixs82v8q; SUB=_2A25OA26gDeRhGeBN71UV9CfLwjWIHXVtDHLorDV6PUNbktANLUjfkmdQ7XiRIiAWGcim-RPUJFDBLv9yPaBCUSFB; SUBP=0033WrSXqPxfM725Ws9jqgMF26451P9D9WFzL6FUia2PJzLiF9NnWlEC5JpX5KzhUgL.Foq0ShNESD.N2K.2dKCoIE7LxK-L1KBLB-qLxKBLB.BLBKWaUJYLxKBLBonL12BLxKqL1MDUVG6Efeh2centt; SSOLoginState=1661038641; ALF=1664046284; MLOGIN=1; M_WEIBOCN_PARAMS=luicode%3D20084629&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;referer&#x27;</span>:<span class="string">&#x27;https://weibo.cn/&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua&#x27;</span>:<span class="string">&#x27;&quot;Chromium&quot;;v=&quot;104&quot;, &quot; Not A;Brand&quot;;v=&quot;99&quot;, &quot;Google Chrome&quot;;v=&quot;104&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-mobile&#x27;</span>:<span class="string">&#x27;?0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-platform&#x27;</span>:<span class="string">&#x27;&quot;Windows&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-dest&#x27;</span>:<span class="string">&#x27;document&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-mode&#x27;</span>:<span class="string">&#x27;navigate&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-site&#x27;</span>:<span class="string">&#x27;same-origin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-user&#x27;</span>:<span class="string">&#x27;?1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;upgrade-insecure-requests&#x27;</span>:<span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;user-agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>
<p>  如此便可获取到个人主页的信息，效果图我就不放了，懒。</p>
<h1 id="7-Handler处理器">7. Handler处理器</h1>
<p>  随着业务逻辑逐渐复杂，请求对象的定制已经满足不了我们的需求，长期爬取一个网站，会被网站识别为爬虫，对我们的设备进行封 IP 操作，所以我们就需要动态<code>cookie</code>和代理来解决这个问题，而代理就是以<code>Handler</code>为基础，这里还是先用百度的简单案例，讲一下如何使用<code>Handler</code>处理器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.某du.com&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line"><span class="comment"># 获取handler对象</span></span><br><span class="line">handler = urllib.request.HTTPHandler()</span><br><span class="line"><span class="comment"># 获取opener对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"><span class="comment"># 调用open方法</span></span><br><span class="line">response = opener.<span class="built_in">open</span>(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>
<h1 id="8-代理的基本使用">8. 代理的基本使用</h1>
<p>  当我们使用爬虫长期、高频的访问某个网站，就会被识别为爬虫，封禁设备 IP，从而导致无法访问该网站，而代理，就是为了隐藏我们的真实 IP，防止被封禁。当然代理不仅仅是这一个用途，像我们打游戏时开的加速器就是代理，访问外网的资源也需要使用代理，以及一些私密单位的内部资源都需要代理来进行访问，代理也能用来提高访问速度，通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将其保存到缓冲区中，当其他用户再访问相同的信息时， 则直接由缓冲区中取出信息，传给用户，以提高访问速度。<br>
  那么下面我就来演示一下，如何用代理爬取网站信息。首先我们上某度网站，某度<code>ip</code>，可以直接显示出我们的本机<code>ip</code>：</p>
<p><img src="https://img-blog.csdnimg.cn/59b4986709cc4f889549bc04de53d276.png#pic_center" alt="在这里插入图片描述"><br>
  我们先使用之前请求对象定制的方法，来爬取该网站信息看是否能正常获取到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.某du.com/s?wd=ip&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;代理.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(content)</span><br></pre></td></tr></table></figure>
<p>  打开<code>代理.html</code>文件，可以看到和我们正常访问一模一样。<br>
  然后换成<code>Hanlder</code>处理器，使用代理访问，我这里用的是<code>快代理</code>网站的免费代理，连接不太稳定，可能换几个代理才有一个能用的：</p>
<p><img src="https://img-blog.csdnimg.cn/6f487c24ae1546b586f9a265b5078166.png#pic_center" alt="在这里插入图片描述"><br>
  代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.某du.com/s?wd=ip&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">	<span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line"><span class="comment"># response = urllib.request.urlopen(request)</span></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;223.82.60.202:8060&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 获取handler对象</span></span><br><span class="line">handler = urllib.request.ProxyHandler(proxies=proxies)</span><br><span class="line"><span class="comment"># 获取opener对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"><span class="comment"># 调用open方法</span></span><br><span class="line">response = opener.<span class="built_in">open</span>(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;代理.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(content)</span><br></pre></td></tr></table></figure>
<p>  效果图如下：</p>
<p><img src="https://img-blog.csdnimg.cn/5cac1e1a3da04b3aafeeb83aea0b86a6.png#pic_center" alt="在这里插入图片描述"><br>
  当然在我们实际生产中，只用一个代理也是不现实的，也会面临被封<code>ip</code>的风险，所有就有了<code>代理池</code>的概念。</p>
<blockquote>
<p><strong>代理池就是由代理IP组成的池子, 它可以提供多个稳定可用的代理IP。</strong></p>
</blockquote>
<p>  我们在做爬虫的时候, 最常见一种反爬手段就是<code>ip</code>反爬，也就是当同一个<code>ip</code>高频的访问这个网站次数过多，就会限制这个<code>ip</code>访问。所以就需要使用代理来隐藏我们的真实<code>ip</code>，同时为了保证我们的代理能长期稳定的使用，就需要随机使用代理池里的代理地址，最简单的代理池实现方式就是将代理地址都放到一个列表里，创建一个随机数，每次请求都随机的从代理池中取一个代理使用，我这里就不贴代码了，就是一个很简单的功能。<br>
  这篇博客就说这些简单的爬虫基础吧，已经万字长文了，后面再更新深入的爬虫相关知识。</p>
<h1 id="参考文献">参考文献</h1>
<p>  【1】<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Db4y1m7Ho?p=51">https://www.bilibili.com/video/BV1Db4y1m7Ho?p=51</a><br>
  【2】<a target="_blank" rel="noopener" href="https://blog.csdn.net/itcast_cn/article/details/123678415">https://blog.csdn.net/itcast_cn/article/details/123678415</a><br>
  【3】<a target="_blank" rel="noopener" href="https://www.runoob.com/python3/python-urllib.html">https://www.runoob.com/python3/python-urllib.html</a><br>
  【4】<a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_46473590/article/details/118328217">https://blog.csdn.net/m0_46473590/article/details/118328217</a><br>
  【5】<a target="_blank" rel="noopener" href="https://blog.csdn.net/Y_peak/article/details/120068358">https://blog.csdn.net/Y_peak/article/details/120068358</a><br>
  【6】<a target="_blank" rel="noopener" href="https://www.weixueyuan.net/a/739.html">https://www.weixueyuan.net/a/739.html</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://program-park.github.io/">一位木带感情的码农</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://program-park.github.io/2022/08/30/reptile_1/">https://program-park.github.io/2022/08/30/reptile_1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">此文章版权归 <a href=https://program-park.github.io/>程序园</a> 所有，如有转载，请注明来自原作者。</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></div><div class="post_share"><div class="social-share" data-image="/img/reptile/1.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/08/15/linux_6/"><img class="next-cover" src="/img/linux/1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Linux 防火墙常用命令总结</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/03/12/python_1/" title="Python 脚本之将 logstash 数据按天保存在本地服务器并加密压缩"><img class="cover" src="/img/python/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-12</div><div class="title">Python 脚本之将 logstash 数据按天保存在本地服务器并加密压缩</div></div></a></div><div><a href="/2021/08/09/python_10/" title="Windows 下安装 Python 环境"><img class="cover" src="/img/python/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-09</div><div class="title">Windows 下安装 Python 环境</div></div></a></div><div><a href="/2021/08/13/python_12/" title="AttributeEroor: ‘list‘ object has no attribute ‘clear‘"><img class="cover" src="/img/python/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-13</div><div class="title">AttributeEroor: ‘list‘ object has no attribute ‘clear‘</div></div></a></div><div><a href="/2021/08/10/python_11/" title="Python 脚本之对文件进行哈希校验"><img class="cover" src="/img/python/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-10</div><div class="title">Python 脚本之对文件进行哈希校验</div></div></a></div><div><a href="/2021/08/23/python_13/" title="python整数相除保留小数SyntaxError: from __future__ imports must occur at the beginning of the file"><img class="cover" src="/img/python/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-23</div><div class="title">python整数相除保留小数SyntaxError: from __future__ imports must occur at the beginning of the file</div></div></a></div><div><a href="/2022/02/24/python_14/" title="Python 连接使用通过 SSL 认证的 Kafka 教程"><img class="cover" src="/img/python/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-24</div><div class="title">Python 连接使用通过 SSL 认证的 Kafka 教程</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatat_img.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">一位木带感情的码农</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">200</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">35</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/program-park"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/program-park" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:lkm869666@126.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/weixin_44758876" target="_blank" title="CSDN"><i class="fa-solid fa-c"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这里是程序园，如有任何意见和疑问，请反馈到我的邮箱。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-urllib%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">1. urllib库的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-text">1.1 获取数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E4%B8%8B%E8%BD%BD"><span class="toc-text">1.2 下载</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E8%AF%B7%E6%B1%82%E5%AF%B9%E8%B1%A1request%E7%9A%84%E5%AE%9A%E5%88%B6"><span class="toc-text">2. 请求对象request的定制</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E7%BC%96%E8%A7%A3%E7%A0%81"><span class="toc-text">3. 编解码</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-get%E8%AF%B7%E6%B1%82"><span class="toc-text">3.1 get请求</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-quote"><span class="toc-text">3.1.1 quote()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-urlencode"><span class="toc-text">3.1.2 urlencode()</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-post%E8%AF%B7%E6%B1%82"><span class="toc-text">3.2 post请求</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E6%A1%88%E4%BE%8B%EF%BC%9A%E6%9F%90%E5%BA%A6%E8%AF%A6%E7%BB%86%E7%BF%BB%E8%AF%91"><span class="toc-text">3.3 案例：某度详细翻译</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Ajax"><span class="toc-text">4. Ajax</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-Ajax%E7%9A%84get%E8%AF%B7%E6%B1%82"><span class="toc-text">4.1 Ajax的get请求</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-Ajax%E7%9A%84post%E8%AF%B7%E6%B1%82"><span class="toc-text">4.2 Ajax的post请求</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E6%8D%95%E8%8E%B7%E5%BC%82%E5%B8%B8"><span class="toc-text">5. 捕获异常</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-cookie%E7%99%BB%E5%BD%95"><span class="toc-text">6. cookie登录</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-Handler%E5%A4%84%E7%90%86%E5%99%A8"><span class="toc-text">7. Handler处理器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-%E4%BB%A3%E7%90%86%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-text">8. 代理的基本使用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-text">参考文献</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/08/30/reptile_1/" title="Python 爬虫基础之 urllib 库的深入使用详解"><img src="/img/reptile/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python 爬虫基础之 urllib 库的深入使用详解"/></a><div class="content"><a class="title" href="/2022/08/30/reptile_1/" title="Python 爬虫基础之 urllib 库的深入使用详解">Python 爬虫基础之 urllib 库的深入使用详解</a><time datetime="2022-08-30T09:15:02.000Z" title="发表于 2022-08-30 17:15:02">2022-08-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/15/linux_6/" title="Linux 防火墙常用命令总结"><img src="/img/linux/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Linux 防火墙常用命令总结"/></a><div class="content"><a class="title" href="/2022/08/15/linux_6/" title="Linux 防火墙常用命令总结">Linux 防火墙常用命令总结</a><time datetime="2022-08-15T07:20:44.000Z" title="发表于 2022-08-15 15:20:44">2022-08-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/31/linux_5/" title="Linux 基础命令之 tar 解压缩详解"><img src="/img/linux/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Linux 基础命令之 tar 解压缩详解"/></a><div class="content"><a class="title" href="/2022/07/31/linux_5/" title="Linux 基础命令之 tar 解压缩详解">Linux 基础命令之 tar 解压缩详解</a><time datetime="2022-07-31T00:22:50.000Z" title="发表于 2022-07-31 08:22:50">2022-07-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/13/nginx_16/" title="Nginx 从入门到入坟（十五）- Nginx + Tomcat 部署实现动静分离"><img src="/img/nginx/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Nginx 从入门到入坟（十五）- Nginx + Tomcat 部署实现动静分离"/></a><div class="content"><a class="title" href="/2022/07/13/nginx_16/" title="Nginx 从入门到入坟（十五）- Nginx + Tomcat 部署实现动静分离">Nginx 从入门到入坟（十五）- Nginx + Tomcat 部署实现动静分离</a><time datetime="2022-07-13T07:33:23.000Z" title="发表于 2022-07-13 15:33:23">2022-07-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/04/nginx_15/" title="Nginx 从入门到入坟（十四）- Nginx 缓存深入研究"><img src="/img/nginx/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Nginx 从入门到入坟（十四）- Nginx 缓存深入研究"/></a><div class="content"><a class="title" href="/2022/07/04/nginx_15/" title="Nginx 从入门到入坟（十四）- Nginx 缓存深入研究">Nginx 从入门到入坟（十四）- Nginx 缓存深入研究</a><time datetime="2022-07-04T04:20:02.000Z" title="发表于 2022-07-04 12:20:02">2022-07-04</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/reptile/1.png')"><div id="footer-wrap"><div class="copyright">&copy;2022 By 一位木带感情的码农</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'https://program-park.github.io/2022/08/30/reptile_1/'
    this.page.identifier = '/2022/08/30/reptile_1/'
    this.page.title = 'Python 爬虫基础之 urllib 库的深入使用详解'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }

  document.getElementById('darkmode').addEventListener('click', () => {
    setTimeout(() => window.disqusReset(), 200)
  })
}

if ('Valine' === 'Disqus' || !false) {
  if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>