<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Hadoop生态圈（二十八）- MapReduce Map 阶段核心源码分析 | 程序园</title><meta name="keywords" content="大数据,Hadoop"><meta name="author" content="一位木带感情的码农"><meta name="copyright" content="一位木带感情的码农"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="本文是《Hadoop生态圈》系列，第二十八篇：MapReduce Map 阶段核心源码分析。">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop生态圈（二十八）- MapReduce Map 阶段核心源码分析">
<meta property="og:url" content="https://program-park.github.io/2022/02/06/hadoop_32/index.html">
<meta property="og:site_name" content="程序园">
<meta property="og:description" content="本文是《Hadoop生态圈》系列，第二十八篇：MapReduce Map 阶段核心源码分析。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://program-park.github.io/img/hadoop/1.png">
<meta property="article:published_time" content="2022-02-06T08:22:35.000Z">
<meta property="article:modified_time" content="2022-12-05T04:36:25.229Z">
<meta property="article:author" content="一位木带感情的码农">
<meta property="article:tag" content="大数据">
<meta property="article:tag" content="Hadoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://program-park.github.io/img/hadoop/1.png"><link rel="shortcut icon" href="/img/favicon_logo/favicon.png"><link rel="canonical" href="https://program-park.github.io/2022/02/06/hadoop_32/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":350},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":30,"languages":{"author":"作者: 一位木带感情的码农","link":"链接: ","source":"来源: 程序园","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hadoop生态圈（二十八）- MapReduce Map 阶段核心源码分析',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-12-05 12:36:25'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatat_img.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">200</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">35</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">程序园</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Hadoop生态圈（二十八）- MapReduce Map 阶段核心源码分析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2022-02-06T08:22:35.000Z" title="发表于 2022-02-06 16:22:35">2022-02-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Hadoop/">Hadoop</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Hadoop生态圈（二十八）- MapReduce Map 阶段核心源码分析"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h1 id="前言">前言</h1>
<p><font color=#999AAA >部分内容摘自尚硅谷、黑马等等培训资料</font></p>
<h1 id="1-Map阶段整体概述">1. Map阶段整体概述</h1>
<p>  input File 通过 split 被逻辑切分为多个 split 文件，通过 Record 按行读取内容给 map（用户自己实现的）进行处理，数据被 map 处理结束之后交给 OutputCollector 收集器，对其结果 key 进行分区（默认使用 hash 分区），然后写入 buffer，每个 map task 都有一个内存缓冲区，存储着 map 的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，当整个 map task 结束后再对磁盘中这个 map task 产生的所有临时文件做合并，生成最终的正式输出文件，然后等待 reduce task 来拉数据。</p>
<h1 id="2-前置：解读MapTask类">2. 前置：解读MapTask类</h1>
<p>  在 MapReduce 程序中，初登场的 task 叫做 maptask。<code>MapTask</code>类作为 maptask 的一个载体，调用的就是里面的 run 方法，开启 map 任务。</p>
<p><img src="https://img-blog.csdnimg.cn/f17eafc932b94604be1fbf2e4445edf5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h2 id="2-1-第一层调用（run）">2.1 第一层调用（run）</h2>
<p>  在 MapTask.run 方法的第一层调用中，有下面两个重要的代码段。</p>
<h3 id="2-1-1-map阶段的任务划分">2.1.1 map阶段的任务划分</h3>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (isMapTask()) &#123; <span class="comment">// 判断当前的task是否为maptask</span></span><br><span class="line">	<span class="comment">// If there are no reducers then there won&#x27;t be any sort. Hence the map </span></span><br><span class="line">	<span class="comment">// phase will govern the entire attempt&#x27;s progress.</span></span><br><span class="line">	<span class="comment">// 如果reducetask的个数为0 也就意味着程序没有reducer阶段 mapper的输出就是程序最终的输出</span></span><br><span class="line">	<span class="comment">// 这样的话，就没有必要进行shuffle了。</span></span><br><span class="line">	<span class="keyword">if</span> (conf.getNumReduceTasks() == <span class="number">0</span>) &#123;</span><br><span class="line">		<span class="comment">//map阶段占据整个maptask任务的100%</span></span><br><span class="line">		mapPhase = getProgress().addPhase(<span class="string">&quot;map&quot;</span>, <span class="number">1.0f</span>);</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		<span class="comment">// If there are reducers then the entire attempt&#x27;s progress will be </span></span><br><span class="line">		<span class="comment">// split between the map phase (67%) and the sort phase (33%).</span></span><br><span class="line">		<span class="comment">// 如果有reducetask的话，map阶段占据67%，sort阶段占据33%</span></span><br><span class="line">		<span class="comment">// 为什么要sort 因为要shuffle给reducetask</span></span><br><span class="line">		mapPhase = getProgress().addPhase(<span class="string">&quot;map&quot;</span>, <span class="number">0.667f</span>);</span><br><span class="line">		sortPhase  = getProgress().addPhase(<span class="string">&quot;sort&quot;</span>, <span class="number">0.333f</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-1-2-运行Mapper">2.1.2 运行Mapper</h3>
<p>  在提交任务的时候 MR 框架会自己进行选择使用什么 API。<code>默认情况下，使用的都是新的API</code>，除非特别指定了使用 old API 详细见<code>job.setUseNewAPI()</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/791cc4015a3b400fa91a83601c2a5df4.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="2-2-第二层调用（runNewMapper）准备部分">2.2 第二层调用（runNewMapper）准备部分</h2>
<p>  默认情况下，框架使用 new API 来运行，所以将执行<code>runNewMapper()</code>。<br>
  runNewMapper 内第一大部分代码我们称之为 maptask 运行的准备部分，其主要逻辑是创建 maptask 运行时需要的各种依赖：包括 Split 切片信息、inputFormat、LineRecordReader、用户写的 map 函数（位于自定义 mapper 中）、taskContext 上下文、输出收集器 OutputCollector，用于 maptask 处理完数据输出结果。</p>
<h3 id="2-2-1-TaskContext">2.2.1 TaskContext</h3>
<p>  TaskContext 为 Task 的上下文对象，基于此对象可以获取到 Task 执行期间的相关状态信息。</p>
<p><img src="https://img-blog.csdnimg.cn/cc78b38bcec34ddfa41aaceaaf6b5132.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h3 id="2-2-2-split">2.2.2 split</h3>
<p><img src="https://img-blog.csdnimg.cn/7f46aa26d3a94ade8dca36d061adcb7c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h3 id="2-2-3-inputFormat">2.2.3 inputFormat</h3>
<p><img src="https://img-blog.csdnimg.cn/9662e66f7a9a4bac95e93283f4afddea.png#pic_center" alt="在这里插入图片描述"></p>
<h3 id="2-2-4-RecordReader">2.2.4 RecordReader</h3>
<p><img src="https://img-blog.csdnimg.cn/aa806bcc69a94d148a02c025e7127306.png#pic_center" alt="在这里插入图片描述"><br>
  在 NewTrackingRecordReader 方法中，最终创建了 RecordReader</p>
<p><img src="https://img-blog.csdnimg.cn/0d7dc5394c574edcbf8e211f7c3a6097.png#pic_center" alt="在这里插入图片描述"></p>
<h3 id="2-2-5-mapper">2.2.5 mapper</h3>
<p><img src="https://img-blog.csdnimg.cn/ea0b30452c56430e8afb8181d32cccc9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h3 id="2-2-6-OutputCollector">2.2.6 OutputCollector</h3>
<p><img src="https://img-blog.csdnimg.cn/dc2e2d1dddc441d8972b33bee7e299a7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h2 id="2-3-第二层调用（runNewMapper）工作部分">2.3 第二层调用（runNewMapper）工作部分</h2>
<p>  runNewMapper 内第二大部分代码我们称之为 maptask 工作干活的部分，其主要逻辑是：<code>如何从切片读取数据</code>，<code>如何调用map处理数据</code>，<code>如何调用OutputCollector收集输出的数据</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/97ab247ee7b5441f8ed2ba02e12bdc05.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h3 id="2-3-1-RecoderReader-initialize">2.3.1 RecoderReader.initialize</h3>
<p>  默认情况下的实现逻辑位于 LineRecorderReader 中。核心逻辑：<br>
  打开文件定位切片的位置，判断文件是否压缩，如果切片不是第一个切片那么读取数据的时候舍去第一行数据不要读取。</p>
<p><img src="https://img-blog.csdnimg.cn/abde8ae1e8a4487487f6d354ddeb9185.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h3 id="2-3-2-mapper-run">2.3.2 mapper.run</h3>
<p><img src="https://img-blog.csdnimg.cn/116aab12e125438ca8b61c9a48dd42cd.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/6f3cf7f4368241039d3cbdec455be7ea.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h3 id="2-3-3-NewOutputCollector">2.3.3 NewOutputCollector</h3>
<p><img src="https://img-blog.csdnimg.cn/fe712e547ec8439db3e1e4469c80cf0d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/28784d9462fd471e8897dadd8701b0e3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  在这段程序中，createSortingCollector<code>创建map输出收集器是最复杂的一部分，因为和后续环形缓冲区操作有关</code>。进入 createSortingCollector 方法。</p>
<p><img src="https://img-blog.csdnimg.cn/0d87fadbf2924ca7a5d13ad0c6751432.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  注意此处的默认实现是 MapOutputBuffer。</p>
<p><img src="https://img-blog.csdnimg.cn/6aecd745ec8f48feaa7edd20e0f2de84.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  关于输出收集器和环形缓冲区的细节后面再描述。</p>
<h1 id="3-InputFormat">3. InputFormat</h1>
<p>  整个 MapReduce 以 InputFormat 开始，其负责<code>读取待处理的数据</code>。默认的实现叫做<code>TextInputFormat</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/28c63cde28294de8bf9b5259563a1738.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  InputFormat 核心逻辑体现在两个方面：</p>
<ul>
<li>如何读取待处理目录下的文件。一个一个读？还是一起读？</li>
<li>读取数据的行为是什么以及返回什么样的结果？一行一行读？按字节读？</li>
</ul>
<h2 id="3-1-getSplits">3.1 getSplits</h2>
<p>  对于待处理的目录文件，MapReduce 程序面临的首要问题就是：究竟启动多少个 MapTask 来处理本次 job。<br>
  该问题也叫做<code>maptask的并行度问题，指的是map阶段有多少个并行的task共同处理任务</code>。<br>
  map 阶段并行度由客户端在提交 job 时决定，即<code>客户端提交job之前</code>会对待处理数据进行逻辑切片。切片完成会形成切片规划文件（job.split），每个逻辑切片最终对应启动一个 maptask。<br>
  逻辑切片机制由<code>FileInputFormat实现类的getSplits()</code>方法完成。</p>
<p><img src="https://img-blog.csdnimg.cn/93db4d20ea0b467397f4488ebb81868c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  首先需要计算出 split size 切片大小，其计算方法如下：</p>
<p><img src="https://img-blog.csdnimg.cn/208aa707f8a24310ab77982b9fc6201e.png#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/7de484e7cc79459f924755bba1ee751b.png#pic_center" alt="在这里插入图片描述"><br>
  其中 maxSize，minSize 的默认值为：</p>
<p><img src="https://img-blog.csdnimg.cn/16ffebe0bb1b4cedbbec9b0ff5840f69.png#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/6a06316eaca5490187de98733b128489.png#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/2db540b8cbce42138c4107530db8ef21.png#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/73e5494c3d6241d3a9a8defac8f4a528.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  因此，通过计算，默认情况下，最终<code>split size= block size =128M</code>。<br>
  以切片大小逐个遍历待处理的文件，形成逻辑规划文件，比如待处理目录下有下面几个文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a.txt 300M</span><br><span class="line">b.txt 100M</span><br></pre></td></tr></table></figure>
<p>  将会生成如下几个逻辑切片信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">split0	-&gt;	a.txt 0~128M</span><br><span class="line">split1	-&gt;	a.txt 128~256M</span><br><span class="line">split2	-&gt;	a.txt 256~300M</span><br><span class="line">split3	-&gt;	b.txt 0~100M</span><br></pre></td></tr></table></figure>
<p>  默认情况下，<code>有多少个split就对应启动多少个MapTask</code>。<br>
  在 getSplits 方法中，创建了一个集合 splits，用于保存最终的切片信息。</p>
<p><img src="https://img-blog.csdnimg.cn/715acf25844a49e6a6a4b0aecc4ff251.png#pic_center" alt="在这里插入图片描述"><br>
  集合中的每个元素就是一个切片的具体信息：</p>
<p><img src="https://img-blog.csdnimg.cn/8b964eb383df46b9998fdd0897af7264.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  生成的切片信息在客户端提交 job 中，也就是 JobSubmitter.writeSplits 方法中，把<code>所有切片进行排序，大的切片在前，然后序列化到一个文件中，此文件叫做逻辑切片文件</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/5b6a674e6613416ba8caf6988a95272c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h3 id="3-1-1-bytesRemaining">3.1.1 bytesRemaining</h3>
<p>  在进行逻辑切片的时候，假如说一个文件恰好是 129M 大小，那么根据默认的逻辑切片规则将会形成一大一小两个切片（0-128 128-129），并且将启动两个 maptask。这明显对资源的利用效率不高。<br>
  因此在设计中，MapReduce 时刻会进行 bytesRemaining，剩下文件大小，<code>如果剩下的不满足 bytesRemaining/splitSize &gt; SPLIT_SLOP，那么将不再继续split,而是剩下的所有作为一个切片整体</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/434a59e981534818bbb7a70d6f29851c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/e29db3c5d04d409191d76c5ee05a2da9.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="3-2-createRecordReader">3.2 	createRecordReader</h2>
<p>  最终负责读取切片数据的是 RecordReader 类，默认实现是<code>LineRecordReader</code>。其名字已经透露出来其读取数据的行为是：<code>一行一行按行读取数据</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/3907c72d6c67431292b53c13f7a98bc0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  在 LineRecordReader 中，核心的方法有： initialize 初始化方法，nextKeyValue 读取数据方法。</p>
<h3 id="3-2-1-initialize">3.2.1 initialize</h3>
<p>  initialize 属于 LineRecordReader 的初始化方法，会被 MapTask 调用且调用一次。里面描述了如何从切片读取数据。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initialize</span><span class="params">(InputSplit genericSplit, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">	<span class="type">FileSplit</span> <span class="variable">split</span> <span class="operator">=</span> (FileSplit) genericSplit;<span class="comment">//拿到分配的切片</span></span><br><span class="line">	<span class="type">Configuration</span> <span class="variable">job</span> <span class="operator">=</span> context.getConfiguration();</span><br><span class="line">	<span class="built_in">this</span>.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);<span class="comment">//一行能够处理的最大长度</span></span><br><span class="line">	start = split.getStart();<span class="comment">//要处理的切片中第一个字节的位置</span></span><br><span class="line">	end = start + split.getLength();<span class="comment">//切片的结束位置</span></span><br><span class="line">	<span class="keyword">final</span> <span class="type">Path</span> <span class="variable">file</span> <span class="operator">=</span> split.getPath();<span class="comment">//切片的存储路径</span></span><br><span class="line">	<span class="comment">// open the file and seek to the start of the split</span></span><br><span class="line">	<span class="keyword">final</span> <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> file.getFileSystem(job);</span><br><span class="line">	fileIn = fs.open(file);<span class="comment">//打开切片文件开始读数据，返回的是FSDataInputStream输入流</span></span><br><span class="line">	</span><br><span class="line">	<span class="type">CompressionCodec</span> <span class="variable">codec</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CompressionCodecFactory</span>(job).getCodec(file);<span class="comment">//获得文件中编码器</span></span><br><span class="line">	<span class="keyword">if</span> (<span class="literal">null</span>!=codec) &#123;<span class="comment">//判断是否进行编码压缩  如果不为空 意味着文件被编码了</span></span><br><span class="line">		isCompressedInput = <span class="literal">true</span>;</span><br><span class="line">		decompressor = CodecPool.getDecompressor(codec);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (codec <span class="keyword">instanceof</span> SplittableCompressionCodec) &#123;<span class="comment">//判断压缩文件是否可切分 如果是可切分的压缩算法</span></span><br><span class="line">			<span class="keyword">final</span> <span class="type">SplitCompressionInputStream</span> <span class="variable">cIn</span> <span class="operator">=</span> ((SplittableCompressionCodec)codec).createInputStream(fileIn, decompressor, start, end, SplittableCompressionCodec.READ_MODE.BYBLOCK);</span><br><span class="line">			in = <span class="keyword">new</span> <span class="title class_">CompressedSplitLineReader</span>(cIn, job, <span class="built_in">this</span>.recordDelimiterBytes);</span><br><span class="line">			start = cIn.getAdjustedStart();</span><br><span class="line">			end = cIn.getAdjustedEnd();</span><br><span class="line">			filePosition = cIn;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;<span class="comment">//来到这里 表示压缩的编码算法是不可被切分的</span></span><br><span class="line">			<span class="keyword">if</span> (start != <span class="number">0</span>) &#123;</span><br><span class="line">				<span class="comment">// So we have a split that is only part of a file stored using</span></span><br><span class="line">				<span class="comment">// a Compression codec that cannot be split.</span></span><br><span class="line">				<span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IOException</span>(<span class="string">&quot;Cannot seek in &quot;</span> +</span><br><span class="line">				    codec.getClass().getSimpleName() + <span class="string">&quot; compressed stream&quot;</span>);</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="comment">//不可切分的压缩文件整体由SplitLineReader来处理</span></span><br><span class="line">			in = <span class="keyword">new</span> <span class="title class_">SplitLineReader</span>(codec.createInputStream(fileIn, decompressor), job, <span class="built_in">this</span>.recordDelimiterBytes);</span><br><span class="line">			filePosition = fileIn;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;<span class="comment">//这里表示文件未被编码压缩</span></span><br><span class="line">		fileIn.seek(start);</span><br><span class="line">		in = <span class="keyword">new</span> <span class="title class_">UncompressedSplitLineReader</span>(</span><br><span class="line">		    fileIn, job, <span class="built_in">this</span>.recordDelimiterBytes, split.getLength());</span><br><span class="line">		filePosition = fileIn;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// If this is not the first split, we always throw away first record</span></span><br><span class="line">	<span class="comment">// because we always (except the last split) read one extra line in</span></span><br><span class="line">	<span class="comment">// next() method.</span></span><br><span class="line">	<span class="comment">//如果当前处理的不是第一个切片 那么将舍弃第一行记录不处理</span></span><br><span class="line">	<span class="keyword">if</span> (start != <span class="number">0</span>) &#123;</span><br><span class="line">		<span class="comment">//读取一行数据 读取的时候会判断用户是否指定了换行符 如果指定使用用户指定的 如果未指定使用默认的</span></span><br><span class="line">		<span class="comment">//默认的换行符取决于操作系统 Linux:\n   windows:\r\n    Mac:\n</span></span><br><span class="line">		start += in.readLine(<span class="keyword">new</span> <span class="title class_">Text</span>(), <span class="number">0</span>, maxBytesToConsume(start));</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">this</span>.pos = start;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-2-nextKeyValue">3.2.2 nextKeyValue</h3>
<p>  nextKeyValue 方法用于判断是否还有下一行数据以及定义了按行读取数据的逻辑：<br>
  一行一行读取，返回&lt;key,value&gt;键值对类型数据<br>
  其中<code>key是每行起始位置的offset偏移量，value为这一行的内容</code>。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">	<span class="keyword">if</span> (key == <span class="literal">null</span>) &#123;</span><br><span class="line">		key = <span class="keyword">new</span> <span class="title class_">LongWritable</span>();</span><br><span class="line">	&#125;</span><br><span class="line">	key.set(pos);<span class="comment">//起始位置偏移量</span></span><br><span class="line">	<span class="keyword">if</span> (value == <span class="literal">null</span>) &#123;</span><br><span class="line">		value = <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="type">int</span> <span class="variable">newSize</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">	<span class="comment">// We always read one extra line, which lies outside the upper</span></span><br><span class="line">	<span class="comment">// split limit i.e. (end - 1)</span></span><br><span class="line">	<span class="comment">//AllenWoon: 总是多读取下一个切片的一行数据</span></span><br><span class="line">	<span class="keyword">while</span> (getFilePosition() &lt;= end || in.needAdditionalRecordAfterSplit()) &#123;</span><br><span class="line">		<span class="keyword">if</span> (pos == <span class="number">0</span>) &#123;<span class="comment">//起始位置为0的话 跳过文本的UTF-8 BOM头信息</span></span><br><span class="line">			newSize = skipUtfByteOrderMark();</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="comment">//读取该行数据 默认使用readDefaultLine方法读取数据 根据\r\n回车换行符读取一行行数据</span></span><br><span class="line">			newSize = in.readLine(value, maxLineLength, maxBytesToConsume(pos));</span><br><span class="line">			pos += newSize;<span class="comment">//更新偏移量</span></span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> ((newSize == <span class="number">0</span>) || (newSize &lt; maxLineLength)) &#123;</span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// line too long. try again</span></span><br><span class="line">		LOG.info(<span class="string">&quot;Skipped line of size &quot;</span> + newSize + <span class="string">&quot; at pos &quot;</span> + (pos - newSize));</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> (newSize == <span class="number">0</span>) &#123;</span><br><span class="line">		key = <span class="literal">null</span>;</span><br><span class="line">		value = <span class="literal">null</span>;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/c950bb9484784fd090619f3db59e8a5e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/40086164eabc454289ac42ecf1bf88be.png#pic_center" alt="在这里插入图片描述"></p>
<h3 id="3-2-3-优化措施">3.2.3 优化措施</h3>
<p>  由于文件在 HDFS 上进行存储的时候，物理上会进行分块存储，可能会导致文件内容的完整性被破坏。为了避免这个问题，在实际读取 split 数据的时候，每个 maptask 会进行读取行为的调整，具体来说：<br>
  一是：<code>每个maptask都多处理下一个split的第一行数据</code>；</p>
<p><img src="https://img-blog.csdnimg.cn/b61c788ce8214e12a34490a59a3ff8b0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  二是：<code>除了第一个，每个maptask都舍去自己的第一行数据不处理</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/a2481b98276a4ab3aba3582d2e1c5d8b.png#pic_center" alt="在这里插入图片描述"></p>
<h1 id="4-Mapper">4. Mapper</h1>
<p>  mapper 中有 3 个方法，分别是<code>setup初始化方法</code>、<code>map方法</code>、<code>cleanup扫尾清理方法</code>。而 maptask 的业务处理核心是在 map 方法中定义的。用户可以在自定义的 mapper 中重写父类的 map 方法逻辑。</p>
<p><img src="https://img-blog.csdnimg.cn/9fb6db28d5454841b60161682d083951.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h2 id="4-1-map">4.1 map</h2>
<p>  对于 map 方法，如果用户不重写，父类中也有默认实现逻辑。其逻辑为：输入什么，原封不动的输出什么，也就意味着不对数据进行任何处理。<br>
  此外还要注意，map 方法的调用周期、次数取决于父类中 run 方法。当 LineRecordReader.nextKeyValue 返回 true 时，意味着还有数据，<code>LineRecordReader每读取一行数据，返回一个kv键值对，就调用一次map方法</code>。<br>
  因此得出结论：mapper 阶段默认情况下是基于行处理输入数据的。</p>
<p><img src="https://img-blog.csdnimg.cn/09cb9e66c5c548188d3f80ab0e01495b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h1 id="5-OutputCollector">5. OutputCollector</h1>
<p>  map 方法中根据业务逻辑一行行处理完之后，最终是调用的是 context.write() 方法将结果输出。至于输出的数据到哪里，在 MapTask 类中定义了两种情况，即：MR 程序是否有 Reducer 阶段？<br>
  如果<code>有reducer阶段</code>，则<code>创建输出收集器</code>。<br>
  如果<code>没有reducer阶段</code>，则<code>创建outputFormat</code>，默认实现是<code>TextOutputFormat</code>，直接将处理的结果输出到指定目录文件中。</p>
<p><img src="https://img-blog.csdnimg.cn/faabc3e5333b401baac2e05b5736129a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  我们关注的重点当然是带有 reducer 阶段的 MR 程序，否则程序到此就结束了。<br>
  进入 NewOutputCollector 构造方法，核心方法是<code>createSortingCollector</code>。此外还确定了程序是否需要进行分区以及分区的实现类是什么。</p>
<p><img src="https://img-blog.csdnimg.cn/f24831d01fff4d88ab7194ec0757a5cb.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  在 createSortingCollector 方法内部，核心是创建具体的<code>输出收集器MapOutputBuffer</code>。<br>
  MapOutputBuffer 就是口语中俗称的<code>map输出的缓冲区</code>，即在有 reduce 阶段的情况下，map 的输出结果不是直接写入磁盘的，还是先写入内存的缓冲区中。</p>
<p><img src="https://img-blog.csdnimg.cn/304ee5fa7d7349aeb2511a38e5678522.png#pic_center" alt="在这里插入图片描述"><br>
  当创建好 MapOutputBuffer 之后，在返回给 MapTask 之前对其进行了 init 初始化。关于初始化的细节我们在环形缓冲区中细说。</p>
<h2 id="5-1-Partitioner">5.1 Partitioner</h2>
<p>  在程序的 mapper 阶段 context.write 打上断点，追踪一下输出的数据进行了哪些操作。</p>
<p><img src="https://img-blog.csdnimg.cn/98db01da9f00476d9410d5de303e9004.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  不断进入发现，最终调用的是 MapTask 中的 Write 方法。Write 方法中把输出的数据 kv 通过收集器写入了环形缓冲区，在写入之前这里还进行了<code>数据分区计算</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/6241047ab0d64eef816c10070e591f61.png#pic_center" alt="在这里插入图片描述"><br>
  partitioner.getPartition(key, value, partitions) 就是计算每个 mapper 的输出分区编号是多少。注意，只有当<code>reducetask&gt;1</code>的时候。才会进行分区的计算。<br>
  默认的分区器在 JobContextImpl 中定义，是<code>HashPartitioner</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/5156a4aa356e476dbc412d1dd359a9c4.png#pic_center" alt="在这里插入图片描述"><br>
  默认的分区规则也很简单：<code>key.hashCode() % numReduceTasks</code><br>
  <code>为了避免hashcode值为负数，通过和Integer最大值进行与计算修正hashcode为正</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/b03eabb5f8d04d7fbf71d4f75afcef61.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h2 id="5-2-Circular-buffer">5.2 Circular buffer</h2>
<h3 id="5-2-1-环形缓冲区概念及意义">5.2.1 环形缓冲区概念及意义</h3>
<p>  <code>环形缓冲区（Circular buffer）</code>的环形是一个抽象概念。缓冲区的作用是批量收集 mapper 的输出结果，减少磁盘 IO 的影响。想一下，一个一个写和一个批次一个批次写，哪种效率高？<br>
  环形缓冲区本质是 byte 数组，里面存放着<code>key、value的序列化数据</code>和<code>key、value的元数据信息</code>。<br>
  其中<code>kvbuffer</code>字节数组存储真正的 kv 数据，<code>kvmeta</code>存储对应的元数据。<br>
  每个 key/value 对应一个元数据，元数据由 4 个 int 组成：第一个 int 存放 value 的起始位置，第二个存放 key 的起始位置，第三个存放 partition，最后一个存放 value 的长度。<br>
  因为 key/value 写入 kvbuffer 时是要经过序列化的，所以我们要记录每一个 key 和 value 序列化后在 kvbuffer 中的起始和终止位置。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//aw:存储元数据信息 注意这是一个intbuffer</span></span><br><span class="line"><span class="keyword">private</span> IntBuffer kvmeta; </span><br><span class="line"></span><br><span class="line"><span class="comment">//分割标识，因为meta数据和key value内容都存放在同一个环形缓冲区，所以需要分隔开</span></span><br><span class="line"><span class="type">int</span> equator; </span><br><span class="line"></span><br><span class="line"><span class="comment">//aw:内存缓冲区的核心 存储key value序列化之后的数据 注意是字节数组</span></span><br><span class="line"><span class="type">byte</span>[] kvbuffer; </span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * aw:一个key/value键值对对应一条元数据，一条元数据由4个int组成。</span></span><br><span class="line"><span class="comment"> *    第一个存放value的起始位置(VALSTART)</span></span><br><span class="line"><span class="comment"> *    第二个存放key的起始位置(KEYSTART)</span></span><br><span class="line"><span class="comment"> *    第三个存放partition(PARTITION)</span></span><br><span class="line"><span class="comment"> *    第四个存放value的长度(VALLEN)</span></span><br><span class="line"><span class="comment"> * 以此类推，然后下面4个int是下一个kv的元数据。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">VALSTART</span> <span class="operator">=</span> <span class="number">0</span>;         <span class="comment">// val offset in acct</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">KEYSTART</span> <span class="operator">=</span> <span class="number">1</span>;         <span class="comment">// key offset in acct</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">PARTITION</span> <span class="operator">=</span> <span class="number">2</span>;        <span class="comment">// partition offset in acct</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">VALLEN</span> <span class="operator">=</span> <span class="number">3</span>;           <span class="comment">// length of value</span></span><br></pre></td></tr></table></figure>
<p>  key/value 序列化的数据和元数据在环形缓冲区中的存储是由 equator（赤道）分隔的。<br>
  key/value 按照索引递增的方向存储，meta 则按照索引递减的方向存储，将其数组抽象为一个环形结构之后，以 equator 为界，key/value 顺时针存储，meta 逆时针存储。</p>
<p><img src="https://img-blog.csdnimg.cn/afd6a63fedce4c338e00b889de97f002.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  环形缓冲区是有大小限制，默认是<code>100MB</code>。由参数<code>mapreduce.task.io.sort.mb</code>控制。</p>
<h3 id="5-2-2-环形缓冲区的初始化">5.2.2 环形缓冲区的初始化</h3>
<p>  在 MapTask 中创建 OutputCollector 的时候，对环形缓冲区进行了初始化的动作。</p>
<p><img src="https://img-blog.csdnimg.cn/01a92de8c8854811a57780bb554b30c0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  初始化的过程中，主要是构造环形缓冲区的抽象数据结构。包括不限于：<code>设置缓冲区大小、溢出比、初始化kvbuffer|kvmeta、设置Equator标识分界线、构造排序的实现类、combiner、压缩编码等</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/ef81a3e71d894295bac996f2b19d5d57.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/e6f7a9fea7cf4658a7955de19a173238.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/fc6fed8904744a9dbbac24922e2ea2af.png#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/b248b3161721447ca60b23bb811e687c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h3 id="5-2-3-环形缓冲区的数据收集">5.2.3 环形缓冲区的数据收集</h3>
<p>  mapper 的 map 方法处理完数据之后，是调用 context.write 方法将结果进行输出。debug 不断进入发现，最终调用的是 MapTask 中的 Write 方法。</p>
<p><img src="https://img-blog.csdnimg.cn/8a43d99c994846b4969ac544b0b940dc.png#pic_center" alt="在这里插入图片描述"><br>
  在 write 方法中，调用<code>collector.collect</code>向环形缓冲区中写入数据，数据写入之前也进行了分区 partition 计算。在有 reducer 阶段的情况下，<code>collector的实现是MapOutputBuffer</code>。<br>
  收集数据到环形缓冲区核心逻辑有：<code>序列化key到字节数组，序列化value到字节数组，写入该条数据的元数据（起始位置、partition、长度）、更新kvindex</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/985d39ea430f4419aa731637063748ba.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/7db133a5fd594925b7e7e2e90addccc9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h2 id="5-3-Spill、Sort">5.3 Spill、Sort</h2>
<h3 id="5-3-1-Spill溢写">5.3.1 Spill溢写</h3>
<p>  环形缓冲区虽然可以减少 IO 次数，但是总归有容量限制，不能把所有数据一直写入内存，数据最终还是要落入磁盘上存储的，所以需要在一定条件下将缓冲区中的数据临时写入磁盘，然后重新利用这块缓冲区。这个从<code>内存往磁盘写数据的过程被称为Spill，中文可译为溢写</code>。<br>
  这个溢写是由单独线程来完成，不影响往缓冲区继续写数据。整个缓冲区有个溢写的比例 spill.percent。这个比例默认是<code>0.8</code>，也就是当缓冲区的数据已经达到阈值（buffer size * spill percent = 100MB * 0.8 = <code>80MB</code>），spill 线程启动。<br>
  <code>spill线程是由startSpill()方法唤醒的</code>，在进行 spill 操作的时候，此时 map 向 buffer 的写入操作并没有阻塞，剩下 20M 可以继续使用。</p>
<p><img src="https://img-blog.csdnimg.cn/10eea08c2a84428583153ba40c015039.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/39f5f634e2e84673822f628004fe3b5d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  溢写的线程叫做<code>SpillThread</code>，查看其 run 方法，run 中主要是 sortAndSpill。</p>
<p><img src="https://img-blog.csdnimg.cn/c05708a5545a4c9b929d227f063444e0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  创建溢写记录（索引）、溢写文件（该文件位于机器的本地文件系统，而不是 HDFS）。<br>
  <code>SpillThread</code>输出的每个 spill 文件都有一个索引，其中包含有关每个文件中分区的信息-分区的开始位置和结束位置。这些索引存储在内存中，叫做<code>SpillRecord</code>溢写记录，可使用内存量为<code>mapreduce.task.index.cache.limit.bytes</code>，默认情况下等于 1MB。<br>
  如果不足以将索引存储在内存中，则所有下一个创建的溢出文件的索引都将与溢出文件一起写入磁盘。</p>
<p><img src="https://img-blog.csdnimg.cn/46de25c3826b4bb3b464897b4c8e65cf.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/3260141a1f4e467186fd9dcae1f24b8c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  溢写数据到临时文件中：</p>
<p><img src="https://img-blog.csdnimg.cn/58386c004fc44c4fbb06b3fbc050a400.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  更新 spillRec：</p>
<p><img src="https://img-blog.csdnimg.cn/52e4b6d80a444adcb2bd8621f3b07015.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  将内存中的 spillRec 写入磁盘变成索引文件。</p>
<p><img src="https://img-blog.csdnimg.cn/48c423b98107497386afa8be90c43c58.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  在 spill 的同时，map 往 buffer 的写操作并没有停止，依然在调用 collect。满足条件继续 spill，以此往复。</p>
<h3 id="5-3-2-Sort排序">5.3.2 Sort排序</h3>
<p>  在溢写的过程中，会<code>对数据进行排序</code>。<br>
  排序规则是 MapOutputBuffer.compare，采用的是<code>QuickSort快速排序</code>。<br>
  先对 partition 进行排序其次对 key 值排序。这样，<code>数据按分区排序，并且在每个分区内按键对数据排序</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/4b31ba6545cd4739890d4e53ccb5def4.png#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/6fd75cf76261487b931b40ecf79cac64.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h2 id="5-4-Merge">5.4 Merge</h2>
<p>  每次 spill 都会在磁盘上生成一个临时文件，如果 map 的输出结果真的很大，有多次这样的 spill 发生，磁盘上相应的就会有多个临时文件存在。这样将不利于 reducetask 处理数据。<br>
  当 mapper 和最后一次溢出都结束时，溢出线程终止，<code>合并（merge）</code>阶段开始。<br>
  在合并阶段，应<code>将所有溢出文件合并在一起以形成一个map输出文件</code>。<br>
  默认情况下，一个合并过程最多可以处理 100 个溢出文件（负责此操作的参数是<code>mapreduce.task.io.sort.factor</code>）。如果超过，将进行多次 merge 合并。<br>
  最终一个 maptask 的结果是一个输出文件，其中包含 map 的所有输出数据以及索引文件，索引文件描述了 ReduceTask 的分区开始-停止信息，以便能够轻松获取与其将运行的相关分区数据。</p>
<h1 id="6-Combiner">6. Combiner</h1>
<p>  <code>Combiner（规约）</code>的作用就是对 map 端的输出先做一次合并，以减少在 map 和 reduce 节点之间的数据传输量，以提高网络 IO 性能，是 MapReduce 的一种优化手段之一。<code>默认情况下不开启</code>。<br>
  当 job 设置了 Combiner，可能会在 spill 和 merge 的两个阶段执行。<br>
  spill 时 combiner 执行情况源码：</p>
<p><img src="https://img-blog.csdnimg.cn/07a50caeb15b4e11b00840da918cc29b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>
  merge 时 combiner 执行情况源码：MapTask 中搜 mergeParts 方法。</p>
<p><img src="https://img-blog.csdnimg.cn/18fea2e322bd4e0ca6fbd03fb08f939f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5L2N5pyo5bim5oSf5oOF55qE56CB5Yac,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://www.progrem-park.top/">一位木带感情的码农</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://www.progrem-park.top/2022/02/06/hadoop_32/">https://www.progrem-park.top/2022/02/06/hadoop_32/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">此文章版权归 <a href=https://www.progrem-park.top/>程序园</a> 所有，如有转载，请注明来自原作者。</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a></div><div class="post_share"><div class="social-share" data-image="/img/hadoop/1.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/02/09/hadoop_33/"><img class="prev-cover" src="/img/hadoop/1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Hadoop生态圈（二十九）- MapReduce Reduce 阶段核心源码分析</div></div></a></div><div class="next-post pull-right"><a href="/2022/02/06/hadoop_31/"><img class="next-cover" src="/img/hadoop/1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Hadoop 生态圈（二十七）- MapReduce Job 提交源码分析</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/01/11/hadoop_1/" title="Hadoop 3.x 在 centos 上的完全分布式部署（包括免密登录、集群测试、历史服务器、日志聚集、常用命令、群起脚本）"><img class="cover" src="/img/hadoop/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-11</div><div class="title">Hadoop 3.x 在 centos 上的完全分布式部署（包括免密登录、集群测试、历史服务器、日志聚集、常用命令、群起脚本）</div></div></a></div><div><a href="/2022/01/17/hadoop_10/" title="Hadoop 生态圈（七）- HDFS 优化方案"><img class="cover" src="/img/hadoop/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-17</div><div class="title">Hadoop 生态圈（七）- HDFS 优化方案</div></div></a></div><div><a href="/2022/01/17/hadoop_11/" title="ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain"><img class="cover" src="/img/hadoop/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-17</div><div class="title">ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain</div></div></a></div><div><a href="/2022/01/17/hadoop_12/" title="Hadoop 生态圈（八）- HDFS 动态节点管理"><img class="cover" src="/img/hadoop/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-17</div><div class="title">Hadoop 生态圈（八）- HDFS 动态节点管理</div></div></a></div><div><a href="/2022/01/19/hadoop_13/" title="Hadoop 生态圈（九）- HDFS High Availability（HA）高可用集群"><img class="cover" src="/img/hadoop/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-19</div><div class="title">Hadoop 生态圈（九）- HDFS High Availability（HA）高可用集群</div></div></a></div><div><a href="/2022/01/19/hadoop_15/" title="Hadoop 生态圈（十一）- HDFS 集群滚动升级"><img class="cover" src="/img/hadoop/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-19</div><div class="title">Hadoop 生态圈（十一）- HDFS 集群滚动升级</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatat_img.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">一位木带感情的码农</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">200</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">35</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/program-park"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/program-park" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:lkm869666@126.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/weixin_44758876" target="_blank" title="CSDN"><i class="fa-solid fa-c"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站正在优化中......</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Map%E9%98%B6%E6%AE%B5%E6%95%B4%E4%BD%93%E6%A6%82%E8%BF%B0"><span class="toc-text">1. Map阶段整体概述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E5%89%8D%E7%BD%AE%EF%BC%9A%E8%A7%A3%E8%AF%BBMapTask%E7%B1%BB"><span class="toc-text">2. 前置：解读MapTask类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E7%AC%AC%E4%B8%80%E5%B1%82%E8%B0%83%E7%94%A8%EF%BC%88run%EF%BC%89"><span class="toc-text">2.1 第一层调用（run）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-1-map%E9%98%B6%E6%AE%B5%E7%9A%84%E4%BB%BB%E5%8A%A1%E5%88%92%E5%88%86"><span class="toc-text">2.1.1 map阶段的任务划分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2-%E8%BF%90%E8%A1%8CMapper"><span class="toc-text">2.1.2 运行Mapper</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E7%AC%AC%E4%BA%8C%E5%B1%82%E8%B0%83%E7%94%A8%EF%BC%88runNewMapper%EF%BC%89%E5%87%86%E5%A4%87%E9%83%A8%E5%88%86"><span class="toc-text">2.2 第二层调用（runNewMapper）准备部分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-TaskContext"><span class="toc-text">2.2.1 TaskContext</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-split"><span class="toc-text">2.2.2 split</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-3-inputFormat"><span class="toc-text">2.2.3 inputFormat</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-4-RecordReader"><span class="toc-text">2.2.4 RecordReader</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-5-mapper"><span class="toc-text">2.2.5 mapper</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-6-OutputCollector"><span class="toc-text">2.2.6 OutputCollector</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E7%AC%AC%E4%BA%8C%E5%B1%82%E8%B0%83%E7%94%A8%EF%BC%88runNewMapper%EF%BC%89%E5%B7%A5%E4%BD%9C%E9%83%A8%E5%88%86"><span class="toc-text">2.3 第二层调用（runNewMapper）工作部分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-1-RecoderReader-initialize"><span class="toc-text">2.3.1 RecoderReader.initialize</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-2-mapper-run"><span class="toc-text">2.3.2 mapper.run</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-3-NewOutputCollector"><span class="toc-text">2.3.3 NewOutputCollector</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-InputFormat"><span class="toc-text">3. InputFormat</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-getSplits"><span class="toc-text">3.1 getSplits</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-bytesRemaining"><span class="toc-text">3.1.1 bytesRemaining</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-createRecordReader"><span class="toc-text">3.2 	createRecordReader</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-initialize"><span class="toc-text">3.2.1 initialize</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-nextKeyValue"><span class="toc-text">3.2.2 nextKeyValue</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-%E4%BC%98%E5%8C%96%E6%8E%AA%E6%96%BD"><span class="toc-text">3.2.3 优化措施</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Mapper"><span class="toc-text">4. Mapper</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-map"><span class="toc-text">4.1 map</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-OutputCollector"><span class="toc-text">5. OutputCollector</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-Partitioner"><span class="toc-text">5.1 Partitioner</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-Circular-buffer"><span class="toc-text">5.2 Circular buffer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-1-%E7%8E%AF%E5%BD%A2%E7%BC%93%E5%86%B2%E5%8C%BA%E6%A6%82%E5%BF%B5%E5%8F%8A%E6%84%8F%E4%B9%89"><span class="toc-text">5.2.1 环形缓冲区概念及意义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-2-%E7%8E%AF%E5%BD%A2%E7%BC%93%E5%86%B2%E5%8C%BA%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">5.2.2 环形缓冲区的初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-3-%E7%8E%AF%E5%BD%A2%E7%BC%93%E5%86%B2%E5%8C%BA%E7%9A%84%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86"><span class="toc-text">5.2.3 环形缓冲区的数据收集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-Spill%E3%80%81Sort"><span class="toc-text">5.3 Spill、Sort</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-1-Spill%E6%BA%A2%E5%86%99"><span class="toc-text">5.3.1 Spill溢写</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-2-Sort%E6%8E%92%E5%BA%8F"><span class="toc-text">5.3.2 Sort排序</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-Merge"><span class="toc-text">5.4 Merge</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Combiner"><span class="toc-text">6. Combiner</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/08/30/reptile_1/" title="Python 爬虫基础之 urllib 库的深入使用详解"><img src="/img/reptile/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python 爬虫基础之 urllib 库的深入使用详解"/></a><div class="content"><a class="title" href="/2022/08/30/reptile_1/" title="Python 爬虫基础之 urllib 库的深入使用详解">Python 爬虫基础之 urllib 库的深入使用详解</a><time datetime="2022-08-30T09:15:02.000Z" title="发表于 2022-08-30 17:15:02">2022-08-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/15/linux_6/" title="Linux 防火墙常用命令总结"><img src="/img/linux/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Linux 防火墙常用命令总结"/></a><div class="content"><a class="title" href="/2022/08/15/linux_6/" title="Linux 防火墙常用命令总结">Linux 防火墙常用命令总结</a><time datetime="2022-08-15T07:20:44.000Z" title="发表于 2022-08-15 15:20:44">2022-08-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/31/linux_5/" title="Linux 基础命令之 tar 解压缩详解"><img src="/img/linux/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Linux 基础命令之 tar 解压缩详解"/></a><div class="content"><a class="title" href="/2022/07/31/linux_5/" title="Linux 基础命令之 tar 解压缩详解">Linux 基础命令之 tar 解压缩详解</a><time datetime="2022-07-31T00:22:50.000Z" title="发表于 2022-07-31 08:22:50">2022-07-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/13/nginx_16/" title="Nginx 从入门到入坟（十五）- Nginx + Tomcat 部署实现动静分离"><img src="/img/nginx/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Nginx 从入门到入坟（十五）- Nginx + Tomcat 部署实现动静分离"/></a><div class="content"><a class="title" href="/2022/07/13/nginx_16/" title="Nginx 从入门到入坟（十五）- Nginx + Tomcat 部署实现动静分离">Nginx 从入门到入坟（十五）- Nginx + Tomcat 部署实现动静分离</a><time datetime="2022-07-13T07:33:23.000Z" title="发表于 2022-07-13 15:33:23">2022-07-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/04/nginx_15/" title="Nginx 从入门到入坟（十四）- Nginx 缓存深入研究"><img src="/img/nginx/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Nginx 从入门到入坟（十四）- Nginx 缓存深入研究"/></a><div class="content"><a class="title" href="/2022/07/04/nginx_15/" title="Nginx 从入门到入坟（十四）- Nginx 缓存深入研究">Nginx 从入门到入坟（十四）- Nginx 缓存深入研究</a><time datetime="2022-07-04T04:20:02.000Z" title="发表于 2022-07-04 12:20:02">2022-07-04</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/9.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2022 By 一位木带感情的码农</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'https://program-park.github.io/2022/02/06/hadoop_32/'
    this.page.identifier = '/2022/02/06/hadoop_32/'
    this.page.title = 'Hadoop生态圈（二十八）- MapReduce Map 阶段核心源码分析'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }

  document.getElementById('darkmode').addEventListener('click', () => {
    setTimeout(() => window.disqusReset(), 200)
  })
}

if ('Valine' === 'Disqus' || !false) {
  if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>