<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Flink：从入门到放弃 | 程序园</title><meta name="keywords" content="Flink,大数据"><meta name="author" content="一位木带感情的码农"><meta name="copyright" content="一位木带感情的码农"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Flink 新手入门教程，从安装部署到原理实现，再到流批一体 API、时间语义、Table API、Flink CEP 等等，全文使用 Scala 编程。">
<meta property="og:type" content="article">
<meta property="og:title" content="Flink：从入门到放弃">
<meta property="og:url" content="https://program-park.github.io/2021/05/19/flink_1/index.html">
<meta property="og:site_name" content="程序园">
<meta property="og:description" content="Flink 新手入门教程，从安装部署到原理实现，再到流批一体 API、时间语义、Table API、Flink CEP 等等，全文使用 Scala 编程。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://program-park.github.io/img/flink/1.png">
<meta property="article:published_time" content="2021-05-19T09:22:47.000Z">
<meta property="article:modified_time" content="2022-09-02T07:30:52.858Z">
<meta property="article:author" content="一位木带感情的码农">
<meta property="article:tag" content="Flink">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://program-park.github.io/img/flink/1.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://program-park.github.io/2021/05/19/flink_1/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":350},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":30,"languages":{"author":"作者: 一位木带感情的码农","link":"链接: ","source":"来源: 程序园","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Flink：从入门到放弃',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-02 15:30:52'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatat_img.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">38</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/flink/1.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">程序园</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Flink：从入门到放弃</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-05-19T09:22:47.000Z" title="发表于 2021-05-19 17:22:47">2021-05-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-02T07:30:52.858Z" title="更新于 2022-09-02 15:30:52">2022-09-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Flink/">Flink</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">33.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>126分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Flink：从入门到放弃"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="一、Flink简介">一、Flink简介</h1>
<p><strong>Apache Flink 是一个<font color=red>框架</font>和<font color=red>分布式</font>处理引擎，用于对<font color=red>无界</font>和<font color=red>有界数据流</font>进行<font color=red>状态</font>计算。</strong></p>
<h2 id="1-Flink组件栈">1. Flink组件栈</h2>
<p>每一层所包含的组件都提供了特定的抽象，用来服务于上层组件：<br>
<img src="https://img-blog.csdnimg.cn/20210416153047694.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<strong>物理部署层：</strong> Flink 支持本地运行、能在独立集群或者在被 YARN 管理的集群上运行， 也能部署在云上，该层主要涉及Flink的部署模式，目前Flink支持多种部署模式：本地、集群(Standalone、YARN)、云(GCE/EC2)、Kubenetes。Flink能够通过该层能够支持不同平台的部署，用户可以根据需要选择使用对应的部署模式。<br>
<strong>Runtime核心层：</strong> Runtime层提供了支持Flink计算的全部核心实现，为上层API层提供基础服务，该层主要负责对上层不同接口提供基础服务，也是Flink分布式计算框架的核心实现层，支持分布式Stream作业的执行、JobGraph到ExecutionGraph的映射转换、任务调度等。将DataSteam和DataSet转成统一的可执行的Task Operator，达到在流式引擎下同时处理批量计算和流式计算的目的。<br>
<strong>API&amp;Libraries层：</strong> Flink 首先支持了 Scala 和 Java 的 API，Python 也正在测试中。DataStream、DataSet、Table、SQL API，作为分布式数据处理框架，Flink同时提供了支撑计算和批计算的接口，两者都提供给用户丰富的数据处理高级API，例如Map、FlatMap操作等，也提供比较低级的Process Function API，用户可以直接操作状态和时间等底层数据。<br>
<strong>扩展库：</strong> Flink 还包括用于复杂事件处理的CEP，机器学习库FlinkML，图处理库Gelly等。Table 是一种接口化的 SQL 支持，也就是 API 支持(DSL)，而不是文本化的SQL 解析和执行。</p>
<h2 id="2-Flink基石">2. Flink基石</h2>
<p><img src="https://img-blog.csdnimg.cn/20210416160638126.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<ul>
<li><strong>Checkpoint</strong>
<ul>
<li>Flink基于Chandy-Lamport算法实现了一个分布式的一致性的快照，从而提供了一致性的语义。</li>
</ul>
</li>
<li><strong>State</strong>
<ul>
<li>提供了一致性的语义之后，Flink为了让用户在编程时能够更轻松、更容易地去管理状态，还提供了一套非常简单明了的State API，包括里面的有ValueState、ListState、MapState，近期添加了BroadcastState，使用State API能够自动享受到这种一致性的语义。</li>
</ul>
</li>
<li><strong>Time</strong>
<ul>
<li>Flink还实现了Watermark的机制，能够支持基于事件的时间的处理，能够容忍迟到/乱序的数据。</li>
</ul>
</li>
<li><strong>Window</strong>
<ul>
<li>流计算中一般在对流数据进行操作之前都会先进行开窗，即基于一个什么样的窗口上做这个计算。Flink提供了开箱即用的各种窗口，比如滑动窗口、滚动窗口、会话窗口以及非常灵活的自定义的窗口。</li>
</ul>
</li>
</ul>
<h2 id="3-Fink的应用场景">3. Fink的应用场景</h2>
<h3 id="3-1-Event-driven-Applications【事件驱动】">3.1 Event-driven Applications【事件驱动】</h3>
<p><strong>事件驱动型应用是一类具有状态的应用，它从一个或多个事件流提取数据，并根据到来的事件触发计算、状态更新或其他外部动作。</strong> 比较典型的就是以 kafka 为代表的消息队列几乎都是事件驱动型应用。<br>
事件驱动型应用是在计算存储分离的传统应用基础上进化而来。<br>
在传统架构中，应用需要读写远程事务型数据库。<br>
相反，事件驱动型应用是基于<strong>状态化流处理</strong>来完成。在该设计中，数据和计算不会分离，应用只需访问本地(内存或磁盘)即可获取数据。<br>
系统容错性的实现依赖于定期向远程持久化存储写入 checkpoint。下图描述了传统应用和事件驱动型应用架构的区别。<br>
<img src="https://img-blog.csdnimg.cn/20210419161855636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
从某种程度上来说，所有的实时的数据处理或者是流式数据处理都应该是属于Data Driven，流计算本质上是Data Driven 计算。应用较多的如风控系统，当风控系统需要处理各种各样复杂的规则时，Data Driven 就会把处理的规则和逻辑写入到Datastream 的API 或者是ProcessFunction 的API 中，然后将逻辑抽象到整个Flink 引擎，当外面的数据流或者是事件进入就会触发相应的规则，这就是Data Driven 的原理。在触发某些规则后，Data Driven 会进行处理或者是进行预警，这些预警会发到下游产生业务通知，这是Data Driven 的应用场景，Data Driven 在应用上更多应用于复杂事件的处理。</p>
<ul>
<li>典型实例：
<ul>
<li>欺诈检测(Fraud detection)</li>
<li>异常检测(Anomaly detection)</li>
<li>基于规则的告警(Rule-based alerting)</li>
<li>业务流程监控(Business process monitoring)</li>
<li>Web应用程序(社交网络)<br>
<img src="https://img-blog.csdnimg.cn/20210419162232930.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li>
</ul>
</li>
</ul>
<h3 id="3-2-Data-Analytics-Applications【数据分析】">3.2 Data Analytics Applications【数据分析】</h3>
<p><strong>数据分析任务需要从原始数据中提取有价值的信息和指标。</strong><br>
如下图所示，Apache Flink 同时支持流式及批量分析应用。<br>
<img src="https://img-blog.csdnimg.cn/20210419162342556.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
Data Analytics Applications包含<strong>Batch analytics(批处理分析)</strong> 和<strong>Streaming analytics(流处理分析)</strong><br>
Batch analytics可以理解为<strong>周期性查询</strong>：Batch Analytics 就是传统意义上使用类似于Map Reduce、Hive、Spark Batch 等，对作业进行分析、处理、生成离线报表。比如Flink应用凌晨从Recorded Events中读取昨天的数据，然后做周期查询运算，最后将数据写入Database或者HDFS，或者直接将数据生成报表供公司上层领导决策使用。<br>
Streaming analytics可以理解为<strong>连续性查询</strong>：比如实时展示双十一天猫销售GMV(Gross Merchandise Volume成交总额)，用户下单数据需要实时写入消息队列，Flink 应用源源不断读取数据做实时计算，然后不断的将数据更新至Database或者K-VStore，最后做大屏实时展示。</p>
<ul>
<li>典型实例
<ul>
<li>电信网络质量监控</li>
<li>移动应用中的产品更新及实验评估分析</li>
<li>消费者技术中的实时数据即席分析</li>
<li>大规模图分析</li>
</ul>
</li>
</ul>
<h3 id="3-3-Data-Pipeline-Applications【数据管道】">3.3 Data Pipeline Applications【数据管道】</h3>
<p>什么是数据管道？<br>
<strong>提取-转换-加载(ETL)是一种在存储系统之间进行数据转换和迁移的常用方法。</strong><br>
ETL 作业通常会周期性地触发，将数据从事务型数据库拷贝到分析型数据库或数据仓库。<br>
数据管道和 ETL 作业的用途相似，都可以转换、丰富数据，并将其从某个存储系统移动到另一个。<br>
但数据管道是以<strong>持续流</strong>模式运行，而非周期性触发。<br>
因此数据管道支持从一个不断生成数据的源头读取记录，并将它们以低延迟移动到终点。<br>
例如：数据管道可以用来监控文件系统目录中的新文件，并将其数据写入事件日志；另一个应用可能会将事件流物化到数据库或增量构建和优化查询索引。<br>
和周期性 ETL 作业相比，持续数据管道可以明显降低将数据移动到目的端的延迟。<br>
此外，由于它能够持续消费和发送数据，因此用途更广，支持用例更多。<br>
下图描述了周期性ETL作业和持续数据管道的差异。<br>
<img src="https://img-blog.csdnimg.cn/20210419162848396.png#pic_center" alt="在这里插入图片描述"><br>
<strong>Periodic ETL</strong>：比如每天凌晨周期性的启动一个Flink ETL Job，读取传统数据库中的数据，然后做ETL，最后写入数据库和文件系统。<br>
<strong>Data Pipeline</strong>：比如启动一个Flink 实时应用，数据源(比如数据库、Kafka)中的数据不断的通过Flink Data Pipeline流入或者追加到数据仓库(数据库或者文件系统)，或者Kafka消息队列。<br>
Data Pipeline 的核心场景类似于数据搬运并在搬运的过程中进行部分数据清洗或者处理，而整个业务架构图的左边是Periodic ETL，它提供了流式ETL 或者实时ETL，能够订阅消息队列的消息并进行处理，清洗完成后实时写入到下游的Database或File system 中。</p>
<ul>
<li>典型实例
<ul>
<li>电子商务中的持续 ETL(实时数仓)
<ul>
<li>当下游要构建实时数仓时，上游则可能需要实时的Stream ETL。这个过程会进行实时清洗或扩展数据，清洗完成后写入到下游的实时数仓的整个链路中，可保证数据查询的时效性，形成实时数据采集、实时数据处理以及下游的实时Query。</li>
</ul>
</li>
<li>电子商务中的实时查询索引构建(搜索引擎推荐)
<ul>
<li>搜索引擎这块以淘宝为例，当卖家上线新商品时，后台会实时产生消息流，该消息流经过Flink 系统时会进行数据的处理、扩展。然后将处理及扩展后的数据生成实时索引，写入到搜索引擎中。这样当淘宝卖家上线新商品时，能在秒级或者分钟级实现搜索引擎的搜索。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="4-Flink的优点">4. Flink的优点</h2>
<p><img src="https://img-blog.csdnimg.cn/20210419164216526.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<ul>
<li><strong>主要优点</strong>
<ul>
<li><font color=red>Flink 具备统一的框架处理有界和无界两种数据流的能力</font></li>
<li><font color=red>部署灵活，Flink 底层支持多种资源调度器</font>，包括Yarn、Kubernetes 等。Flink 自身带的Standalone 的调度器，在部署上也十分灵活</li>
<li><font color=red>极高的可伸缩性</font>，可伸缩性对于分布式系统十分重要，阿里巴巴双11大屏采用Flink 处理海量数据，使用过程中测得Flink 峰值可达17 亿条/秒。</li>
<li><font color=red>极致的流式处理性能</font>。Flink 相对于Storm 最大的特点是将状态语义完全抽象到框架中，支持本地状态读取，避免了大量网络IO，可以极大提升状态存取的性能。</li>
</ul>
</li>
<li><strong>其他优点</strong>
<ul>
<li><font color=red>同时支持高吞吐、低延迟、高性能</font>
<ul>
<li>Flink 是目前开源社区中唯一一套集高吞吐、低延迟、高性能三者于一身的分布式流式数据处理框架</li>
<li>Spark 只能兼顾高吞吐和高性能特性，无法做到低延迟保障,因为Spark是用批处理来做流处理</li>
<li>Storm 只能支持低延时和高性能特性，无法满足高吞吐的要求</li>
</ul>
</li>
<li><font color=red>支持事件时间(Event Time)概念</font>
<ul>
<li>在流式计算领域中，窗口计算的地位举足轻重，但目前大多数框架窗口计算采用的都是系统时间(Process Time)，也就是事件传输到计算框架处理时，系统主机的当前时间</li>
<li>Flink 能够支持基于事件时间(Event Time)语义进行窗口计算</li>
<li>这种基于事件驱动的机制使得事件即使乱序到达甚至延迟到达，流系统也能够计算出精确的结果，保持了事件原本产生时的时序性，尽可能避免网络传输或硬件系统的影响</li>
</ul>
</li>
<li><font color=red>支持有状态计算</font>
<ul>
<li>Flink1.4开始支持有状态计算</li>
<li>所谓状态就是在流式计算过程中将算子的中间结果保存在内存或者文件系统中，等下一个事件进入算子后可以从之前的状态中获取中间结果，计算当前的结果，从而无须每次都基于全部的原始数据来统计结果，极大的提升了系统性能，状态化意味着应用可以维护随着时间推移已经产生的数据聚合</li>
</ul>
</li>
<li><font color=red>支持高度灵活的窗口(Window)操作</font>
<ul>
<li>Flink 将窗口划分为基于 Time 、Count 、Session、以及Data-Driven等类型的窗口操作，窗口可以用灵活的触发条件定制化来达到对复杂的流传输模式的支持，用户可以定义不同的窗口触发机制来满足不同的需求</li>
</ul>
</li>
<li><font color=red>基于轻量级分布式快照(Snapshot/Checkpoints)的容错机制</font>
<ul>
<li>Flink 能够分布运行在上千个节点上，通过基于分布式快照技术的Checkpoints，将执行过程中的状态信息进行持久化存储，一旦任务出现异常停止，Flink 能够从 Checkpoints 中进行任务的自动恢复，以确保数据处理过程中的一致性</li>
<li>Flink 的容错能力是轻量级的，允许系统保持高并发，同时在相同时间内提供强一致性保证</li>
</ul>
</li>
<li><font color=red>基于 JVM 实现的独立的内存管理</font>
<ul>
<li>Flink 实现了自身管理内存的机制，通过使用散列，索引，缓存和排序有效地进行内存管理，通过序列化/反序列化机制将所有的数据对象转换成二进制在内存中存储，降低数据存储大小的同时，更加有效的利用空间。使其独立于 Java 的默认垃圾收集器，尽可能减少 JVM GC 对系统的影响</li>
</ul>
</li>
<li><font color=red>SavePoints 保存点</font>
<ul>
<li>对于 7 * 24 小时运行的流式应用，数据源源不断的流入，在一段时间内应用的终止有可能导致数据的丢失或者计算结果的不准确（比如集群版本的升级，停机运维操作等）</li>
<li>Flink 通过SavePoints 技术将任务执行的快照保存在存储介质上，当任务重启的时候，可以从事先保存的 SavePoints 恢复原有的计算状态，使得任务继续按照停机之前的状态运行</li>
<li>Flink 保存点提供了一个状态化的版本机制，使得能以无丢失状态和最短停机时间的方式更新应用或者回退历史数据</li>
</ul>
</li>
<li><font color=red>灵活的部署方式，支持大规模集群</font>
<ul>
<li>Flink 被设计成能用上千个点在大规模集群上运行</li>
<li>除了支持独立集群部署外，Flink 还支持 YARN 和Mesos 方式部署</li>
</ul>
</li>
<li><font color=red>Flink 的程序内在是并行和分布式的</font>
<ul>
<li>数据流可以被分区成 stream partitions，operators 被划分为operator subtasks，这些 subtasks 在不同的机器或容器中分不同的线程独立运行</li>
<li>operator subtasks 的数量就是operator的并行计算数，不同的 operator 阶段可能有不同的并行数</li>
</ul>
</li>
<li><font color=red>丰富的库</font>
<ul>
<li>Flink 拥有丰富的库来进行机器学习，图形处理，关系数据处理等</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="5-流处理-批处理">5. 流处理&amp;批处理</h2>
<p><img src="https://img-blog.csdnimg.cn/20210419171037746.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<ul>
<li><strong>Batch Analytics 批量计算：</strong> 统一收集数据-&gt;存储到DB-&gt;对数据进行批量处理，就是传统意义上使用类似于 Map Reduce、Hive、Spark Batch 等，对作业进行分析、处理、生成离线报表</li>
<li><strong>Streaming Analytics 流式计算：</strong> 顾名思义，就是对数据流进行处理，如使用流式分析引擎如 Storm，Flink 实时处理分析数据，应用较多的场景如实时大屏、实时报表</li>
<li><strong>主要区别</strong>
<ul>
<li>与批量计算那样慢慢积累数据不同，流式计算立刻计算，数据持续流动，计算完之后就丢弃</li>
<li>批量计算是维护一张表，对表进行实施各种计算逻辑。流式计算相反，是必须先定义好计算逻辑，提交到流式计算系统，这个计算作业逻辑在整个运行期间是不可更改的</li>
<li>计算结果上，批量计算对全部数据进行计算后传输结果，流式计算是每次小批量计算后，结果可以立刻实时化展现</li>
</ul>
</li>
</ul>
<h2 id="6-流批统一">6. 流批统一</h2>
<p>在大数据处理领域，批处理任务与流处理任务一般被认为是两种不同的任务，一个大数据框架一般会被设计为只能处理其中一种任务：</p>
<ul>
<li>MapReduce只支持批处理任务</li>
<li>Storm只支持流处理任务</li>
<li>Spark Streaming采用micro-batch架构，本质上还是基于Spark批处理对流式数据进行处理</li>
<li>Flink通过灵活的执行引擎，能够同时支持批处理任务与流处理任务<br>
<img src="https://img-blog.csdnimg.cn/20210419171537108.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li>
</ul>
<p>在执行引擎这一层，流处理系统与批处理系统最大不同在于节点间的<strong>数据传输方式</strong>：</p>
<ul>
<li>对于一个流处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，然后立刻通过网络传输到下一个节点，由下一个节点继续处理</li>
<li>对于一个批处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，并不会立刻通过网络传输到下一个节点，当缓存写满，就持久化到本地硬盘上，当所有数据都被处理完成后，才开始将处理后的数据通过网络传输到下一个节点</li>
</ul>
<p>这两种数据传输模式是两个极端，对应的是流处理系统对低延迟的要求和批处理系统对高吞吐量的要求。<br>
<strong>Flink的执行引擎采用了一种十分灵活的方式，同时支持了这两种数据传输模型：</strong></p>
<ul>
<li>Flink以固定的缓存块为单位进行网络数据传输，用户可以通过设置缓存块超时值指定缓存块的传输时机</li>
<li>如果缓存块的超时值为0，则Flink的数据传输方式类似上面所提到流处理系统的标准模型，此时系统可以获得最低的处理延迟</li>
<li>如果缓存块的超时值为无限大/-1，则Flink的数据传输方式类似上文所提到批处理系统的标准模型，此时系统可以获得最高的吞吐量</li>
<li>同时缓存块的超时值也可以设置为0到无限大之间的任意值。缓存块的超时阈值越小，则Flink流处理执行引擎的数据处理延迟越低，但吞吐量也会降低，反之亦然。通过调整缓存块的超时阈值，用户可根据需求灵活地权衡系统延迟和吞吐量</li>
</ul>
<p>默认情况下，流中的元素并不会一个一个的在网络中传输，而是缓存起来伺机一起发送(默认为32KB，通过taskmanager.memory.segment-size设置),这样可以避免导致频繁的网络传输,提高吞吐量，但如果数据源输入不够快的话会导致后续的数据处理延迟，所以可以使用env.setBufferTimeout(默认100ms)，来为缓存填入设置一个最大等待时间。等待时间到了之后，即使缓存还未填满，缓存中的数据也会自动发送</p>
<ul>
<li>timeoutMillis &gt; 0 表示最长等待 timeoutMillis 时间，就会flush</li>
<li>timeoutMillis = 0 表示每条数据都会触发 flush，直接将数据发送到下游，相当于没有Buffer了(避免设置为0，可能导致性能下降)</li>
<li>timeoutMillis = -1 表示只有等到 buffer满了或 CheckPoint的时候，才会flush。相当于取消了 timeout 策略</li>
</ul>
<p><strong>Flink以缓存块为单位进行网络数据传输,用户可以设置缓存块超时时间和缓存块大小来控制缓冲块传输时机,从而控制Flink的延迟性和吞吐量</strong></p>
<h1 id="二、Flink安装部署">二、Flink安装部署</h1>
<h2 id="1-Local本地模式">1. Local本地模式</h2>
<h3 id="1-1-原理">1.1 原理</h3>
<p><img src="https://img-blog.csdnimg.cn/20210420091847434.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<ol>
<li>Flink程序由JobClient进行提交</li>
<li>JobClient将作业提交给JobManager</li>
<li>JobManager负责协调资源分配和作业执行。资源分配完成后，任务将提交给相应的TaskManager</li>
<li>TaskManager启动一个线程以开始执行。TaskManager会向JobManager报告状态更改,如开始执行，正在进行或已完成</li>
<li>作业执行完成后，结果将发送回客户端（JobClient）</li>
</ol>
<h3 id="1-2-操作">1.2 操作</h3>
<ol>
<li>
<p>下载安装包<br>
<a target="_blank" rel="noopener" href="https://archive.apache.org/dist/flink/">https://archive.apache.org/dist/flink/</a></p>
</li>
<li>
<p>上传flink-1.12.0-bin-scala_2.12.tgz到指定目录</p>
</li>
<li>
<p>解压</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf flink-1.12.0-bin-scala_2.12.tgz</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>如果出现权限问题，需要修改权限</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown -R root:root /data/flink-1.12.0</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>改名或创建软链接</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv flink-1.12.0 flink</span><br><span class="line">ln -s /data/flink-1.12.0 /data/flink</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="1-3-测试">1.3 测试</h3>
<ol>
<li>
<p>准备文件/data/words.txt</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /data/words.txt</span><br></pre></td></tr></table></figure>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hello me you her</span><br><span class="line">hello me you</span><br><span class="line">hello me</span><br><span class="line">hello</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>启动Flink本地集群</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/start-cluster.sh</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>使用jps可以查看到下面两个进程</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- TaskManagerRunner</span><br><span class="line">- StandaloneSessionClusterEntrypoint</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>访问Flink的Web UI<br>
<a target="_blank" rel="noopener" href="http://xn--IP-im8ckc:8081/#/overview">http://IP地址:8081/#/overview</a><br>
<img src="https://img-blog.csdnimg.cn/20210420093806515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
slot在Flink里面可以认为是资源组，Flink是通过将任务分成子任务并且将这些子任务分配到slot来并行执行程序</p>
</li>
<li>
<p>执行官方示例</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/flink run /data/flink/examples/batch/WordCount.jar --input /data/words.txt --output /data/out</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>停止Flink</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/start-scala-shell.sh local</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="2-Standalone独立集群模式">2. Standalone独立集群模式</h2>
<h3 id="2-1-原理">2.1 原理</h3>
<p><img src="https://img-blog.csdnimg.cn/20210420094339920.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<ol>
<li>client客户端提交任务给JobManager</li>
<li>JobManager负责申请任务运行所需要的资源并管理任务和资源</li>
<li>JobManager分发任务给TaskManager执行</li>
<li>TaskManager定期向JobManager汇报状态</li>
</ol>
<h3 id="2-2-操作">2.2 操作</h3>
<ol>
<li>集群规划：</li>
</ol>
<ul>
<li>服务器: flink1(Master + Slave): JobManager + TaskManager</li>
<li>服务器: flink2(Slave): TaskManager</li>
<li>服务器: flink3(Slave): TaskManager</li>
</ul>
<ol start="2">
<li>
<p>修改flink-conf.yaml</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /data/flink/conf/flink-conf.yaml</span><br></pre></td></tr></table></figure>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">jobmanager.rpc.address: node1</span><br><span class="line">taskmanager.numberOfTaskSlots: 2</span><br><span class="line">web.submit.enable: true</span><br><span class="line"></span><br><span class="line">#历史服务器</span><br><span class="line">jobmanager.archive.fs.dir: hdfs://flink1:8020/flink/completed-jobs/</span><br><span class="line">historyserver.web.address: flink1</span><br><span class="line">historyserver.web.port: 8082</span><br><span class="line">historyserver.archive.fs.dir: hdfs://flink1:8020/flink/completed-jobs/</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>修改masters</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /data/flink/conf/masters</span><br></pre></td></tr></table></figure>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink1:8081</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>修改slaves</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /data/flink/conf/workers</span><br></pre></td></tr></table></figure>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flink1</span><br><span class="line">flink2</span><br><span class="line">flink3</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>添加HADOOP_CONF_DIR环境变量</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_CONF_DIR=/data/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>分发</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp -r /data/flink flink2:/data/flink</span><br><span class="line">scp -r /data/flink flink3:/data/flink</span><br><span class="line">scp  /etc/profile flink2:/etc/profile</span><br><span class="line">scp  /etc/profile flink3:/etc/profile</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>source</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="2-3-测试">2.3 测试</h3>
<ol>
<li>
<p>启动集群，在flink1上执行如下命令</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/start-cluster.sh</span><br></pre></td></tr></table></figure>
<p>或者单独启动</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/jobmanager.sh ((start|start-foreground) cluster)|stop|stop-all</span><br><span class="line">/data/flink/bin/taskmanager.sh start|start-foreground|stop|stop-all</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>启动历史服务器</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/historyserver.sh start</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>访问Flink UI界面或使用jps查看<br>
<a target="_blank" rel="noopener" href="http://flink1:8081/#/overview">http://flink1:8081/#/overview</a><br>
<a target="_blank" rel="noopener" href="http://flink1:8082/#/overview">http://flink1:8082/#/overview</a><br>
TaskManager界面：可以查看到当前Flink集群中有多少个TaskManager，每个TaskManager的slots、内存、CPU Core是多少<br>
<img src="https://img-blog.csdnimg.cn/20210420100505412.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</li>
<li>
<p>执行官方测试案例</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/flink run  /data/flink/examples/batch/WordCount.jar --input hdfs://flink1:8020/wordcount/input/words.txt --output hdfs://flink1:8020/wordcount/output/result.txt  --parallelism 2</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210420100734236.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</li>
<li>
<p>查看历史日志<br>
<a target="_blank" rel="noopener" href="http://flink1:50070/explorer.html#/flink/completed-jobs">http://flink1:50070/explorer.html#/flink/completed-jobs</a><br>
<a target="_blank" rel="noopener" href="http://flink1:8082/#/overview">http://flink1:8082/#/overview</a></p>
</li>
<li>
<p>停止Flink集群</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/stop-cluster.sh</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="3-Standalone-HA高可用集群模式">3. Standalone-HA高可用集群模式</h2>
<h3 id="3-1-原理">3.1 原理</h3>
<p><img src="https://img-blog.csdnimg.cn/20210420101230750.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
从之前的架构中可以很明显的发现 JobManager 有明显的单点问题(SPOF，single point of failure)。JobManager 肩负着任务调度以及资源分配，一旦 JobManager 出现意外，其后果可想而知。<br>
在 Zookeeper 的帮助下，一个 Standalone的Flink集群会同时有多个活着的 JobManager，其中只有一个处于工作状态，其他处于 Standby 状态。当工作中的 JobManager 失去连接后(如宕机或 Crash)，Zookeeper 会从 Standby 中选一个新的 JobManager 来接管 Flink 集群。</p>
<h3 id="3-2-操作">3.2 操作</h3>
<ol>
<li>集群规划</li>
</ol>
<ul>
<li>服务器: flink1(Master + Slave): JobManager + TaskManager</li>
<li>服务器: flink2(Master + Slave): JobManager + TaskManager</li>
<li>服务器: flink3(Slave): TaskManager</li>
</ul>
<ol start="2">
<li>
<p>启动ZooKeeper</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh status</span><br><span class="line">zkServer.sh stop</span><br><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>启动HDFS</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/hadoop/sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>停止Flink集群</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/stop-cluster.sh</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>修改flink-conf.yaml</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /data/flink/conf/flink-conf.yaml</span><br></pre></td></tr></table></figure>
<p>增加如下内容</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#开启HA，使用文件系统作为快照存储</span><br><span class="line">state.backend: filesystem</span><br><span class="line">#启用检查点，可以将快照保存到HDFS</span><br><span class="line">state.backend.fs.checkpointdir: hdfs://flink1:8020/flink-checkpoints</span><br><span class="line">#使用zookeeper搭建高可用</span><br><span class="line">high-availability: zookeeper</span><br><span class="line"># 存储JobManager的元数据到HDFS</span><br><span class="line">high-availability.storageDir: hdfs://flink1:8020/flink/ha/</span><br><span class="line"># 配置ZK集群地址</span><br><span class="line">high-availability.zookeeper.quorum: flink1:2181,flink2:2181,flink3:2181</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>修改masters</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /data/flink/conf/masters</span><br></pre></td></tr></table></figure>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">flink1:8081</span><br><span class="line">flink2:8081</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>同步</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp -r /data/flink/conf/flink-conf.yaml flink2:/data/flink/conf/</span><br><span class="line">scp -r /data/flink/conf/flink-conf.yaml flink3:/data/flink/conf/</span><br><span class="line">scp -r /data/flink/conf/masters flink2:/data/flink/conf/</span><br><span class="line">scp -r /data/flink/conf/masters flink3:/data/flink/conf/</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>修改flink2上的flink-conf.yaml</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /data/flink/conf/flink-conf.yaml</span><br></pre></td></tr></table></figure>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jobmanager.rpc.address: flink2</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>重新启动Flink集群,flink1上执行</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/stop-cluster.sh</span><br><span class="line">/data/flink/bin/start-cluster.sh</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>使用jps命令查看<br>
发现没有Flink相关进程被启动</p>
</li>
<li>
<p>查看日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /data/flink/log/flink-root-standalonesession-0-node1.log</span><br></pre></td></tr></table></figure>
<p>发现如下错误<img src="https://img-blog.csdnimg.cn/20210420103648464.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
因为在Flink1.8版本后,Flink官方提供的安装包里没有整合HDFS的jar</p>
</li>
<li>
<p>下载jar包并在Flink的lib目录下放入该jar包并分发使Flink能够支持对Hadoop的操作<br>
<a target="_blank" rel="noopener" href="https://flink.apache.org/downloads.html">https://flink.apache.org/downloads.html</a><br>
<img src="https://img-blog.csdnimg.cn/20210420103809956.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
依次放入lib目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /data/flink/lib</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>重新启动Flink集群，flink1上执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/start-cluster.sh</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>使用jps命令查看,发现三台机器服务已启动</p>
</li>
</ol>
<h3 id="3-3-测试">3.3 测试</h3>
<ol>
<li>
<p>访问WebUI<br>
<a target="_blank" rel="noopener" href="http://flink1:8081/#/job-manager/config">http://flink1:8081/#/job-manager/config</a><br>
<a target="_blank" rel="noopener" href="http://flink2:8081/#/job-manager/config">http://flink2:8081/#/job-manager/config</a></p>
</li>
<li>
<p>执行wordcount</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/flink run  /data/flink/examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>kill掉其中一个master</p>
</li>
<li>
<p>重新执行wc,还是可以正常执行</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/flink run  /data/flink/examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>停止集群</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/stop-cluster.sh</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="4-Flink-On-Yarn模式">4. Flink On Yarn模式</h2>
<h3 id="4-1-原理">4.1 原理</h3>
<p>在实际开发中，使用Flink时，更多的使用方式是Flink On Yarn模式，原因如下：</p>
<ul>
<li>Yarn的资源可以按需使用，提高集群的资源利用率</li>
<li>Yarn的任务有优先级，根据优先级运行作业</li>
<li>基于Yarn调度系统，能够自动化地处理各个角色的 Failover(容错)
<ul>
<li>JobManager 进程和 TaskManager 进程都由 Yarn NodeManager 监控</li>
<li>如果 JobManager 进程异常退出，则 Yarn ResourceManager 会重新调度 JobManager 到其他机器</li>
<li>如果 TaskManager 进程异常退出，JobManager 会收到消息并重新向 Yarn ResourceManager 申请资源，重新启动 TaskManager</li>
</ul>
</li>
</ul>
<h4 id="4-1-1-Flink如何和Yarn进行交互">4.1.1 Flink如何和Yarn进行交互</h4>
<p><img src="https://img-blog.csdnimg.cn/20210420105320520.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210420105324842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<ol>
<li>Client上传jar包和配置文件到HDFS集群上</li>
<li>Client向Yarn ResourceManager提交任务并申请资源</li>
<li>ResourceManager分配Container资源并启动ApplicationMaster,然后AppMaster加载Flink的Jar包和配置构建环境,启动JobManager<br>
JobManager和ApplicationMaster运行在同一个container上。<br>
一旦他们被成功启动，AppMaster就知道JobManager的地址(AM它自己所在的机器)。<br>
它就会为TaskManager生成一个新的Flink配置文件(他们就可以连接到JobManager)。<br>
这个配置文件也被上传到HDFS上。<br>
此外，AppMaster容器也提供了Flink的web服务接口。<br>
YARN所分配的所有端口都是临时端口，这允许用户并行执行多个Flink</li>
<li>ApplicationMaster向ResourceManager申请工作资源,NodeManager加载Flink的Jar包和配置构建环境并启动TaskManager</li>
<li>TaskManager启动后向JobManager发送心跳包，并等待JobManager向其分配任务</li>
</ol>
<h4 id="4-1-2-两种方式">4.1.2 两种方式</h4>
<h5 id="4-1-2-1-Session模式">4.1.2.1 Session模式</h5>
<p><img src="https://img-blog.csdnimg.cn/20210420105510454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210420105514949.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<strong>特点：</strong> 需要事先申请资源，启动JobManager和TaskManger<br>
<strong>优点：</strong> 不需要每次递交作业申请资源，而是使用已经申请好的资源，从而提高执行效率<br>
<strong>缺点：</strong> 作业执行完成以后，资源不会被释放，因此一直会占用系统资源<br>
<strong>应用场景：</strong> 适合作业递交比较频繁的场景，小作业比较多的场景</p>
<h5 id="4-1-2-2-Per-Job模式">4.1.2.2 Per-Job模式</h5>
<p><img src="https://img-blog.csdnimg.cn/20210420105630609.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210420105634918.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<strong>特点：</strong> 每次递交作业都需要申请一次资源<br>
<strong>优点：</strong> 作业运行完成，资源会立刻被释放，不会一直占用系统资源<br>
<strong>缺点：</strong> 每次递交作业都需要申请资源，会影响执行效率，因为申请资源需要消耗时间<br>
<strong>应用场景：</strong> 适合作业比较少的场景、大作业的场景</p>
<h3 id="4-2-操作">4.2 操作</h3>
<ol>
<li>
<p>关闭yarn的内存检查</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /data/hadoop/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure>
<p>添加：</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 关闭yarn内存检查 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>说明:<br>
是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true。<br>
在这里面我们需要关闭，因为对于flink使用yarn模式下，很容易内存超标，这个时候yarn会自动杀掉job</p>
</li>
<li>
<p>同步</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /data/hadoop/etc/hadoop/yarn-site.xml flink2:/data/hadoop/etc/hadoop/yarn-site.xml</span><br><span class="line">scp -r /data/hadoop/etc/hadoop/yarn-site.xml flink3:/data/hadoop/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>重启yarn</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/data/hadoop/sbin/stop-yarn.sh</span><br><span class="line">/data/hadoop/sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="4-3-测试">4.3 测试</h3>
<h4 id="4-3-1-Session模式">4.3.1 Session模式</h4>
<ol>
<li>
<p>在yarn上启动一个Flink会话，flink1上执行以下命令</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/yarn-session.sh -n 2 -tm 800 -s 1 -d</span><br></pre></td></tr></table></figure>
<p>说明：申请2个CPU、1600M内存<br>
-n 表示申请2个容器，这里指的就是多少个taskmanager<br>
-tm 表示每个TaskManager的内存大小<br>
-s 表示每个TaskManager的slots数量<br>
-d 表示以后台程序方式运行</p>
</li>
<li>
<p>查看UI界面<br>
<a target="_blank" rel="noopener" href="http://flink1:8088/cluster">http://flink1:8088/cluster</a><br>
<img src="https://img-blog.csdnimg.cn/20210420114840343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</li>
<li>
<p>使用flink run提交任务</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/flink run  /data/flink/examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure>
<p>运行完之后可以继续运行其他的小任务</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/flink run /data/flink/examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>通过上方的ApplicationMaster可以进入Flink的管理界面<br>
<img src="https://img-blog.csdnimg.cn/20210420115043539.png" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/2021042011504793.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</li>
<li>
<p>关闭yarn-session</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn application -kill application_1599402747874_0001</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/202104201151345.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /tmp/.yarn-properties-root</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="4-3-2-Per-Job分离模式">4.3.2 Per-Job分离模式</h4>
<ol>
<li>
<p>直接提交job</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/data/flink/bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 </span><br><span class="line">/data/flink/examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure>
<p>-m  jobmanager的地址<br>
-yjm 1024 指定jobmanager的内存信息<br>
-ytm 1024 指定taskmanager的内存信息</p>
</li>
<li>
<p>查看UI界面<br>
<a target="_blank" rel="noopener" href="http://flink1:8088/cluster">http://flink1:8088/cluster</a><br>
<img src="https://img-blog.csdnimg.cn/20210420115506284.png" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210420115515915.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</li>
<li>
<p>注意<br>
在之前版本中如果使用的是flink on yarn方式，想切换回standalone模式的话，如果报错需要删除：【/tmp/.yarn-properties-root】</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /tmp/.yarn-properties-root</span><br></pre></td></tr></table></figure>
<p>因为默认查找当前yarn集群中已有的yarn-session信息中的jobmanager</p>
</li>
</ol>
<h1 id="三、Flink入门案例">三、Flink入门案例</h1>
<h2 id="1-前置说明">1. 前置说明</h2>
<h3 id="1-1-API">1.1 API</h3>
<p>Flink提供了多个层次的API供开发者使用，越往上抽象程度越高，使用起来越方便；越往下越底层，使用起来难度越大<br>
<img src="https://img-blog.csdnimg.cn/20210427103102655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210427103110578.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<font color=red><strong>注：</strong></font>在Flink1.12时支持流批一体，DataSet API已经不推荐使用了，后续都会优先使用DataStream流式API，既支持无界数据处理/流处理，也支持有界数据处理/批处理</p>
<h3 id="1-2-编程模型">1.2 编程模型</h3>
<p>Flink 应用程序结构主要包含三部分,Source/Transformation/Sink,如下图所示<br>
<img src="https://img-blog.csdnimg.cn/20210427103511706.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210427103521568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="2-准备工作">2. 准备工作</h2>
<h3 id="2-1-pom文件">2.1 pom文件</h3>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 </span></span></span><br><span class="line"><span class="string"><span class="tag">         http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>cn.itcast<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink_study_42<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定仓库位置，依次为aliyun、apache和cloudera仓库 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>apache<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.apache.org/content/repositories/snapshots/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">java.version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">java.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.12<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">flink.version</span>&gt;</span>1.12.0<span class="tag">&lt;/<span class="name">flink.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-scala-bridge_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-java-bridge_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- flink执行计划,这是1.9版本之前的--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- blink执行计划,1.11+默认的--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-blink_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;flink-cep_2.12&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;--&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- flink连接器--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-sql-connector-kafka_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-jdbc_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-csv<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-json<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- &lt;dependency&gt;</span></span><br><span class="line"><span class="comment">           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">           &lt;artifactId&gt;flink-connector-filesystem_2.12&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">           &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span></span><br><span class="line"><span class="comment">       &lt;/dependency&gt;--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;flink-jdbc_2.12&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">              &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">              &lt;artifactId&gt;flink-parquet_2.12&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">              &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;</span></span><br><span class="line"><span class="comment">         &lt;/dependency&gt;--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.avro&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;avro&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;1.9.2&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;</span></span><br><span class="line"><span class="comment">        &lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.parquet&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;parquet-avro&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;1.10.0&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;--&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.bahir<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-runtime_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hive_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-metastore<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-shaded-hadoop-2-uber<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.5-10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;version&gt;8.0.20&lt;/version&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- 高性能异步组件：Vertx--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>io.vertx<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>vertx-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>io.vertx<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>vertx-jdbc-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>io.vertx<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>vertx-redis-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- 日志 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.44<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.projectlombok<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>lombok<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.18.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- 参考：https://blog.csdn.net/f641385712/article/details/84109098--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;commons-collections4&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;4.4&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.thrift&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;libfb303&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;0.9.3&lt;/version&gt;</span></span><br><span class="line"><span class="comment">            &lt;type&gt;pom&lt;/type&gt;</span></span><br><span class="line"><span class="comment">            &lt;scope&gt;provided&lt;/scope&gt;</span></span><br><span class="line"><span class="comment">         &lt;/dependency&gt;--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">           &lt;groupId&gt;com.google.guava&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">           &lt;artifactId&gt;guava&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">           &lt;version&gt;28.2-jre&lt;/version&gt;</span></span><br><span class="line"><span class="comment">       &lt;/dependency&gt;--&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>src/main/java<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 编译插件 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.5.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                    <span class="comment">&lt;!--&lt;encoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/encoding&gt;--&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-surefire-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.18.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">useFile</span>&gt;</span>false<span class="tag">&lt;/<span class="name">useFile</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">disableXmlReport</span>&gt;</span>true<span class="tag">&lt;/<span class="name">disableXmlReport</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/*Test.*<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/*Suite.*<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 打包插件(会包含所有依赖) --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                        <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">                                        zip -d learn_spark.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF --&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span> <span class="attr">implementation</span>=<span class="string">&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;</span>&gt;</span></span><br><span class="line">                                    <span class="comment">&lt;!-- 设置jar包的入口类(可选) --&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span><span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-log4j-properties">2.2 log4j.properties</h3>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=WARN, console</span><br><span class="line">log4j.appender.console=org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.console.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.console.layout.ConversionPattern=%d&#123;HH:mm:ss,SSS&#125; %-5p %-60c %x - %m%n</span><br></pre></td></tr></table></figure>
<h2 id="3-Flink实现WordCount">3. Flink实现WordCount</h2>
<h3 id="3-1-编码步骤">3.1 编码步骤</h3>
<ol>
<li>准备环境-env</li>
<li>准备数据-source</li>
<li>处理数据-transformation</li>
<li>输出结果-sink</li>
<li>触发执行-execute</li>
</ol>
<p>其中创建环境可以使用下面三种方式</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">getExecutionEnvironment() <span class="comment">//推荐使用</span></span><br><span class="line">createLocalEnvironment()</span><br><span class="line">createRemoteEnvironment(<span class="type">String</span> host, int port, <span class="type">String</span>... jarFiles)</span><br></pre></td></tr></table></figure>
<h3 id="3-2-代码实现">3.2 代码实现</h3>
<p><img src="https://img-blog.csdnimg.cn/20210427105014204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h4 id="3-2-1-批处理wordcount">3.2.1 批处理wordcount</h4>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	</span><br><span class="line">		<span class="comment">// 创建执行环境</span></span><br><span class="line">		<span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 从文件中读取数据</span></span><br><span class="line">		<span class="keyword">val</span> inputPath = <span class="string">&quot;D:\\Projects\\BigData\\TestWC1\\src\\main\\resources\\hello.txt&quot;</span> <span class="keyword">val</span> inputDS: <span class="type">DataSet</span>[<span class="type">String</span>] = env.readTextFile(inputPath)</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 分词之后，对单词进行groupby 分组，然后用sum 进行聚合</span></span><br><span class="line">		<span class="keyword">val</span> wordCountDS: <span class="type">AggregateDataSet</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = inputDS.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map((_, <span class="number">1</span>)).groupBy(<span class="number">0</span>).sum(<span class="number">1</span>)</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 打印输出</span></span><br><span class="line">		wordCountDS.print()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-2-2-流处理wordcount">3.2.2 流处理wordcount</h4>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	</span><br><span class="line">		<span class="comment">// 从外部命令中获取参数</span></span><br><span class="line">		<span class="keyword">val</span> params: <span class="type">ParameterTool</span> = <span class="type">ParameterTool</span>.fromArgs(args)</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">val</span> host: <span class="type">String</span> = params.get(<span class="string">&quot;host&quot;</span>)</span><br><span class="line">		<span class="keyword">val</span> port: <span class="type">Int</span> = params.getInt(<span class="string">&quot;port&quot;</span>)</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 创建流处理环境</span></span><br><span class="line">		<span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 接收socket 文本流</span></span><br><span class="line">		<span class="keyword">val</span> textDstream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.socketTextStream(host, port)</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// flatMap 和Map 需要引用的隐式转换</span></span><br><span class="line">		<span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">		<span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = textDstream.flatMap(_.split(<span class="string">&quot;\\s&quot;</span>)).filter(_.nonEmpty).map((_, <span class="number">1</span>)).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>)</span><br><span class="line">		</span><br><span class="line">		dataStream.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 启动executor，执行任务</span></span><br><span class="line">		env.execute(<span class="string">&quot;Socket stream word count&quot;</span>)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="四、Flink原理">四、Flink原理</h1>
<h2 id="1-角色分工">1. 角色分工</h2>
<p>在实际生产中，Flink 都是以集群在运行，在运行的过程中包含了两类进程：</p>
<ul>
<li><strong>JobManager</strong>
<ul>
<li>它扮演的是集群管理者的角色，负责调度任务、协调 checkpoints、协调故障恢复、收集 Job 的状态信息，并管理 Flink 集群中的从节点 TaskManager</li>
</ul>
</li>
<li><strong>TaskManager</strong>
<ul>
<li>实际负责执行计算的 Worker，在其上执行 Flink Job 的一组 Task；TaskManager 还是所在节点的管理员，它负责把该节点上的服务器信息比如内存、磁盘、任务运行情况等向 JobManager 汇报</li>
</ul>
</li>
<li><strong>Client</strong>
<ul>
<li>用户在提交编写好的 Flink 工程时，会先创建一个客户端再进行提交，这个客户端就是 Client<br>
<img src="https://img-blog.csdnimg.cn/20210427112143315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210427112153791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li>
</ul>
</li>
</ul>
<h2 id="2-执行流程">2. 执行流程</h2>
<h3 id="2-1-Standalone版">2.1 Standalone版</h3>
<p><img src="https://img-blog.csdnimg.cn/20210427112250878.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h3 id="2-2-On-Yarn版">2.2 On Yarn版</h3>
<p><img src="https://img-blog.csdnimg.cn/20210427112316946.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<ol>
<li>Client向HDFS上传Flink的Jar包和配置</li>
<li>Client向Yarn ResourceManager提交任务并申请资源</li>
<li>ResourceManager分配Container资源并启动ApplicationMaster,然后AppMaster加载Flink的Jar包和配置构建环境,启动JobManager</li>
<li>ApplicationMaster向ResourceManager申请工作资源,NodeManager加载Flink的Jar包和配置构建环境并启动TaskManager</li>
<li>TaskManager启动后向JobManager发送心跳包，并等待JobManager向其分配任务</li>
</ol>
<h2 id="3-Flink-Streaming-Dataflow">3. Flink Streaming Dataflow</h2>
<h3 id="3-1-Dataflow、Operator、Partition、SubTask、Parallelism">3.1 Dataflow、Operator、Partition、SubTask、Parallelism</h3>
<ul>
<li>Dataflow
<ul>
<li>Flink程序在执行的时候会被映射成一个数据流模型</li>
</ul>
</li>
<li>Operator
<ul>
<li>数据流模型中的每一个操作被称作Operator,Operator分为:Source/Transform/Sink</li>
</ul>
</li>
<li>Partition
<ul>
<li>数据流模型是分布式的和并行的,执行中会形成1~n个分区</li>
</ul>
</li>
<li>Subtask
<ul>
<li>多个分区任务可以并行,每一个都是独立运行在一个线程中的,也就是一个Subtask子任</li>
</ul>
</li>
<li>Parallelism
<ul>
<li>并行度,就是可以同时真正执行的子任务数/分区数<br>
<img src="https://img-blog.csdnimg.cn/20210427113503606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li>
</ul>
</li>
</ul>
<h3 id="3-2-Operator传递模式">3.2 Operator传递模式</h3>
<p>数据在两个operator(算子)之间传递的时候有两种模式：</p>
<ul>
<li>One to One模式
<ul>
<li>两个operator用此模式传递的时候，会保持数据的分区数和数据的排序；如上图中的Source1到Map1，它就保留的Source的分区特性，以及分区元素处理的有序性（类似于Spark中的窄依赖）</li>
</ul>
</li>
<li>Redistributing模式
<ul>
<li>这种模式会改变数据的分区数；每个一个operator subtask会根据选择transformation把数据发送到不同的目标subtasks,比如keyBy()会通过hashcode重新分区,broadcast()和rebalance()方法会随机重新分区（类似于Spark中的宽依赖）</li>
</ul>
</li>
</ul>
<h3 id="3-3-Operator-Chain">3.3 Operator Chain</h3>
<p><img src="https://img-blog.csdnimg.cn/20210427113734406.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
客户端在提交任务的时候会对Operator进行优化操作，能进行合并的Operator会被合并为一个Operator，合并后的Operator称为Operator chain，实际上就是一个执行链，每个执行链会在TaskManager上一个独立的线程中执行（就是SubTask）</p>
<h3 id="3-4-TaskSlot-And-Slot-Sharing">3.4 TaskSlot And Slot Sharing</h3>
<p><img src="https://img-blog.csdnimg.cn/20210427113853330.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<ul>
<li>任务槽（TaskSlot）
<ul>
<li>每个TaskManager是一个JVM的进程, 为了控制一个TaskManager(worker)能接收多少个task，Flink通过Task Slot来进行控制<br>
TaskSlot数量是用来限制一个TaskManager工作进程中可以同时运行多少个工作线程，TaskSlot 是一个 TaskManager 中的最小资源分配单位，一个 TaskManager 中有多少个 TaskSlot 就意味着能支持多少并发的Task处理</li>
</ul>
</li>
</ul>
<p>Flink将进程的内存进行了划分到多个slot中，内存被划分到不同的slot之后可以获得如下好处:</p>
<ul>
<li>TaskManager最多能同时并发执行的子任务数是可以通过TaskSolt数量来控制的</li>
<li>TaskSolt有独占的内存空间，这样在一个TaskManager中可以运行多个不同的作业，作业之间不受影响</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20210427114058889.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<ul>
<li>槽共享（Slot Sharing）
<ul>
<li>Flink允许子任务共享插槽，即使它们是不同任务(阶段)的子任务(subTask)，只要它们来自同一个作业<br>
比如图左下角中的map和keyBy和sink 在一个 TaskSlot 里执行以达到资源共享的目的</li>
</ul>
</li>
</ul>
<p>允许插槽共享有两个主要好处：</p>
<ul>
<li>资源分配更加公平，如果有比较空闲的slot可以将更多的任务分配给它</li>
<li>有了任务槽共享，可以提高资源的利用率</li>
</ul>
<p><font color=red><strong>注：</strong></font><br>
slot是静态的概念，是指taskmanager具有的并发执行能力<br>
parallelism是动态的概念，是指程序运行时实际使用的并发能力</p>
<h2 id="4-Flink运行时的组件">4. Flink运行时的组件</h2>
<p>Flink运行时架构主要包括四个不同的组件，它们会在运行流处理应用程序时协同工作：</p>
<ul>
<li>作业管理器（JobManager）
<ul>
<li>分配任务、调度checkpoint做快照</li>
</ul>
</li>
<li>任务管理器（TaskManager）
<ul>
<li>主要干活的</li>
</ul>
</li>
<li>资源管理器（ResourceManager）
<ul>
<li>管理分配资源</li>
</ul>
</li>
<li>分发器（Dispatcher）
<ul>
<li>方便递交任务的接口，WebUI</li>
</ul>
</li>
</ul>
<p>因为Flink是用Java和Scala实现的，所以所有组件都会运行在Java虚拟机上。每个组件的职责如下：</p>
<ul>
<li><strong>作业管理器（JobManager）</strong>
<ul>
<li>控制一个应用程序执行的主进程，也就是说，每个应用程序都会被一个不同的JobManager 所控制执行</li>
<li>JobManager 会先接收到要执行的应用程序，这个应用程序会包括：作业图（JobGraph）、逻辑数据流图（logical dataflow graph）和打包了所有的类、库和其它资源的JAR包</li>
<li>JobManager 会把JobGraph转换成一个物理层面的数据流图，这个图被叫做“执行图”（ExecutionGraph），包含了所有可以并发执行的任务</li>
<li>JobManager 会向资源管理器（ResourceManager）请求执行任务必要的资源，也就是任务管理器（TaskManager）上的插槽（slot）。一旦它获取到了足够的资源，就会将执行图分发到真正运行它们的TaskManager上。而在运行过程中，JobManager会负责所有需要中央协调的操作，比如说检查点（checkpoints）的协调</li>
</ul>
</li>
<li><strong>任务管理器（TaskManager）</strong>
<ul>
<li>Flink中的工作进程。通常在Flink中会有多个TaskManager运行，每一个TaskManager都包含了一定数量的插槽（slots）。插槽的数量限制了TaskManager能够执行的任务数量</li>
<li>启动之后，TaskManager会向资源管理器注册它的插槽；收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给JobManager调用。JobManager就可以向插槽分配任务（tasks）来执行了</li>
<li>在执行过程中，一个TaskManager可以跟其它运行同一应用程序的TaskManager交换数据</li>
</ul>
</li>
<li><strong>资源管理器（ResourceManager）</strong>
<ul>
<li>主要负责管理任务管理器（TaskManager）的插槽（slot），TaskManger 插槽是Flink中定义的处理资源单元</li>
<li>Flink为不同的环境和资源管理工具提供了不同资源管理器，比如YARN、Mesos、K8s，以及standalone部署</li>
<li>当JobManager申请插槽资源时，ResourceManager会将有空闲插槽的TaskManager分配给JobManager。如果ResourceManager没有足够的插槽来满足JobManager的请求，它还可以向资源提供平台发起会话，以提供启动TaskManager进程的容器</li>
</ul>
</li>
<li><strong>分发器（Dispatcher）</strong>
<ul>
<li>可以跨作业运行，它为应用提交提供了REST接口</li>
<li>当一个应用被提交执行时，分发器就会启动并将应用移交给一个JobManager</li>
<li>Dispatcher也会启动一个Web UI，用来方便地展示和监控作业执行的信息</li>
<li>Dispatcher在架构中可能并不是必需的，这取决于应用提交运行的方式</li>
</ul>
</li>
</ul>
<h2 id="5-Flink执行图（ExecutionGraph）">5. Flink执行图（ExecutionGraph）</h2>
<p>由Flink程序直接映射成的数据流图是StreamGraph，也被称为逻辑流图，因为它们表示的是计算逻辑的高级视图。为了执行一个流处理程序，Flink需要将逻辑流图转换为物理数据流图（也叫执行图），详细说明程序的执行方式。<br>
Flink 中的执行图可以分成四层：<font color=red>StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图</font><br>
<img src="https://img-blog.csdnimg.cn/2021042711515181.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<ul>
<li>原理介绍
<ul>
<li>Flink执行executor会自动根据程序代码生成DAG数据流图</li>
<li>Flink 中的执行图可以分成四层：StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图
<ul>
<li><strong>StreamGraph：</strong> 是根据用户通过 Stream API 编写的代码生成的最初的图。表示程序的拓扑结构</li>
<li><strong>JobGraph：</strong> StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗</li>
<li><strong>ExecutionGraph：</strong> JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构</li>
<li><strong>物理执行图：</strong> JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构</li>
</ul>
</li>
</ul>
</li>
<li>简单理解
<ul>
<li><strong>StreamGraph：</strong> 最初的程序执行逻辑流程，也就是算子之间的前后顺序（在Client上生成）</li>
<li><strong>JobGraph：</strong> 将OneToOne的Operator合并为OperatorChain（在Client上生成）</li>
<li><strong>ExecutionGraph：</strong> 将JobGraph根据代码中设置的并行度和请求的资源进行并行化规划!（在JobManager上生成）</li>
<li><strong>物理执行图：</strong> 将ExecutionGraph的并行计划,落实到具体的TaskManager上，将具体的SubTask落实到具体的TaskSlot内进行运行</li>
</ul>
</li>
</ul>
<h1 id="五、流批一体API">五、流批一体API</h1>
<h2 id="1-DataStream-API">1. DataStream API</h2>
<ul>
<li>DataStream API 支持批执行模式
<ul>
<li>Flink 的核心 API 最初是针对特定的场景设计的，尽管 Table API / SQL 针对流处理和批处理已经实现了统一的 API，但当用户使用较底层的 API 时，仍然需要在批处理（DataSet API）和流处理（DataStream API）这两种不同的 API 之间进行选择。鉴于批处理是流处理的一种特例，将这两种 API 合并成统一的 API，有一些非常明显的好处，比如：
<ul>
<li>可复用性：作业可以在流和批这两种执行模式之间自由地切换，而无需重写任何代码。因此，用户可以复用同一个作业，来处理实时数据和历史数据</li>
<li>维护简单：统一的 API 意味着流和批可以共用同一组 connector，维护同一套代码，并能够轻松地实现流批混合执行，例如 backfilling 之类的场景</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>考虑到这些优点，社区已朝着流批统一的 DataStream API 迈出了第一步：支持高效的批处理（FLIP-134）。从长远来看，这意味着 DataSet API 将被弃用（FLIP-131），其功能将被包含在 DataStream API 和 Table API / SQL 中。</p>
<h2 id="2-Source">2. Source</h2>
<h3 id="2-1-预定义Source">2.1 预定义Source</h3>
<h4 id="2-1-1-基于集合的Source">2.1.1 基于集合的Source</h4>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义样例类，传感器id，时间戳，温度</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SensorReading</span>(<span class="params">id: <span class="type">String</span>, timestamp: <span class="type">Long</span>, temperature: <span class="type">Double</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Sensor</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">		<span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">		<span class="keyword">val</span> stream1: <span class="type">DataStream</span>[<span class="type">String</span>] = env</span><br><span class="line">			.fromCollection(<span class="type">List</span>(</span><br><span class="line">				<span class="type">SensorReading</span>(<span class="string">&quot;sensor_1&quot;</span>, <span class="number">1547718199</span>, <span class="number">35.8</span>),</span><br><span class="line">				<span class="type">SensorReading</span>(<span class="string">&quot;sensor_6&quot;</span>, <span class="number">1547718201</span>, <span class="number">15.4</span>),</span><br><span class="line">				<span class="type">SensorReading</span>(<span class="string">&quot;sensor_7&quot;</span>, <span class="number">1547718202</span>, <span class="number">6.7</span>),</span><br><span class="line">				<span class="type">SensorReading</span>(<span class="string">&quot;sensor_10&quot;</span>, <span class="number">1547718205</span>, <span class="number">38.1</span>)</span><br><span class="line">			))</span><br><span class="line">			</span><br><span class="line">		stream1.print(<span class="string">&quot;stream1:&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">		env.execute()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-1-2-基于文件的Source">2.1.2 基于文件的Source</h4>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream2: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">&quot;FILE_PATH&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="2-1-3-基于Socket的Source">2.1.3 基于Socket的Source</h4>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	</span><br><span class="line">		<span class="comment">// 从外部命令中获取参数</span></span><br><span class="line">		<span class="keyword">val</span> params: <span class="type">ParameterTool</span> = <span class="type">ParameterTool</span>.fromArgs(args)</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">val</span> host: <span class="type">String</span> = params.get(<span class="string">&quot;host&quot;</span>)</span><br><span class="line">		<span class="keyword">val</span> port: <span class="type">Int</span> = params.getInt(<span class="string">&quot;port&quot;</span>)</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 创建流处理环境</span></span><br><span class="line">		<span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 接收socket 文本流</span></span><br><span class="line">		<span class="keyword">val</span> textDstream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.socketTextStream(host, port)</span><br><span class="line">		</span><br><span class="line">		textDstream.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 启动executor，执行任务</span></span><br><span class="line">		env.execute(<span class="string">&quot;Socket stream word count&quot;</span>)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-1-4-Kafka-Source">2.1.4 Kafka Source</h4>
<p>需要引入Kafka连接器的依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	</span><br><span class="line">		<span class="comment">// 创建流处理环境</span></span><br><span class="line">		<span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">		<span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">		properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;localhost:9092&quot;</span>)</span><br><span class="line">		properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;consumer-group&quot;</span>)</span><br><span class="line">		properties.setProperty(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>)</span><br><span class="line">		properties.setProperty(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>)</span><br><span class="line">		properties.setProperty(<span class="string">&quot;auto.offset.reset&quot;</span>, <span class="string">&quot;latest&quot;</span>)		</span><br><span class="line"></span><br><span class="line">		<span class="keyword">val</span> stream = env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer011</span>[<span class="type">String</span>](<span class="string">&quot;sensor&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties))</span><br><span class="line">		</span><br><span class="line">		stream.print()</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 启动executor，执行任务</span></span><br><span class="line">		env.execute(<span class="string">&quot;Socket stream word count&quot;</span>)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-2-自定义Source">2.2 自定义Source</h3>
<p>除了以上的 source 数据来源， 我们还可以自定义 source。需要做的， 只是传入一个 SourceFunction 就可以。具体调用如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream = env.addSource( <span class="keyword">new</span> <span class="type">MySensorSource</span>() )</span><br></pre></td></tr></table></figure>
<p>我们希望可以随机生成传感器数据， MySensorSource 具体的代码实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySensorSource</span> <span class="keyword">extends</span> <span class="title">SourceFunction</span>[<span class="type">SensorReading</span>]</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// flag: 表示数据源是否还在正常运行</span></span><br><span class="line">	<span class="keyword">var</span> running: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">		running = <span class="literal">false</span></span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">SensorReading</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	</span><br><span class="line">		<span class="comment">// 初始化一个随机数发生器</span></span><br><span class="line">		<span class="keyword">val</span> rand = <span class="keyword">new</span> <span class="type">Random</span>()</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">var</span> curTemp = <span class="number">1.</span>to(<span class="number">10</span>).map(</span><br><span class="line">			i =&gt; ( <span class="string">&quot;sensor_&quot;</span> + i, <span class="number">65</span> + rand.nextGaussian() * <span class="number">20</span> )</span><br><span class="line">		)</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">while</span>(running)&#123;</span><br><span class="line">		</span><br><span class="line">			<span class="comment">// 更新温度值</span></span><br><span class="line">			curTemp = curTemp.map(</span><br><span class="line">				t =&gt; (t._1, t._2 + rand.nextGaussian() )</span><br><span class="line">			)</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 获取当前时间戳</span></span><br><span class="line">			<span class="keyword">val</span> curTime = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">			</span><br><span class="line">			curTemp.foreach(</span><br><span class="line">				t =&gt; ctx.collect(<span class="type">SensorReading</span>(t._1, curTime, t._2))</span><br><span class="line">			)</span><br><span class="line">			</span><br><span class="line">			<span class="type">Thread</span>.sleep(<span class="number">100</span>)</span><br><span class="line">		</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-Transformation">3. Transformation</h2>
<p><img src="https://img-blog.csdnimg.cn/20210427154434759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210427154440564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
整体来说，流式数据上的操作可以分为四类：</p>
<ol>
<li>第一类是对于单条记录的操作，比如筛除掉不符合要求的记录（Filter 操作），或者将每条记录都做一个转换（Map 操作）</li>
<li>第二类是对多条记录的操作。比如说统计一个小时内的订单总成交量，就需要将一个小时内的所有订单记录的成交量加到一起。为了支持这种类型的操作，就得通过 Window 将需要的记录关联到一起进行处理</li>
<li>第三类是对多个流进行操作并转换为单个流。例如，多个流可以通过 Union、Join 或 Connect 等操作合到一起。这些操作合并的逻辑不同，但是它们最终都会产生了一个新的统一的流，从而可以进行一些跨流的操作</li>
<li>DataStream 还支持与合并对称的拆分操作，即把一个流按一定规则拆分为多个流（Split 操作），每个流是之前流的一个子集，这样我们就可以对不同的流作不同的处理</li>
</ol>
<h3 id="3-1-map">3.1 map</h3>
<p><strong>map:</strong> 将函数作用在集合中的每一个元素上,并返回作用后的结果<br>
<img src="https://img-blog.csdnimg.cn/20210427154625112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> streamMap = stream.map &#123; x =&gt; x * <span class="number">2</span> &#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-flatMap">3.2 flatMap</h3>
<p><strong>flatMap:</strong> 将集合中的每个元素变成一个或多个元素,并返回扁平化之后的结果</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> streamFlatMap = stream.flatMap&#123;</span><br><span class="line">	x =&gt; x.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>flatMap 的函数签名：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">A</span>,<span class="type">B</span>](as: <span class="type">List</span>[<span class="type">A</span>])(f: <span class="type">A</span> ⇒ <span class="type">List</span>[<span class="type">B</span>]): <span class="type">List</span>[<span class="type">B</span>]</span><br></pre></td></tr></table></figure>
<p>例如:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatMap(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))(i =&gt; <span class="type">List</span>(i,i))</span><br></pre></td></tr></table></figure>
<p>结果是 List(1,1,2,2,3,3)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">List</span>(<span class="string">&quot;a b&quot;</span>, <span class="string">&quot;c d&quot;</span>).flatMap(line =&gt; line.split(<span class="string">&quot; &quot;</span>))</span><br></pre></td></tr></table></figure>
<p>结果是 List(a, b, c, d)</p>
<h3 id="3-3-filter">3.3 filter</h3>
<p><strong>filter:</strong> 按照指定的条件对集合中的元素进行过滤,过滤出返回true/符合条件的元素<br>
<img src="https://img-blog.csdnimg.cn/20210427155213297.png#pic_center" alt="在这里插入图片描述"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> streamFilter = stream.filter&#123;</span><br><span class="line">	x =&gt; x == <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-4-keyBy">3.4 keyBy</h3>
<p><strong>DataStream → KeyedStream：</strong> 逻辑地将一个流拆分成不相交的分区，每个分区包含具有相同 key 的元素，在内部以 hash 的形式实现的。<br>
<img src="https://img-blog.csdnimg.cn/20210427155521872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<strong><font color=red>注：</font></strong> 流处理中没有groupBy,而是keyBy</p>
<h3 id="3-5-滚动聚合算子（Rolling-Aggregation）">3.5 滚动聚合算子（Rolling Aggregation）</h3>
<p>这些算子可以针对 KeyedStream 的每一个支流做聚合：</p>
<ul>
<li>sum()</li>
<li>min()</li>
<li>max()</li>
<li>minBy()</li>
<li>maxBy()</li>
</ul>
<h3 id="3-6-Reduce">3.6 Reduce</h3>
<p><strong>KeyedStream → DataStream：</strong> 一个分组数据流的聚合操作，合并当前的元素 和上次聚合的结果，产生一个新的值，返回的流中包含每一次聚合的结果，而不是 只返回最后一次聚合的最终结果</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream2 = env.readTextFile(<span class="string">&quot;YOUR_PATH\\sensor.txt&quot;</span>)</span><br><span class="line">	.map( data =&gt; &#123;</span><br><span class="line">		<span class="keyword">val</span> dataArray = data.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">		<span class="type">SensorReading</span>(dataArray(<span class="number">0</span>).trim, dataArray(<span class="number">1</span>).trim.toLong, dataArray(<span class="number">2</span>).trim.toDouble)</span><br><span class="line">&#125;)</span><br><span class="line">	.keyBy(<span class="string">&quot;id&quot;</span>)</span><br><span class="line">	.reduce( (x, y) =&gt; <span class="type">SensorReading</span>(x.id, x.timestamp + <span class="number">1</span>, y.temperature) )</span><br></pre></td></tr></table></figure>
<h3 id="3-7-合并-拆分">3.7 合并&amp;拆分</h3>
<h4 id="3-7-1-Split-和-Select">3.7.1 Split 和 Select</h4>
<p><strong>Split：</strong> 根据某些特征把一个 DataStream 拆分成两个或者 多个 DataStream（<strong>DataStream → SplitStream</strong>）<br>
<img src="https://img-blog.csdnimg.cn/20210427160847349.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<strong>Select：</strong> 从一个 SplitStream 中获取一个或者多个DataStream（<strong>SplitStream→DataStream</strong>）<br>
<img src="https://img-blog.csdnimg.cn/20210427160943799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<strong>需求：</strong> 传感器数据按照温度高低（以 30 度为界），拆分成两个流</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> splitStream = stream2</span><br><span class="line">	.split( sensorData =&gt; &#123;</span><br><span class="line">		<span class="keyword">if</span> (sensorData.temperature &gt; <span class="number">30</span>) <span class="type">Seq</span>(<span class="string">&quot;high&quot;</span>) <span class="keyword">else</span> <span class="type">Seq</span>(<span class="string">&quot;low&quot;</span>)</span><br><span class="line">	&#125; )</span><br><span class="line"><span class="keyword">val</span> high = splitStream.select(<span class="string">&quot;high&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> low = splitStream.select(<span class="string">&quot;low&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> all = splitStream.select(<span class="string">&quot;high&quot;</span>, <span class="string">&quot;low&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="3-7-2-Connect-和-CoMap">3.7.2 Connect 和 CoMap</h4>
<p><strong>Connect：</strong> 连接两个保持他们类型的数据流， 两个数据流被 Connect 之后， 只是被放在了一个同一个流中， 内部依然保持各自的数据和形式不发生任何变化， 两个流相互独立（<strong>DataStream,DataStream → ConnectedStreams</strong>）<br>
<img src="https://img-blog.csdnimg.cn/20210427161344411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<strong>CoMap,CoFlatMap：</strong> 作用于 ConnectedStreams 上， 功能与 map 和 flatMap 一样， 对ConnectedStreams 中的每一个 Stream 分别进行 map 和 flatMap 处理（<strong>ConnectedStreams → DataStream</strong>）<br>
<img src="https://img-blog.csdnimg.cn/20210427162551975.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> warning = high.map( sensorData =&gt; (sensorData.id, sensorData.temperature) )</span><br><span class="line"><span class="keyword">val</span> connected = warning.connect(low)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> coMap = connected.map(</span><br><span class="line">	warningData =&gt; (warningData._1, warningData._2, <span class="string">&quot;warning&quot;</span>),</span><br><span class="line">	lowData =&gt; (lowData.id, <span class="string">&quot;healthy&quot;</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="3-7-3-Union">3.7.3 Union</h4>
<p><strong>DataStream → DataStream：</strong> 对两个或者两个以上的 DataStream 进行 union 操 作，产生一个包含所有 DataStream 元素的新 DataStream<br>
<img src="https://img-blog.csdnimg.cn/20210427162915472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//合并以后打印 </span></span><br><span class="line"><span class="keyword">val</span> unionStream: <span class="type">DataStream</span>[<span class="type">StartUpLog</span>] = appStoreStream.union(otherStream) unionStream.print(<span class="string">&quot;union:::&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Connect 与 Union 区别：</strong></p>
<ol>
<li>Union 之前两个流的类型必须是一样，Connect 可以不一样，在之后的 coMap 中再去调整成为一样的</li>
<li>Connect 只能操作两个流，Union 可以操作多个</li>
</ol>
<h3 id="3-8-分区">3.8 分区</h3>
<h4 id="3-8-1-rebalance重平衡分区">3.8.1 rebalance重平衡分区</h4>
<p>类似于Spark中的repartition,但是功能更强大,可以直接解决数据倾斜<br>
Flink也有数据倾斜的时候，比如当前有数据量大概10亿条数据需要处理，在处理过程中可能会发生如图所示的状况，出现了数据倾斜，其他3台机器执行完毕也要等待机器1执行完毕后才算整体将任务完成：<br>
<img src="https://img-blog.csdnimg.cn/20210427163401911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
所以在实际的工作中，出现这种情况比较好的解决方案就是rebalance（内部使用round robin方法将数据均匀打散）<br>
<img src="https://img-blog.csdnimg.cn/20210427163434122.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h4 id="3-8-2-其他分区">3.8.2 其他分区</h4>
<p><img src="https://img-blog.csdnimg.cn/20210427164148326.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<strong>recale分区：</strong> 基于上下游Operator的并行度，将记录以循环的方式输出到下游Operator的每个实例<br>
<strong>举例：</strong> 上游并行度是2，下游是4，则上游一个并行度以循环的方式将记录输出到下游的两个并行度上;上游另一个并行度以循环的方式将记录输出到下游另两个并行度上。若上游并行度是4，下游并行度是2，则上游两个并行度将记录输出到下游一个并行度上；上游另两个并行度将记录输出到下游另一个并行度上。</p>
<h3 id="3-9-支持的数据类型">3.9 支持的数据类型</h3>
<p>Flink 流应用程序处理的是以数据对象表示的事件流。所以在 Flink 内部， 我们需要能够处理这些对象。它们需要被序列化和反序列化， 以便通过网络传送它们； 或者从状态后端、检查点和保存点读取它们。为了有效地做到这一点，Flink 需要明确知道应用程序所处理的数据类型。Flink 使用类型信息的概念来表示数据类型，并为每个数据类型生成特定的序列化器、反序列化器和比较器。<br>
Flink 还具有一个类型提取系统，该系统分析函数的输入和返回类型，以自动获取类型信息，从而获得序列化器和反序列化器。但是，在某些情况下，例如 lambda 函数或泛型类型， 需要显式地提供类型信息， 才能使应用程序正常工作或提高其性能。</p>
<h4 id="3-9-1-基础数据类型">3.9.1 基础数据类型</h4>
<p>Flink 支持所有的 Java 和 Scala 基础数据类型， Int, Double, Long, String, …</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> numbers: <span class="type">DataStream</span>[<span class="type">Long</span>] = env.fromElements(<span class="number">1</span>L, <span class="number">2</span>L, <span class="number">3</span>L, <span class="number">4</span>L)</span><br><span class="line">numbers.map( n =&gt; n + <span class="number">1</span> )</span><br></pre></td></tr></table></figure>
<h4 id="3-9-2-Java-和Scala-元组（Tuples）">3.9.2 Java 和Scala 元组（Tuples）</h4>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> persons: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Integer</span>)] = env</span><br><span class="line">	.fromElements( (<span class="string">&quot;Adam&quot;</span>, <span class="number">17</span>), (<span class="string">&quot;Sarah&quot;</span>, <span class="number">23</span>) ) </span><br><span class="line">persons.filter(p =&gt; p._2 &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure>
<h4 id="3-9-3-Scala-样例类（case-classes）">3.9.3 Scala 样例类（case classes）</h4>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> persons: <span class="type">DataStream</span>[<span class="type">Person</span>] = env.</span><br><span class="line">	fromElements( <span class="type">Person</span>(<span class="string">&quot;Adam&quot;</span>, <span class="number">17</span>), <span class="type">Person</span>(<span class="string">&quot;Sarah&quot;</span>, <span class="number">23</span>) )</span><br><span class="line">persons.filter(p =&gt; p.age &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure>
<h4 id="3-9-4-Java-简单对象（POJOs）">3.9.4 Java 简单对象（POJOs）</h4>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Person</span> &#123;</span><br><span class="line">	<span class="keyword">public</span> String name;</span><br><span class="line">	<span class="keyword">public</span> <span class="type">int</span> age;</span><br><span class="line">	<span class="keyword">public</span> <span class="title function_">Person</span><span class="params">()</span> &#123;&#125;</span><br><span class="line">	<span class="keyword">public</span> <span class="title function_">Person</span><span class="params">(String name, <span class="type">int</span> age)</span> &#123;</span><br><span class="line">		<span class="built_in">this</span>.name = name;</span><br><span class="line">		<span class="built_in">this</span>.age = age;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">DataStream&lt;Person&gt; persons = env.fromElements(</span><br><span class="line">	<span class="keyword">new</span> <span class="title class_">Person</span>(<span class="string">&quot;Alex&quot;</span>, <span class="number">42</span>),</span><br><span class="line">	<span class="keyword">new</span> <span class="title class_">Person</span>(<span class="string">&quot;Wendy&quot;</span>, <span class="number">23</span>));</span><br></pre></td></tr></table></figure>
<h4 id="3-9-5-其它（Arrays-Lists-Maps-Enums-等等）">3.9.5 其它（Arrays, Lists, Maps, Enums, 等等）</h4>
<p>Flink 对 Java 和 Scala 中的一些特殊目的的类型也都是支持的，比如 Java 的 ArrayList，HashMap，Enum 等等。</p>
<h2 id="4-Sink">4. Sink</h2>
<p>Flink 没有类似于 spark 中 foreach 方法， 让用户进行迭代的操作。虽有对外的输出操作都要利用 Sink 完成。最后通过类似如下方式完成整个任务最终输出操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.addSink(<span class="keyword">new</span> <span class="type">MySink</span>(xxxx))</span><br></pre></td></tr></table></figure>
<p>官方提供了一部分的框架的 sink。除此以外， 需要用户自定义实现 sink</p>
<h3 id="4-1-Kafka">4.1 Kafka</h3>
<ol>
<li>pom依赖</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>主函数中添加sink</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> union = high.union(low).map(_.temperature.toString)</span><br><span class="line"></span><br><span class="line">union.addSink(<span class="keyword">new</span> <span class="type">FlinkKafkaProducer011</span>[<span class="type">String</span>](<span class="string">&quot;localhost:9092&quot;</span>,<span class="string">&quot;test&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()))</span><br></pre></td></tr></table></figure>
<h3 id="4-2-Redis">4.2 Redis</h3>
<ol>
<li>pom依赖</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">https://mvnrepository.com/artifact/org.apache.bahir/flink-connector-redis</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.bahir<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>定义一个 redis 的 mapper 类， 用于定义保存到 redis 时调用的命令</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRedisMapper</span> <span class="keyword">extends</span> <span class="title">RedisMapper</span>[<span class="type">SensorReading</span>]</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCommandDescription</span></span>: <span class="type">RedisCommandDescription</span> = &#123;</span><br><span class="line">	</span><br><span class="line">		<span class="keyword">new</span> <span class="type">RedisCommandDescription</span>(<span class="type">RedisCommand</span>.<span class="type">HSET</span>, <span class="string">&quot;sensor_temperature&quot;</span>)</span><br><span class="line">	</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValueFromData</span></span>(t: <span class="type">SensorReading</span>): <span class="type">String</span> = t.temperature.toString</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getKeyFromData</span></span>(t: <span class="type">SensorReading</span>): <span class="type">String</span> = t.id</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>在主函数中调用</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">FlinkJedisPoolConfig</span>.<span class="type">Builder</span>().setHost(<span class="string">&quot;localhost&quot;</span>).setPort(<span class="number">6379</span>).build() </span><br><span class="line">dataStream.addSink( <span class="keyword">new</span> <span class="type">RedisSink</span>[<span class="type">SensorReading</span>](conf, <span class="keyword">new</span> <span class="type">MyRedisMapper</span>) )</span><br></pre></td></tr></table></figure>
<h3 id="4-3-Elasticsearch">4.3 Elasticsearch</h3>
<ol>
<li>pom依赖</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>在主函数中调用</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> httpHosts = <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">HttpHost</span>]()</span><br><span class="line">httpHosts.add(<span class="keyword">new</span> <span class="type">HttpHost</span>(<span class="string">&quot;localhost&quot;</span>, <span class="number">9200</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> esSinkBuilder = <span class="keyword">new</span> <span class="type">ElasticsearchSink</span>.<span class="type">Builder</span>[<span class="type">SensorReading</span>]( httpHosts,<span class="keyword">new</span> <span class="type">ElasticsearchSinkFunction</span>[<span class="type">SensorReading</span>] &#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(t: <span class="type">SensorReading</span>, runtimeContext: <span class="type">RuntimeContext</span>, requestIndexer: <span class="type">RequestIndexer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">		println(<span class="string">&quot;saving data: &quot;</span> + t)</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">val</span> json = <span class="keyword">new</span> util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]() json.put(<span class="string">&quot;data&quot;</span>, t.toString)</span><br><span class="line">		<span class="keyword">val</span> indexRequest = <span class="type">Requests</span>.indexRequest().index(<span class="string">&quot;sensor&quot;</span>).`<span class="class"><span class="keyword">type</span>`(<span class="params">&quot;readingData&quot;</span>).<span class="title">source</span>(<span class="params">json</span>)</span></span><br><span class="line">		requestIndexer.add(indexRequest)</span><br><span class="line">		</span><br><span class="line">		println(<span class="string">&quot;saved successfully&quot;</span>)</span><br><span class="line">	</span><br><span class="line">	&#125;</span><br><span class="line">&#125; )</span><br><span class="line"></span><br><span class="line">dataStream.addSink( esSinkBuilder.build() )</span><br></pre></td></tr></table></figure>
<h3 id="4-4-JDBC自定义sink">4.4 JDBC自定义sink</h3>
<ol>
<li>pom依赖</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.44<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>添加 MyJdbcSink</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyJdbcSink</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>[<span class="type">SensorReading</span>]</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> conn: <span class="type">Connection</span> = _</span><br><span class="line">	<span class="keyword">var</span> insertStmt: <span class="type">PreparedStatement</span> = _</span><br><span class="line">	<span class="keyword">var</span> updateStmt: <span class="type">PreparedStatement</span> = _</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// open 主要是创建连接</span></span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">	</span><br><span class="line">		<span class="keyword">super</span>.open(parameters)</span><br><span class="line">		</span><br><span class="line">		conn = <span class="type">DriverManager</span>.getConnection(<span class="string">&quot;jdbc:mysql://localhost:3306/test&quot;</span>,<span class="string">&quot;root&quot;</span>, <span class="string">&quot;123456&quot;</span>)</span><br><span class="line">		insertStmt = conn.prepareStatement(<span class="string">&quot;INSERT INTO temperatures (sensor, temp) VALUES (?, ?)&quot;</span>)</span><br><span class="line">		updateStmt = conn.prepareStatement(<span class="string">&quot;UPDATE temperatures SET temp = ? WHERE sensor = ?&quot;</span>)</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 调用连接，执行sql</span></span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(value: <span class="type">SensorReading</span>, context: <span class="type">SinkFunction</span>.<span class="type">Context</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	</span><br><span class="line">		updateStmt.setDouble(<span class="number">1</span>, value.temperature)</span><br><span class="line">		updateStmt.setString(<span class="number">2</span>, value.id)</span><br><span class="line">		</span><br><span class="line">		updateStmt.execute()</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (updateStmt.getUpdateCount == <span class="number">0</span>) &#123;</span><br><span class="line">			insertStmt.setString(<span class="number">1</span>, value.id)</span><br><span class="line">			insertStmt.setDouble(<span class="number">2</span>, value.temperature)</span><br><span class="line">			insertStmt.execute()</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">		insertStmt.close()</span><br><span class="line">		updateStmt.close()</span><br><span class="line">		conn.close()</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>在 main 方法中增加</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.addSink(<span class="keyword">new</span> <span class="type">MyJdbcSink</span>())</span><br></pre></td></tr></table></figure>
<h1 id="六、Flink中的Window">六、Flink中的Window</h1>
<h2 id="1-Window概述">1. Window概述</h2>
<ol>
<li>简介<br>
streaming 流式计算是一种被设计用于处理无限数据集的数据处理引擎，而无限数据集是指一种不断增长的本质上无限的数据集， 而 window 是一种<strong>切割无限数据为有限块进行处理</strong>的手段。<br>
Window 是无限数据流处理的核心，可以将一个无限的 stream 拆分成有限大小的” buckets” 桶， 我们可以在这些桶上做计算操作。</li>
<li>为什么需要Window？<br>
在流处理应用中，数据是连续不断的，有时我们需要做一些聚合类的处理，例如：在过去的1分钟内有多少用户点击了我们的网页。<br>
在这种情况下，我们必须定义一个窗口(window)，用来收集最近1分钟内的数据，并对这个窗口内的数据进行计算。</li>
</ol>
<h2 id="2-Window的分类">2. Window的分类</h2>
<h3 id="2-1-按照time和count分类">2.1 按照time和count分类</h3>
<p><strong>time-window</strong>:时间窗口，根据时间划分窗口,如:每xx分钟统计最近xx分钟的数据。<br>
<strong>count-window</strong>:数量窗口，根据数量划分窗口,如:每xx个数据统计最近xx个数据。<br>
<img src="https://img-blog.csdnimg.cn/20210514102329841.png#pic_center" alt="在这里插入图片描述"></p>
<h3 id="2-2-按照side和size分类">2.2 按照side和size分类</h3>
<p>窗口有两个重要的属性: 窗口大小size和滑动间隔slide,根据它们的大小关系可分为：<br>
<img src="https://img-blog.csdnimg.cn/20210514102446708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<strong>tumbling-window</strong>:滚动窗口，size=slide,如:每隔10s统计最近10s的数据<br>
<strong><font color=red>特点</font></strong>：时间对齐，窗口长度固定，没有重叠<br>
<strong><font color=red>适用场景</font></strong>：适合做 BI 统计等（做每个时间段的聚合计算）<br>
<img src="https://img-blog.csdnimg.cn/20210514102627862.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<strong>sliding-window</strong>:滑动窗口，size&gt;slide,如:每隔5s统计最近10s的数据<br>
<strong><font color=red>特点</font></strong>：时间对齐，窗口长度固定，可以有重叠<br>
<strong><font color=red>适用场景</font></strong>：对最近一个时间段内的统计（求某接口最近 5min 的失败率来决定是 否要报警）<br>
<img src="https://img-blog.csdnimg.cn/2021051410403539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<strong>session-windows</strong>：会话窗口，size=slide,如:每隔10s统计最近10s的数据<br>
<strong><font color=red>特点</font></strong>：时间无对齐<br>
session 窗口分配器通过 session 活动来对元素进行分组，session 窗口跟滚动窗 口和滑动窗口相比，不会有重叠和固定的开始时间和结束时间的情况，相反，当它 在一个固定的时间周期内不再收到元素，即非活动间隔产生，那个这个窗口就会关 闭。一个 session 窗口通过一个 session 间隔来配置，这个 session 间隔定义了非活跃 周期的长度，当这个非活跃周期产生，那么当前的 session 将关闭并且后续的元素将 被分配到新的 session 窗口中去</p>
<p><strong><font color=red>注意</font></strong>：当size&lt;slide的时候,如每隔15s统计最近10s的数据,那么中间5s的数据会丢失,所有开发中不用</p>
<h3 id="2-3-总结">2.3 总结</h3>
<p>按照上面窗口的分类方式进行组合,可以得出如下的窗口:</p>
<ol>
<li>基于时间的滚动窗口tumbling-time-window——用的较多</li>
<li>基于时间的滑动窗口sliding-time-window——用的较多</li>
<li>基于数量的滚动窗口tumbling-count-window——用的较少</li>
<li>基于数量的滑动窗口sliding-count-window——用的较少</li>
</ol>
<p><strong><font color=red>注意</font></strong>：Flink还支持一个特殊的窗口:Session会话窗口,需要设置一个会话超时时间,如30s,则表示30s内没有数据到来,则触发上个窗口的计算</p>
<h2 id="3-Window-API">3. Window API</h2>
<h3 id="3-1-TimeWindow">3.1 TimeWindow</h3>
<p>TimeWindow 是将指定时间范围内的所有数据组成一个 window， 一次对一个window 里面的所有数据进行计算。</p>
<h4 id="3-1-1-滚动窗口">3.1.1 滚动窗口</h4>
<p>Flink 默认的时间窗口根据 Processing Time 进行窗口的划分，将 Flink 获取到的数据根据进入 Flink 的时间划分到不同的窗口中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> minTempPerWindow = dataStream</span><br><span class="line">	.map(r =&gt; (r.id, r.temperature))</span><br><span class="line">	.keyBy(_._1)</span><br><span class="line">	.timeWindow(<span class="type">Time</span>.seconds(<span class="number">15</span>))</span><br><span class="line">	.reduce((r1, r2) =&gt; (r1._1, r1._2.min(r2._2)))</span><br></pre></td></tr></table></figure>
<p>时间间隔可以通过 <strong>Time.milliseconds(x)</strong>，<strong>Time.seconds(x)</strong>，<strong>Time.minutes(x)</strong> 等其中的一个来指定。</p>
<h4 id="3-1-2-滑动窗口">3.1.2 滑动窗口</h4>
<p>滑动窗口和滚动窗口的函数名是完全一致的， 只是在传参数时需要传入两个参数， 一个是 window_size， 一个是 sliding_size。</p>
<ul>
<li>下面代码中的 sliding_size 设置为了 5s，也就是说，每 5s 就计算输出结果一次， 每一次计算的 window 范围是 15s 内的所有元素：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> minTempPerWindow: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = dataStream</span><br><span class="line">	.map(r =&gt; (r.id, r.temperature))</span><br><span class="line">	.keyBy(_._1)</span><br><span class="line">	.timeWindow(<span class="type">Time</span>.seconds(<span class="number">15</span>), <span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">	.reduce((r1, r2) =&gt; (r1._1, r1._2.min(r2._2)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// .window(SlidingEventTimeWindows.of(Time.seconds(15),Time.sec onds(5))</span></span><br></pre></td></tr></table></figure>
<p>时间间隔可以通过 <strong>Time.milliseconds(x)</strong>，<strong>Time.seconds(x)</strong>，<strong>Time.minutes(x)</strong> 等其中的一个来指定。</p>
<h3 id="3-2-CountWindow">3.2 CountWindow</h3>
<p>CountWindow 根据窗口中相同 key 元素的数量来触发执行， 执行时只计算元素数量达到窗口大小的 key 对应的结果。<br>
<strong><font color=red>注：</font></strong> CountWindow 的 window_size 指的是相同 Key 的元素的个数，不是输入的所有元素的总数。</p>
<h4 id="3-2-1-滚动窗口">3.2.1 滚动窗口</h4>
<p>默认的 CountWindow 是一个滚动窗口，只需要指定窗口大小即可，当元素数量达到窗口大小时， 就会触发窗口的执行。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> minTempPerWindow: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = dataStream</span><br><span class="line">	.map(r =&gt; (r.id, r.temperature))</span><br><span class="line">	.keyBy(_._1)</span><br><span class="line">	.countWindow(<span class="number">5</span>)</span><br><span class="line">	.reduce((r1, r2) =&gt; (r1._1, r1._2.max(r2._2)))</span><br></pre></td></tr></table></figure>
<h4 id="3-2-2-滑动窗口">3.2.2 滑动窗口</h4>
<p>滑动窗口和滚动窗口的函数名是完全一致的， 只是在传参数时需要传入两个参数， 一个是 window_size， 一个是 sliding_size。</p>
<ul>
<li>下面代码中的 sliding_size 设置为了 2， 也就是说， 每收到两个相同 key 的数据就计算一次， 每一次计算的 window 范围是 10 个元素：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> keyedStream: <span class="type">KeyedStream</span>[(<span class="type">String</span>, <span class="type">Int</span>), <span class="type">Tuple</span>] = dataStream.map(r =&gt; (r.id,r.temperature)).keyBy(<span class="number">0</span>)</span><br><span class="line"><span class="comment">//每当某一个key 的个数达到2 的时候,触发计算，计算最近该key 最近10 个元素的内容</span></span><br><span class="line"><span class="keyword">val</span> windowedStream: <span class="type">WindowedStream</span>[(<span class="type">String</span>, <span class="type">Int</span>), <span class="type">Tuple</span>, <span class="type">GlobalWindow</span>] = keyedStream.countWindow(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> sumDstream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = windowedStream.sum(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-3-window-function">3.3 window function</h3>
<p>window function 定义了要对窗口中收集的数据做的计算操作，主要可以分为两类：</p>
<ul>
<li>增量聚合函数（ incremental aggregation functions）
<ul>
<li>每条数据到来就进行计算， 保持一个简单的状态</li>
<li>典型的增量聚合函数有ReduceFunction, AggregateFunction</li>
</ul>
</li>
<li>全窗口函数（ full window functions）
<ul>
<li>先把窗口所有数据收集起来， 等到计算的时候会遍历所有数据</li>
<li>ProcessWindowFunction 就是一个全窗口函数</li>
</ul>
</li>
</ul>
<h3 id="3-4-其他API">3.4 其他API</h3>
<ul>
<li>.trigger() —— 触发器
<ul>
<li>定义 window 什么时候关闭， 触发计算并输出结果</li>
</ul>
</li>
<li>.evitor() —— 移除器
<ul>
<li>定义移除某些数据的逻辑</li>
</ul>
</li>
<li>.allowedLateness() —— 允许处理迟到的数据</li>
<li>.sideOutputLateData() —— 将迟到的数据放入侧输出流</li>
<li>.getSideOutput() —— 获取侧输出流<br>
<img src="https://img-blog.csdnimg.cn/2021051414553022.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li>
</ul>
<h1 id="七、时间语义与Wartermark">七、时间语义与Wartermark</h1>
<h2 id="1-Flink中的时间语义">1. Flink中的时间语义</h2>
<p>在Flink的流式处理中，会涉及到时间的不同概念，如下图所示：<br>
<img src="https://img-blog.csdnimg.cn/20210517094220598.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<strong>Event Time</strong>： 是事件创建的时间。它通常由事件中的时间戳描述， 例如采集的日志数据中，每一条日志都会记录自己的生成时间，Flink 通过时间戳分配器访问事件时间戳。<br>
<strong>Ingestion Time</strong>： 是数据进入 Flink 的时间。<br>
<strong>Processing Time</strong>： 是每一个执行基于时间操作的算子的本地系统时间， 与机器相关， 默认的时间属性就是 Processing Time。</p>
<h2 id="2-EventTime的重要性">2. EventTime的重要性</h2>
<p><strong>在 Flink 的流式处理中， 绝大部分的业务都会使用 eventTime</strong>， 一般只在eventTime 无法使用时， 才会被迫使用 ProcessingTime 或者 IngestionTime。<br>
如果要使用 EventTime，那么需要引入 EventTime 的时间属性，引入方式如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="comment">// 从调用时刻开始给env 创建的每一个stream 追加时间特征</span></span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br></pre></td></tr></table></figure>
<h2 id="3-Watermark">3. Watermark</h2>
<h3 id="3-1-基本概念">3.1 基本概念</h3>
<p>我们知道，流处理从事件产生，到流经 source，再到 operator，中间是有一个过程和时间的， 虽然大部分情况下， 流到 operator 的数据都是按照事件产生的时间顺序来的， 但是也不排除由于网络、分布式等原因， 导致乱序的产生， 所谓乱序， 就是指 Flink 接收到的事件的先后顺序不是严格按照事件的 Event Time 顺序排列的。<br>
<img src="https://img-blog.csdnimg.cn/20210517100326243.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
那么此时出现一个问题，一旦出现乱序，如果只根据 eventTime 决定 window 的运行， 我们不能明确数据是否全部到位， 但又不能无限期的等下去， 此时必须要有个机制来保证一个特定的时间后， 必须触发 window 去进行计算了， 这个特别的机制， 就是 Watermark。</p>
<ul>
<li>Watermark 是一种衡量 Event Time 进展的机制。</li>
<li>Watermark 是用于处理乱序事件的， 而正确的处理乱序事件， 通常用Watermark 机制结合 window 来实现。</li>
<li>数据流中的 Watermark 用于表示 timestamp 小于 Watermark 的数据，都已经到达了， 因此， window 的执行也是由 Watermark 触发的。</li>
<li>Watermark 可以理解成一个延迟触发机制，我们可以设置 Watermark 的延时时长 t，每次系统会校验已经到达的数据中最大的 maxEventTime，然后认定 eventTime小于 maxEventTime - t 的所有数据都已经到达， 如果有窗口的停止时间等于maxEventTime – t， 那么这个窗口被触发执行。</li>
</ul>
<p>有序流的 Watermarker 如下图所示（ Watermark 设置为 0）：<img src="https://img-blog.csdnimg.cn/20210517100655646.png#pic_center" alt="在这里插入图片描述"><br>
乱序流的 Watermarker 如下图所示（ Watermark 设置为 2）：<br>
<img src="https://img-blog.csdnimg.cn/20210517100742702.png#pic_center" alt="在这里插入图片描述"><br>
当 Flink 接收到数据时， 会按照一定的规则去生成 Watermark， 这条 Watermark 就等于当前所有到达数据中的 maxEventTime - 延迟时长，也就是说，Watermark 是基于数据携带的时间戳生成的， 一旦 Watermark 比当前未触发的窗口的停止时间要晚， 那么就会触发相应窗口的执行。由于 event time 是由数据携带的， 因此， 如果运行过程中无法获取新的数据， 那么没有被触发的窗口将永远都不被触发。<br>
上图中，我们设置的允许最大延迟到达时间为 2s，所以时间戳为 7s 的事件对应的 Watermark 是 5s， 时间戳为 12s 的事件的 Watermark 是 10s， 如果我们的窗口 1 是 1s~5s， 窗口 2 是 6s~10s， 那么时间戳为 7s 的事件到达时的 Watermarker 恰好触发窗口 1， 时间戳为 12s 的事件到达时的 Watermark 恰好触发窗口 2。<br>
Watermark 就是触发前一窗口的“关窗时间”， 一旦触发关门那么以当前时刻为准在窗口范围内的所有所有数据都会收入窗中。<br>
只要没有达到水位那么不管现实中的时间推进了多久都不会触发关窗。</p>
<h3 id="3-2-Watermark-的引入">3.2 Watermark 的引入</h3>
<p>watermark 的引入很简单， 对于乱序数据， 最常见的引用方式如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dataStream.assignTimestampsAndWatermarks( <span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">SensorReading</span>](<span class="type">Time</span>.milliseconds(<span class="number">1000</span>)) &#123;</span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">SensorReading</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">		element.timestamp * <span class="number">1000</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125; )</span><br></pre></td></tr></table></figure>
<p>Event Time 的使用一定要指定数据源中的时间戳。否则程序无法知道事件的事件时间是什么(数据源里的数据没有时间戳的话， 就只能使用 Processing Time 了)。我们看到上面的例子中创建了一个看起来有点复杂的类， 这个类实现的其实就是分配时间戳的接口。Flink 暴露了 TimestampAssigner 接口供我们实现， 使我们可以自定义如何从事件数据中抽取时间戳。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从调用时刻开始给env 创建的每一个stream 追加时间特性</span></span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> readings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = env</span><br><span class="line">	.addSource(<span class="keyword">new</span> <span class="type">SensorSource</span>)</span><br><span class="line">	.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">MyAssigner</span>())</span><br></pre></td></tr></table></figure>
<p>MyAssigner 有两种类型</p>
<ul>
<li>AssignerWithPeriodicWatermarks</li>
<li>AssignerWithPunctuatedWatermarks</li>
</ul>
<p>以上两个接口都继承自 TimestampAssigner</p>
<h4 id="Assigner-with-periodic-watermarks">Assigner with periodic watermarks</h4>
<p>周期性的生成 watermark： 系统会周期性的将 watermark 插入到流中(水位线也是一种特殊的事件)。默认周期是 200 毫秒。可以使用ExecutionConfig.setAutoWatermarkInterval()方法进行设置。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 每隔 5 秒产生一个watermark</span></span><br><span class="line">env.getConfig.setAutoWatermarkInterval(<span class="number">5000</span>)</span><br></pre></td></tr></table></figure>
<p>产生 watermark 的逻辑： 每隔 5 秒钟， Flink 会调用<br>
AssignerWithPeriodicWatermarks 的 getCurrentWatermark()方法。如果方法返回一个时间戳大于之前水位的时间戳， 新的 watermark 会被插入到流中。这个检查保证了水位线是单调递增的。如果方法返回的时间戳小于等于之前水位的时间戳，  则不会产生新的 watermark。<br>
<strong>例：自定义一个周期性的时间戳抽取</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PeriodicAssigner</span> <span class="keyword">extends</span> <span class="title">AssignerWithPeriodicWatermarks</span>[<span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line">	<span class="keyword">val</span> bound: <span class="type">Long</span> = <span class="number">60</span> * <span class="number">1000</span> <span class="comment">// 延时为1 分钟</span></span><br><span class="line">	<span class="keyword">var</span> maxTs: <span class="type">Long</span> = <span class="type">Long</span>.<span class="type">MinValue</span> <span class="comment">// 观察到的最大时间戳</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCurrentWatermark</span></span>: <span class="type">Watermark</span> = &#123;</span><br><span class="line">		<span class="keyword">new</span> <span class="type">Watermark</span>(maxTs - bound)</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(r: <span class="type">SensorReading</span>, previousTS: <span class="type">Long</span>) = &#123;</span><br><span class="line">		maxTs = maxTs.max(r.timestamp)</span><br><span class="line">		r.timestamp</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>一种简单的特殊情况是， 如果我们事先得知数据流的时间戳是单调递增的， 也就是说没有乱序， 那我们可以使用 assignAscendingTimestamps， 这个方法会直接使用数据的时间戳生成 watermark。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> withTimestampsAndWatermarks = stream</span><br><span class="line">	.assignAscendingTimestamps(e =&gt; e.timestamp)</span><br><span class="line"></span><br><span class="line">&gt;&gt; result:	<span class="type">E</span>(<span class="number">1</span>), <span class="type">W</span>(<span class="number">1</span>), <span class="type">E</span>(<span class="number">2</span>), <span class="type">W</span>(<span class="number">2</span>), ...</span><br></pre></td></tr></table></figure>
<p>而对于乱序数据流， 如果我们能大致估算出数据流中的事件的最大延迟时间， 就可以使用如下代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(</span><br><span class="line">	<span class="keyword">new</span> <span class="type">SensorTimeAssigner</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SensorTimeAssigner</span> <span class="keyword">extends</span> <span class="title">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">SensorReading</span>](<span class="params"><span class="type">Time</span>.seconds(5</span>)) </span>&#123;</span><br><span class="line">	<span class="comment">// 抽取时间戳</span></span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(r: <span class="type">SensorReading</span>): <span class="type">Long</span> = r.timestamp</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&gt;&gt; relust:	<span class="type">E</span>(<span class="number">10</span>), <span class="type">W</span>(<span class="number">0</span>), <span class="type">E</span>(<span class="number">8</span>), <span class="type">E</span>(<span class="number">7</span>), <span class="type">E</span>(<span class="number">11</span>), <span class="type">W</span>(<span class="number">1</span>), ...</span><br></pre></td></tr></table></figure>
<h4 id="Assigner-with-punctuated-watermarks">Assigner with punctuated watermarks</h4>
<p>间断式地生成 watermark。和周期性生成的方式不同，这种方式不是固定时间的， 而是可以根据需要对每条数据进行筛选和处理。直接上代码来举个例子， 我们只给sensor_1 的传感器的数据流插入 watermark：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PunctuatedAssigner</span> <span class="keyword">extends</span> <span class="title">AssignerWithPunctuatedWatermarks</span>[<span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line">	<span class="keyword">val</span> bound: <span class="type">Long</span> = <span class="number">60</span> * <span class="number">1000</span></span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">checkAndGetNextWatermark</span></span>(r: <span class="type">SensorReading</span>, extractedTS: <span class="type">Long</span>): <span class="type">Watermark</span> = &#123;</span><br><span class="line">		<span class="keyword">if</span> (r.id == <span class="string">&quot;sensor_1&quot;</span>) &#123;</span><br><span class="line">			<span class="keyword">new</span> <span class="type">Watermark</span>(extractedTS - bound)</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="literal">null</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(r: <span class="type">SensorReading</span>, previousTS: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">		r.timestamp</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="4-EventTime-在window-中的使用">4. EventTime 在window 中的使用</h2>
<h3 id="4-1-滚动窗口（TumblingEventTimeWindows）">4.1 滚动窗口（TumblingEventTimeWindows）</h3>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	<span class="comment">// 环 境</span></span><br><span class="line">	<span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">	</span><br><span class="line">	env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>) env.setParallelism(<span class="number">1</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> dstream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>,<span class="number">7777</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> textWithTsDstream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>)] = dstream.map&#123;</span><br><span class="line">		text =&gt;</span><br><span class="line">			<span class="keyword">val</span> arr: <span class="type">Array</span>[<span class="type">String</span>] = text.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">			(arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong, <span class="number">1</span>)</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> textWithEventTimeDstream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>)] = textWithTsDstream.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>)](<span class="type">Time</span>.milliseconds(<span class="number">1000</span>)) &#123;</span><br><span class="line">		<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>)): <span class="type">Long</span> = &#123;</span><br><span class="line">			<span class="keyword">return</span>	element._2</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> textKeyStream: <span class="type">KeyedStream</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>), <span class="type">Tuple</span>] = textWithEventTimeDstream.keyBy(<span class="number">0</span>)</span><br><span class="line">	textKeyStream.print(<span class="string">&quot;textkey:&quot;</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> windowStream: <span class="type">WindowedStream</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>), <span class="type">Tuple</span>, <span class="type">TimeWindow</span>] = textKeyStream.window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">2</span>)))</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> groupDstream: <span class="type">DataStream</span>[mutable.<span class="type">HashSet</span>[<span class="type">Long</span>]] = windowStream.fold(<span class="keyword">new</span> mutable.<span class="type">HashSet</span>[<span class="type">Long</span>]()) &#123;</span><br><span class="line">		<span class="keyword">case</span> (set, (key, ts, count)) =&gt;</span><br><span class="line">			set += ts</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	groupDstream.print(<span class="string">&quot;window::::&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">	env.execute()</span><br><span class="line">	</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>结果是按照 Event Time 的时间窗口计算得出的，而无关系统的时间（包括输入的快慢）。</p>
<h3 id="4-2-滑动窗口（SlidingEventTimeWindows）">4.2 滑动窗口（SlidingEventTimeWindows）</h3>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	<span class="comment">// 环 境</span></span><br><span class="line">	<span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">	</span><br><span class="line">	env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">	env.setParallelism(<span class="number">1</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> dstream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>,<span class="number">7777</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> textWithTsDstream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>)] = dstream.map &#123;</span><br><span class="line">		text =&gt;</span><br><span class="line">			<span class="keyword">val</span> arr: <span class="type">Array</span>[<span class="type">String</span>] = text.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">			(arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong, <span class="number">1</span>)</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> textWithEventTimeDstream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>)] = textWithTsDstream.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>)](<span class="type">Time</span>.milliseconds(<span class="number">1000</span>)) &#123;</span><br><span class="line">		<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>)): <span class="type">Long</span> = &#123;</span><br><span class="line">			<span class="keyword">return</span> element._2</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> textKeyStream: <span class="type">KeyedStream</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>), <span class="type">Tuple</span>] = textWithEventTimeDstream.keyBy(<span class="number">0</span>)</span><br><span class="line">	textKeyStream.print(<span class="string">&quot;textkey:&quot;</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> windowStream: <span class="type">WindowedStream</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>), <span class="type">Tuple</span>, <span class="type">TimeWindow</span>] = textKeyStream.window(<span class="type">SlidingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">2</span>),<span class="type">Time</span>.millis econds(<span class="number">500</span>)))</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> groupDstream: <span class="type">DataStream</span>[mutable.<span class="type">HashSet</span>[<span class="type">Long</span>]] = windowStream.fold(<span class="keyword">new</span> mutable.<span class="type">HashSet</span>[<span class="type">Long</span>]()) &#123; </span><br><span class="line">		<span class="keyword">case</span> (set, (key, ts, count)) =&gt;</span><br><span class="line">			set += ts</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	groupDstream.print(<span class="string">&quot;window::::&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">	env.execute()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-3-会话窗口（EventTimeSessionWindows）">4.3 会话窗口（EventTimeSessionWindows）</h3>
<p>相邻两次数据的 EventTime 的时间差超过指定的时间间隔就会触发执行。如果加入 Watermark，会在符合窗口触发的情况下进行延迟，到达延迟水位再进行窗口触发。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	<span class="comment">// 环 境</span></span><br><span class="line">	<span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">	env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>) env.setParallelism(<span class="number">1</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> dstream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>,<span class="number">7777</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> textWithTsDstream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>)] = dstream.map &#123;</span><br><span class="line">		text =&gt;</span><br><span class="line">		<span class="keyword">val</span> arr: <span class="type">Array</span>[<span class="type">String</span>] = text.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">		(arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong, <span class="number">1</span>)</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> textWithEventTimeDstream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>)] = textWithTsDstream.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>)](<span class="type">Time</span>.milliseconds(<span class="number">1000</span>)) &#123;</span><br><span class="line">		<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>)): <span class="type">Long</span> = &#123;</span><br><span class="line">			<span class="keyword">return</span>	element._2</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> textKeyStream: <span class="type">KeyedStream</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>), <span class="type">Tuple</span>] = textWithEventTimeDstream.keyBy(<span class="number">0</span>)</span><br><span class="line">	textKeyStream.print(<span class="string">&quot;textkey:&quot;</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> windowStream: <span class="type">WindowedStream</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">Int</span>), <span class="type">Tuple</span>, <span class="type">TimeWindow</span>] = textKeyStream.window(<span class="type">EventTimeSessionWindows</span>.withGap(<span class="type">Time</span>.milliseconds(<span class="number">500</span>)))</span><br><span class="line">	</span><br><span class="line">	windowStream.reduce(</span><br><span class="line">			(text1,text2) =&gt; (text1._1,<span class="number">0</span>L,text1._3+text2._3)</span><br><span class="line">		) .map(_._3).print(<span class="string">&quot;windows:::&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">	</span><br><span class="line">	env.execute()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="八、ProcessFunction-API（底层-API）">八、ProcessFunction API（底层 API）</h1>
<p>我们之前学习的<strong>转换算子</strong>是无法访问事件的时间戳信息和水位线信息的。而这  在一些应用场景下， 极为重要。例如 MapFunction 这样的 map 转换算子就无法访问时间戳或者当前事件的事件时间。<br>
基于此， DataStream API 提供了一系列的 Low-Level 转换算子。可以<strong>访问时间戳、watermark 以及注册定时事件</strong>。还可以输出<strong>特定的一些事件</strong>，例如超时事件等。Process Function 用来构建事件驱动的应用以及实现自定义的业务逻辑(使用之前的window 函数和转换算子无法实现)。例如， Flink SQL 就是使用 Process Function 实现的。<br>
Flink 提供了 8 个 Process Function：</p>
<ul>
<li>ProcessFunction</li>
<li>KeyedProcessFunction</li>
<li>CoProcessFunction</li>
<li>ProcessJoinFunction</li>
<li>BroadcastProcessFunction</li>
<li>KeyedBroadcastProcessFunction</li>
<li>ProcessWindowFunction</li>
<li>ProcessAllWindowFunction</li>
</ul>
<h2 id="1-KeyedProcessFunction">1. KeyedProcessFunction</h2>
<p>KeyedProcessFunction 用来操作 KeyedStream。KeyedProcessFunction 会处理流的每一个元素，输出为 0 个、1 个或者多个元素。所有的 Process Function 都继承自RichFunction 接口， 所以都有 open()、close()和 getRuntimeContext()等方法。而KeyedProcessFunction[KEY, IN, OUT] 还额外提供了两个方法：</p>
<ul>
<li>processElement(v: IN, ctx: Context, out: Collector[OUT]), 流中的每一个元素都会调用这个方法， 调用结果将会放在 Collector 数据类型中输出。Context 可以访问元素的时间戳，元素的 key，以及 TimerService 时间服务。Context 还可以将结果输出到别的流(side outputs)。</li>
<li>onTimer(timestamp: Long, ctx: OnTimerContext, out: Collector[OUT])是一个回调函数。当之前注册的定时器触发时调用。参数 timestamp 为定时器所设定的触发的时间戳。Collector 为输出结果的集合。OnTimerContext 和processElement 的 Context 参数一样，提供了上下文的一些信息，例如定时器触发的时间信息(事件时间或者处理时间)。</li>
</ul>
<h2 id="2-TimerService-和-定时器（Timers）">2. TimerService 和 定时器（Timers）</h2>
<p>Context 和 OnTimerContext 所持有的 TimerService 对象拥有以下方法:</p>
<ul>
<li>currentProcessingTime(): Long 返回当前处理时间</li>
<li>currentWatermark(): Long 返回当前 watermark 的时间戳</li>
<li>registerProcessingTimeTimer(timestamp: Long): Unit 会注册当前 key 的 processing time 的定时器。当 processing time 到达定时时间时， 触发 timer</li>
<li>registerEventTimeTimer(timestamp: Long): Unit 会注册当前 key 的 event time 定时器。当水位线大于等于定时器注册的时间时，触发定时器执行回调函数</li>
<li>deleteProcessingTimeTimer(timestamp: Long): Unit 删除之前注册处理时间定时器。如果没有这个时间戳的定时器， 则不执行</li>
<li>deleteEventTimeTimer(timestamp: Long): Unit 删除之前注册的事件时间定时器， 如果没有此时间戳的定时器， 则不执行</li>
</ul>
<p>当定时器 timer 触发时， 会执行回调函数 onTimer()。注意定时器 timer 只能在keyed streams 上面使用。<br>
下面举个例子说明 KeyedProcessFunction 如何操作 KeyedStream。</p>
<ul>
<li>需求： 监控温度传感器的温度值， 如果温度值在一秒钟之内(processing time)连续上升， 则报警。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> warnings = readings</span><br><span class="line">	.keyBy(_.id)</span><br><span class="line">	.process(<span class="keyword">new</span> <span class="type">TempIncreaseAlertFunction</span>)</span><br></pre></td></tr></table></figure>
<p>看一下 TempIncreaseAlertFunction 如何实现, 程序中使用了 ValueState 这样一个 状态变量</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TempIncreaseAlertFunction</span> <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">SensorReading</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 保存上一个传感器温度值</span></span><br><span class="line">	<span class="keyword">lazy</span> <span class="keyword">val</span> lastTemp: <span class="type">ValueState</span>[<span class="type">Double</span>] = getRuntimeContext.getState(</span><br><span class="line">		<span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Double</span>](<span class="string">&quot;lastTemp&quot;</span>, <span class="type">Types</span>.of[<span class="type">Double</span>])</span><br><span class="line">	)</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 保存注册的定时器的时间戳</span></span><br><span class="line">	<span class="keyword">lazy</span> <span class="keyword">val</span> currentTimer: <span class="type">ValueState</span>[<span class="type">Long</span>] = getRuntimeContext.getState(</span><br><span class="line">		<span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Long</span>](<span class="string">&quot;timer&quot;</span>, <span class="type">Types</span>.of[<span class="type">Long</span>])</span><br><span class="line">	)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(r: <span class="type">SensorReading</span>,</span><br><span class="line">								ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">SensorReading</span>, <span class="type">String</span>]#<span class="type">Context</span>,</span><br><span class="line">								out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">		<span class="comment">// 取出上一次的温度</span></span><br><span class="line">		<span class="keyword">val</span> prevTemp = lastTemp.value()</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 将当前温度更新到上一次的温度这个变量中</span></span><br><span class="line">		lastTemp.update(r.temperature)</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">val</span> curTimerTimestamp = currentTimer.value()</span><br><span class="line">		<span class="keyword">if</span> (prevTemp == <span class="number">0.0</span> || r.temperature &lt; prevTemp) &#123;</span><br><span class="line">			<span class="comment">// 温度下降或者是第一个温度值，删除定时器</span></span><br><span class="line">			ctx.timerService().deleteProcessingTimeTimer(curTimerTimestamp)</span><br><span class="line">			<span class="comment">// 清空状态变量</span></span><br><span class="line">			currentTimer.clear()</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (r.temperature &gt; prevTemp &amp;&amp; curTimerTimestamp == <span class="number">0</span>) &#123;</span><br><span class="line">			<span class="comment">// 温度上升且我们并没有设置定时器</span></span><br><span class="line">			<span class="keyword">val</span> timerTs = ctx.timerService().currentProcessingTime() + <span class="number">1000</span> ctx.timerService().registerProcessingTimeTimer(timerTs)</span><br><span class="line">			currentTimer.update(timerTs)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTimer</span></span>(ts: <span class="type">Long</span>,</span><br><span class="line">						ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">SensorReading</span>, <span class="type">String</span>]#<span class="type">OnTimerContext</span>,</span><br><span class="line">						out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">		out.collect(<span class="string">&quot;传感器 id 为: &quot;</span> + ctx.getCurrentKey + <span class="string">&quot;的传感器温度值已经连续 1s 上升了。&quot;</span>)</span><br><span class="line">		currentTimer.clear()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-侧输出流（SideOutput）">3. 侧输出流（SideOutput）</h2>
<p>大部分的 DataStream API 的算子的输出是单一输出，也就是某种数据类型的流。除了 split 算子， 可以将一条流分成多条流， 这些流的数据类型也都相同。process function 的 side outputs 功能可以产生多条流， 并且这些流的数据类型可以不一样。一个 side output 可以定义为 OutputTag[X]对象， X 是输出流的数据类型。process function 可以通过 Context 对象发射一个事件到一个或者多个 side outputs。<br>
下面是一个示例程序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> monitoredReadings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = readings.process(<span class="keyword">new</span> <span class="type">FreezingMonitor</span>)</span><br><span class="line"></span><br><span class="line">monitoredReadings</span><br><span class="line">	.getSideOutput(<span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">String</span>](<span class="string">&quot;freezing-alarms&quot;</span>))</span><br><span class="line">	.print()</span><br><span class="line"></span><br><span class="line">readings.print()</span><br></pre></td></tr></table></figure>
<p>接下来我们实现 FreezingMonitor 函数，用来监控传感器温度值，将温度值低于32F 的温度输出到 side output</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FreezingMonitor</span> <span class="keyword">extends</span> <span class="title">ProcessFunction</span>[<span class="type">SensorReading</span>, <span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 定义一个侧输出标签</span></span><br><span class="line">	<span class="keyword">lazy</span> <span class="keyword">val</span> freezingAlarmOutput: <span class="type">OutputTag</span>[<span class="type">String</span>] = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">String</span>](<span class="string">&quot;freezing-alarms&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(r: <span class="type">SensorReading</span>,</span><br><span class="line">								ctx: <span class="type">ProcessFunction</span>[<span class="type">SensorReading</span>, <span class="type">SensorReading</span>]#<span class="type">Context</span>,</span><br><span class="line">								out: <span class="type">Collector</span>[<span class="type">SensorReading</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">		<span class="comment">// 温度在32F 以下时，输出警告信息</span></span><br><span class="line">		<span class="keyword">if</span> (r.temperature &lt; <span class="number">32.0</span>) &#123;</span><br><span class="line">			ctx.output(freezingAlarmOutput, <span class="string">s&quot;Freezing Alarm for <span class="subst">$&#123;r.id&#125;</span>&quot;</span>)</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 所有数据直接常规输出到主流</span></span><br><span class="line">		out.collect(r)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="4-CoProcessFunction">4. CoProcessFunction</h2>
<p>对于两条输入流， DataStream API 提供了 CoProcessFunction 这样的 low-level 操作。CoProcessFunction 提供了操作每一个输入流的方法: processElement1()和processElement2()。<br>
类似于 ProcessFunction， 这两种方法都通过 Context 对象来调用。这个 Context 对象可以访问事件数据， 定时器时间戳， TimerService， 以及 side outputs。<br>
CoProcessFunction 也提供了 onTimer()回调函数。</p>
<h1 id="九、状态编程和容错机制">九、状态编程和容错机制</h1>
<p>流式计算分为无状态和有状态两种情况。无状态的计算观察每个独立事件， 并根据最后一个事件输出结果。例如， 流处理应用程序从传感器接收温度读数， 并在温度超过 90 度时发出警告。有状态的计算则会基于多个事件输出结果。以下是一些例子：</p>
<ul>
<li>所有类型的窗口。例如， 计算过去一小时的平均温度，就是有状态的计算</li>
<li>所有用于复杂事件处理的状态机。例如，若在一分钟内收到两个相差 20 度以上的温度读数， 则发出警告， 这是有状态的计算</li>
<li>流与流之间的所有关联操作， 以及流与静态表或动态表之间的关联操作， 都是有状态的计算</li>
</ul>
<p>下图展示了无状态流处理和有状态流处理的主要区别。无状态流处理分别接收每条数据记录(图中的黑条)，然后根据最新输入的数据生成输出数据(白条)；有状态流处理会维护状态(根据每条输入记录进行更新)， 并基于最新输入的记录和当前的状态值生成输出记录(灰条)<br>
<img src="https://img-blog.csdnimg.cn/20210519104814423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
上图中输入数据由黑条表示。无状态流处理每次只转换一条输入记录， 并且仅根据最新的输入记录输出结果(白条)。有状态流处理维护所有已处理记录的状态值， 并根据每条新输入的记录更新状态，因此输出记录(灰条)反映的是综合考虑多个事件之后的结果。<br>
尽管无状态的计算很重要， 但是流处理对有状态的计算更感兴趣。事实上， 正确地实现有状态的计算比实现无状态的计算难得多。旧的流处理系统并不支持有状 态的计算， 而新一代的流处理系统则将状态及其正确性视为重中之重。</p>
<h2 id="1-有状态的算子和应用程序">1. 有状态的算子和应用程序</h2>
<p>Flink 内置的很多算子，数据源 source，数据存储 sink 都是有状态的，流中的数据都是 buffer records，会保存一定的元素或者元数据。例如: ProcessWindowFunction 会缓存输入流的数据， ProcessFunction 会保存设置的定时器信息等等。<br>
在 Flink 中， 状态始终与特定算子相关联。总的来说， 有两种类型的状态：</p>
<ul>
<li>算子状态（ operator state）</li>
<li>键控状态（ keyed state）</li>
</ul>
<h3 id="1-1-算子状态（operator-state）">1.1 算子状态（operator state）</h3>
<p>算子状态的作用范围限定为算子任务。这意味着由同一并行任务所处理的所有数据都可以访问到相同的状态，  状态对于同一任务而言是共享的。算子状态不能由相同或不同算子的另一个任务访问。<br>
<img src="https://img-blog.csdnimg.cn/20210519105207400.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
Flink 为算子状态提供三种基本数据结构：</p>
<ul>
<li>列表状态（List state）
<ul>
<li>将状态表示为一组数据的列表</li>
</ul>
</li>
<li>联合列表状态（Union list state）
<ul>
<li>也将状态表示为数据的列表。它与常规列表状态的区别在于，在发生故障时，或者从保 存点（savepoint）启动应用程序时如何恢复</li>
</ul>
</li>
<li>广播状态（Broadcast state）
<ul>
<li>如果一个算子有多项任务，而它的每项任务状态又都相同，那么这种特殊情况最适合应用广播状态</li>
</ul>
</li>
</ul>
<h3 id="1-2-键控状态（keyed-state）">1.2 键控状态（keyed state）</h3>
<p>键控状态是根据输入数据流中定义的键（key）来维护和访问的。Flink 为每个键值维护一个状态实例，并将具有相同键的所有数据，都分区到同一个算子任务中，这个任务会维护和处理这个 key 对应的状态。当任务处理一条数据时，它会自动将状态的访问范围限定为当前数据的 key。因此，具有相同 key 的所有数据都会访问相同的状态。Keyed State 很类似于一个分布式的 key-value map 数据结构，只能用于 KeyedStream（ keyBy 算子处理之后）<br>
<img src="https://img-blog.csdnimg.cn/20210519105522865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
Flink 的 Keyed State 支持以下数据类型：</p>
<ul>
<li>ValueState[T]保存单个的值，值的类型为 T
<ul>
<li>get 操作: ValueState.value()</li>
<li>set 操作: ValueState.update(value: T)</li>
</ul>
</li>
<li>ListState[T]保存一个列表，列表里的元素的数据类型为 T。基本操作如下：
<ul>
<li>ListState.add(value: T)</li>
<li>ListState.addAll(values: java.util.List[T])</li>
<li>oListState.get() 返回 Iterable[T]</li>
<li>ListState.update(values: java.util.List[T])</li>
</ul>
</li>
<li>MapState[K, V]保存 Key-Value 对
<ul>
<li>MapState.get(key: K)</li>
<li>MapState.put(key: K, value: V)</li>
<li>MapState.contains(key: K)</li>
<li>MapState.remove(key: K)</li>
</ul>
</li>
<li>ReducingState[T]</li>
<li>AggregatingState[I, O]</li>
</ul>
<p>State.clear()是清空操作</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sensorData: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = ...</span><br><span class="line"><span class="keyword">val</span> keyedData: <span class="type">KeyedStream</span>[<span class="type">SensorReading</span>, <span class="type">String</span>] = sensorData.keyBy(_.id)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> alerts: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)] = keyedData</span><br><span class="line">	.flatMap(<span class="keyword">new</span> <span class="type">TemperatureAlertFunction</span>(<span class="number">1.7</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemperatureAlertFunction</span>(<span class="params">val threshold: <span class="type">Double</span></span>) <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>[<span class="type">SensorReading</span>, (<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)] </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">var</span> lastTempState: <span class="type">ValueState</span>[<span class="type">Double</span>] = _</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">	</span><br><span class="line">		<span class="keyword">val</span> lastTempDescriptor = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Double</span>](<span class="string">&quot;lastTemp&quot;</span>, classOf[<span class="type">Double</span>])</span><br><span class="line">	</span><br><span class="line">		lastTempState = getRuntimeContext.getState[<span class="type">Double</span>](lastTempDescriptor)</span><br><span class="line">	</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(reading: <span class="type">SensorReading</span>,</span><br><span class="line">						out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	</span><br><span class="line">		<span class="keyword">val</span> lastTemp = lastTempState.value()</span><br><span class="line">		<span class="keyword">val</span> tempDiff = (reading.temperature - lastTemp).abs</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (tempDiff &gt; threshold) &#123;</span><br><span class="line">			out.collect((reading.id, reading.temperature, tempDiff))</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">this</span>.lastTempState.update(reading.temperature)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过 RuntimeContext 注册 StateDescriptor。StateDescriptor 以状态 state 的名字和存储的数据类型为参数。<br>
在 open()方法中创建 state 变量。注意复习之前的 RichFunction 相关知识。<br>
接下来我们使用了 FlatMap with keyed ValueState 的快捷方式 flatMapWithState 实现以上需求：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> alerts: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>)] = keyedSensorData</span><br><span class="line">	.flatMapWithState[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Double</span>), <span class="type">Double</span>] &#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">case</span> (in: <span class="type">SensorReading</span>, <span class="type">None</span>) =&gt;</span><br><span class="line">		(<span class="type">List</span>.empty, <span class="type">Some</span>(in.temperature))</span><br><span class="line">	<span class="keyword">case</span> (r: <span class="type">SensorReading</span>, lastTemp: <span class="type">Some</span>[<span class="type">Double</span>]) =&gt;</span><br><span class="line">		<span class="keyword">val</span> tempDiff = (r.temperature - lastTemp.get).abs</span><br><span class="line">		<span class="keyword">if</span> (tempDiff &gt; <span class="number">1.7</span>) &#123;</span><br><span class="line">			(<span class="type">List</span>((r.id, r.temperature, tempDiff)), <span class="type">Some</span>(r.temperature))</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			(<span class="type">List</span>.empty, <span class="type">Some</span>(r.temperature))</span><br><span class="line">		&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="2-状态一致性">2. 状态一致性</h2>
<p>当在分布式系统中引入状态时， 自然也引入了一致性问题。一致性实际上是&quot; 正确性级别&quot;的另一种说法，也就是说在成功处理故障并恢复之后得到的结果，与没  有发生任何故障时得到的结果相比， 前者到底有多正确？ 举例来说， 假设要对最近一小时登录的用户计数。在系统经历故障之后， 计数结果是多少？ 如果有偏差， 是有漏掉的计数还是重复计数？</p>
<h3 id="2-1-一致性级别">2.1 一致性级别</h3>
<p>在流处理中， 一致性可以分为 3 个级别：</p>
<ul>
<li>at-most-once: 这其实是没有正确性保障的委婉说法——故障发生之后， 计数结果可能丢失。同样的还有 udp</li>
<li>at-least-once: 这表示计数结果可能大于正确值， 但绝不会小于正确值。也就是说， 计数程序在发生故障后可能多算， 但是绝不会少算</li>
<li>exactly-once: 这指的是系统保证在发生故障后得到的计数结果与正确值一致</li>
</ul>
<p>曾经， at-least-once 非常流行。第一代流处理器(如 Storm 和 Samza)刚问世时只保证 at-least-once， 原因有二：</p>
<ul>
<li>保证 exactly-once 的系统实现起来更复杂。这在基础架构层(决定什么代表正确， 以及 exactly-once 的范围是什么)和实现层都很有挑战性</li>
<li>流处理系统的早期用户愿意接受框架的局限性， 并在应用层想办法弥补(例如使应用程序具有幂等性， 或者用批量计算层再做一遍计算)</li>
</ul>
<p>最先保证 exactly-once 的系统(Storm Trident 和 Spark Streaming)在性能和表现力这两个方面付出了很大的代价。为了保证 exactly-once，这些系统无法单独地对每条记录运用应用逻辑， 而是同时处理多条(一批)记录， 保证对每一批的处理要么全部成功，要么全部失败。这就导致在得到结果前，必须等待一批记录处理结束。因此，  用户经常不得不使用两个流处理框架(一个用来保证 exactly-once， 另一个用来对每个元素做低延迟处理)， 结果使基础设施更加复杂。曾经， 用户不得不在保证exactly-once 与获得低延迟和效率之间权衡利弊。Flink 避免了这种权衡。<br>
Flink 的一个重大价值在于，<strong>它既保证了 exactly-once， 也具有低延迟和高吞吐的处理能力</strong>。<br>
从根本上说，Flink 通过使自身满足所有需求来避免权衡，它是业界的一次意义重大的技术飞跃。尽管这在外行看来很神奇， 但是一旦了解， 就会恍然大悟。</p>
<h3 id="2-2-端到端（end-to-end）状态一致性">2.2 端到端（end-to-end）状态一致性</h3>
<p>目前我们看到的一致性保证都是由流处理器实现的，也就是说都是在 Flink 流处理器内部保证的； 而在真实应用中， 流处理应用除了流处理器以外还包含了数据源（ 例如 Kafka） 和输出到持久化系统。<br>
端到端的一致性保证， 意味着结果的正确性贯穿了整个流处理应用的始终； 每一个组件都保证了它自己的一致性，  整个端到端的一致性级别取决于所有组件中一致性最弱的组件。具体可以划分如下：</p>
<ul>
<li>内部保证 —— 依赖 checkpoint</li>
<li>source 端 —— 需要外部源可重设数据的读取位置</li>
<li>sink 端 —— 需要保证从故障恢复时， 数据不会重复写入外部系统</li>
</ul>
<p>而对于 sink 端， 又有两种具体的实现方式： 幂等（ Idempotent） 写入和事务性（ Transactional） 写入。</p>
<ul>
<li>幂等写入
<ul>
<li>所谓幂等操作，是说一个操作，可以重复执行很多次，但只导致一次结果更改，  也就是说， 后面再重复执行就不起作用了</li>
</ul>
</li>
<li>事务写入
<ul>
<li>需要构建事务来写入外部系统，构建的事务对应着 checkpoint，等到 checkpoint 真正完成的时候， 才把所有对应的结果写入 sink 系统中</li>
</ul>
</li>
</ul>
<p>对于事务性写入， 具体又有两种实现方式： 预写日志（ WAL） 和两阶段提交（ 2PC）。DataStream API 提供了 GenericWriteAheadSink 模板类和 TwoPhaseCommitSinkFunction 接口， 可以方便地实现这两种方式的事务性写入。<br>
不同 Source 和 Sink 的一致性保证可以用下表说明：<br>
<img src="https://img-blog.csdnimg.cn/20210519111516235.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="3-检查点（checkpoint）">3. 检查点（checkpoint）</h2>
<p>Flink 具体如何保证 exactly-once 呢? 它使用一种被称为&quot;检查点&quot;（checkpoint）的特性，在出现故障时将系统重置回正确状态。下面通过简单的类比来解释检查点 的作用。<br>
假设你和两位朋友正在数项链上有多少颗珠子，如下图所示。你捏住珠子，边数边拨，每拨过一颗珠子就给总数加一。你的朋友也这样数他们手中的珠子。当你分神忘记数到哪里时，怎么办呢? 如果项链上有很多珠子，你显然不想从头再数一 遍，尤其是当三人的速度不一样却又试图合作的时候，更是如此(比如想记录前一分钟三人一共数了多少颗珠子，回想一下一分钟滚动窗口)。<br>
<img src="https://img-blog.csdnimg.cn/20210519111950813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
于是，你想了一个更好的办法：在项链上每隔一段就松松地系上一根有色皮筋，将珠子分隔开；当珠子被拨动的时候，皮筋也可以被拨动； 然后，你安排一个助手， 让他在你和朋友拨到皮筋时记录总数。用这种方法，当有人数错时，就不必从头开始数。相反，你向其他人发出错误警示，然后你们都从上一根皮筋处开始重数，助手则会告诉每个人重数时的起始数值，例如在粉色皮筋处的数值是多少。<br>
Flink 检查点的作用就类似于皮筋标记。数珠子这个类比的关键点是：对于指定的皮筋而言，珠子的相对位置是确定的;  这让皮筋成为重新计数的参考点。总状态(珠子的总数)在每颗珠子被拨动之后更新一次，助手则会保存与每根皮筋对应的检查点状态，如当遇到粉色皮筋时一共数了多少珠子，当遇到橙色皮筋时又是多少。当问题出现时，这种方法使得重新计数变得简单。</p>
<h3 id="3-1-Flink的检查点算法">3.1 Flink的检查点算法</h3>
<p>Flink 检查点的核心作用是确保状态正确，即使遇到程序中断，也要正确。记住 这一基本点之后，我们用一个例子来看检查点是如何运行的。Flink 为用户提供了用 来定义状态的工具。例如，以下这个 Scala 程序按照输入记录的第一个字段(一个字 符串)进行分组并维护第二个字段的计数状态。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = ... </span><br><span class="line"><span class="keyword">val</span> counts: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = stream</span><br><span class="line">	.keyBy(record =&gt; record._1)</span><br><span class="line">	.mapWithState(	(in: (<span class="type">String</span>, <span class="type">Int</span>), state: <span class="type">Option</span>[<span class="type">Int</span>])	=&gt; </span><br><span class="line">		state <span class="keyword">match</span> &#123; </span><br><span class="line">			<span class="keyword">case</span> <span class="type">Some</span>(c) =&gt; ( (in._1, c + in._2), <span class="type">Some</span>(c + in._2) ) </span><br><span class="line">			<span class="keyword">case</span> <span class="type">None</span> =&gt; ( (in._1, in._2), <span class="type">Some</span>(in._2) )</span><br><span class="line">	&#125;)</span><br></pre></td></tr></table></figure>
<p>该程序有两个算子: keyBy 算子用来将记录按照第一个元素(一个字符串)进行分 组，根据该 key 将数据进行重新分区，然后将记录再发送给下一个算子: 有状态的 map 算子(mapWithState)。map 算子在接收到每个元素后，将输入记录的第二个字段 的数据加到现有总数中，再将更新过的元素发射出去。下图表示程序的初始状态: 输 入流中的 6 条记录被检查点分割线(checkpoint barrier)隔开，所有的 map 算子状态均为 0(计数还未开始)。所有 key 为 a 的记录将被顶层的 map 算子处理，所有 key 为 b 的记录将被中间层的 map 算子处理，所有 key 为 c 的记录则将被底层的 map 算子处理。<br>
<img src="https://img-blog.csdnimg.cn/20210519112714781.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="按 key 累加计数程序初始状态"><br>
上图是程序的初始状态。注意，a、b、c 三组的初始计数状态都是 0，即三个圆 柱上的值。ckpt 表示检查点分割线（checkpoint barriers）。每条记录在处理顺序上 严格地遵守在检查点之前或之后的规定，例如[“b”,2]在检查点之前被处理，[“a”,2] 则在检查点之后被处理。<br>
当该程序处理输入流中的 6 条记录时，涉及的操作遍布 3 个并行实例(节点、CPU 内核等)。那么，检查点该如何保证 exactly-once 呢?<br>
检查点分割线和普通数据记录类似。它们由算子处理，但并不参与计算，而是 会触发与检查点相关的行为。当读取输入流的数据源(在本例中与 keyBy 算子内联) 遇到检查点屏障时，它将其在输入流中的位置保存到持久化存储中。如果输入流来 自消息传输系统(Kafka)，这个位置就是偏移量。Flink 的存储机制是插件化的，持久 化存储可以是分布式文件系统，如 HDFS。下图展示了这个过程（遇到 checkpoint barrier 时， 保存其在输入流中的位置）<br>
<img src="https://img-blog.csdnimg.cn/20210519113405164.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
当 Flink 数据源(在本例中与 keyBy 算子内联)遇到检查点分界线（barrier）时， 它会将其在输入流中的位置保存到持久化存储中。这让 Flink 可以根据该位置重启。<br>
检查点像普通数据记录一样在算子之间流动。当 map 算子处理完前 3 条数据并 收到检查点分界线时，它们会将状态以异步的方式写入持久化存储，如下图所示（保存 map 算子状态， 也就是当前各个 key 的计数值）<br>
<img src="https://img-blog.csdnimg.cn/20210519113643798.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
位于检查点之前的所有记录([“b”,2]、[“b”,3]和[“c”,1])被 map 算子处理之后的情 况。此时，持久化存储已经备份了检查点分界线在输入流中的位置(备份操作发生在 barrier 被输入算子处理的时候)。map 算子接着开始处理检查点分界线，并触发将状 态异步备份到稳定存储中这个动作。<br>
当 map 算子的状态备份和检查点分界线的位置备份被确认之后，该检查点操作 就可以被标记为完成，如下图所示。我们在无须停止或者阻断计算的条件下，在一 个逻辑时间点(对应检查点屏障在输入流中的位置)为计算状态拍了快照。通过确保 备份的状态和位置指向同一个逻辑时间点，后文将解释如何基于备份恢复计算，从 而保证 exactly-once。值得注意的是，当没有出现故障时，Flink 检查点的开销极小， 检查点操作的速度由持久化存储的可用带宽决定。回顾数珠子的例子: 除了因为数 错而需要用到皮筋之外，皮筋会被很快地拨过。<br>
<img src="https://img-blog.csdnimg.cn/20210519113733345.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
检查点操作完成，状态和位置均已备份到稳定存储中。输入流中的所有数据记 录都已处理完成。值得注意的是，备份的状态值与实际的状态值是不同的。备份反 映的是检查点的状态。<br>
如果检查点操作失败，Flink 可以丢弃该检查点并继续正常执行，因为之后的某 一个检查点可能会成功。虽然恢复时间可能更长，但是对于状态的保证依旧很有力。 只有在一系列连续的检查点操作失败之后，Flink 才会抛出错误，因为这通常预示着 发生了严重且持久的错误。<br>
现在来看看下图所示的情况：检查点操作已经完成，但故障紧随其后（故障紧跟检查点， 导致最底部的实例丢失）<br>
<img src="https://img-blog.csdnimg.cn/20210519113827989.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
在这种情况下（故障时的状态恢复），Flink 会重新拓扑(可能会获取新的执行资源)，将输入流倒回到 上一个检查点，然后恢复状态值并从该处开始继续计算。在本例中，[“a”,2]、[“a”,2] 和[“c”,2]这几条记录将被重播。<br>
下图展示了这一重新处理过程。从上一个检查点开始重新计算，可以保证在剩 下的记录被处理之后，得到的 map 算子的状态值与没有发生故障时的状态值一致。<br>
<img src="https://img-blog.csdnimg.cn/20210519114227718.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
Flink 将输入流倒回到上一个检查点屏障的位置，同时恢复 map 算子的状态值。 然后，Flink 从此处开始重新处理。这样做保证了在记录被处理之后，map 算子的状 态值与没有发生故障时的一致。<br>
Flink 检查点算法的正式名称是异步分界线快照(asynchronous barrier snapshotting)。该算法大致基于 Chandy-Lamport 分布式快照算法。<br>
检查点是 Flink 最有价值的创新之一，因为<strong>它使 Flink 可以保证 exactly-once， 并且不需要牺牲性能</strong>。</p>
<h3 id="3-2-Flink-Kafka-如何实现端到端的-exactly-once-语义">3.2 Flink+Kafka 如何实现端到端的 exactly-once 语义</h3>
<p>我们知道，端到端的状态一致性的实现，需要每一个组件都实现，对于 Flink + Kafka 的数据管道系统（Kafka 进、Kafka 出）而言，各组件怎样保证 exactly-once 语义呢？</p>
<ul>
<li>内部 —— 利用 checkpoint 机制，把状态存盘，发生故障的时候可以恢复，保证内部的状态一致性</li>
<li>source —— kafka consumer 作为 source，可以将偏移量保存下来，如果后续任务出现了故障，恢复的时候可以由连接器重置偏移量，重新消费数据，保证一致性</li>
<li>sink —— kafka producer 作为 sink，采用两阶段提交 sink，需要实现一个 TwoPhaseCommitSinkFunction</li>
</ul>
<p>内部的 checkpoint 机制我们已经有了了解，那 source 和 sink 具体又是怎样运行 的呢？接下来我们逐步做一个分析。<br>
我们知道 Flink 由 JobManager 协调各个 TaskManager 进行 checkpoint 存储， checkpoint 保存在 StateBackend 中，默认 StateBackend 是内存级的，也可以改为文件级的进行持久化保存。<br>
<img src="https://img-blog.csdnimg.cn/20210519114544418.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
当 checkpoint 启动时，JobManager 会将检查点分界线（barrier）注入数据流； barrier 会在算子间传递下去。<br>
<img src="https://img-blog.csdnimg.cn/20210519114628309.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
每个算子会对当前的状态做个快照，保存到状态后端。对于 source 任务而言， 就会把当前的 offset 作为状态保存起来。下次从 checkpoint 恢复时，source 任务可以重新提交偏移量，从上次保存的位置开始重新消费数据。<br>
<img src="https://img-blog.csdnimg.cn/20210519114708854.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
每个内部的 transform 任务遇到 barrier 时，都会把状态存到 checkpoint 里。<br>
sink 任务首先把数据写入外部 kafka，这些数据都属于预提交的事务（还不能被消费）；当遇到 barrier 时，把状态保存到状态后端，并开启新的预提交事务。<br>
<img src="https://img-blog.csdnimg.cn/20210519114806911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
当所有算子任务的快照完成，也就是这次的 checkpoint 完成时，JobManager 会向所有任务发通知，确认这次 checkpoint 完成。<br>
当 sink 任务收到确认通知，就会正式提交之前的事务，kafka 中未确认的数据 就改为“已确认”，数据就真正可以被消费了。<br>
<img src="https://img-blog.csdnimg.cn/20210519114847109.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
所以我们看到，执行过程实际上是一个两段式提交，每个算子执行完成，会进 行“预提交”，直到执行完 sink 操作，会发起“确认提交”，如果执行失败，预提 交会放弃掉。<br>
具体的两阶段提交步骤总结如下：</p>
<ul>
<li>第一条数据来了之后，开启一个 kafka 的事务（transaction），正常写入 kafka 分区日志但标记为未提交，这就是“预提交”</li>
<li>jobmanager 触发 checkpoint 操作，barrier 从 source 开始向下传递，遇到 barrier 的算子将状态存入状态后端，并通知 jobmanager</li>
<li>sink 连接器收到 barrier，保存当前状态，存入 checkpoint，通知 jobmanager，并开启下一阶段的事务，用于提交下个检查点的数据</li>
<li>jobmanager 收到所有任务的通知，发出确认信息，表示 checkpoint 完成</li>
<li>sink 任务收到 jobmanager 的确认信息，正式提交这段时间的数据</li>
<li>外部 kafka 关闭事务，提交的数据可以正常消费了</li>
</ul>
<p>所以我们也可以看到，如果宕机需要通过 StateBackend 进行恢复，只能恢复所有确认提交的操作。</p>
<h2 id="4-选择一个状态后端（state-backend）">4. 选择一个状态后端（state backend）</h2>
<ul>
<li><strong>MemoryStateBackend</strong>
<ul>
<li>内存级的状态后端，会将键控状态作为内存中的对象进行管理，将它们存储在 TaskManager 的 JVM 堆上；而将 checkpoint 存储在 JobManager 的内存中</li>
</ul>
</li>
<li><strong>FsStateBackend</strong>
<ul>
<li>将 checkpoint 存到远程的持久化文件系统（FileSystem）上。而对于本地状态，跟 MemoryStateBackend 一样，也会存在 TaskManager 的 JVM 堆上</li>
</ul>
</li>
<li><strong>RocksDBStateBackend</strong>
<ul>
<li>将所有状态序列化后，存入本地的 RocksDB 中存储</li>
</ul>
</li>
</ul>
<p><font color=red>注：</font>RocksDB 的支持并不直接包含在 flink 中，需要引入依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-statebackend-rocksdb_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>设置状态后端为 FsStateBackend：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> checkpointPath: <span class="type">String</span> = ???</span><br><span class="line"><span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(checkpointPath)</span><br><span class="line"></span><br><span class="line">env.setStateBackend(backend)</span><br><span class="line"></span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> <span class="type">FsStateBackend</span>(<span class="string">&quot;file:///tmp/checkpoints&quot;</span>))</span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置重启策略</span></span><br><span class="line">env.setRestartStrategy(<span class="type">RestartStrategies</span>.fixedDelayRestart(<span class="number">60</span>, <span class="type">Time</span>.of(<span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>)))</span><br></pre></td></tr></table></figure>
<h2 id="4-Savepoint">4. Savepoint</h2>
<h3 id="4-1-Savepoint介绍">4.1 Savepoint介绍</h3>
<p>Savepoint：保存点，类似于以前玩游戏的时候,遇到难关了/遇到boss了,赶紧手动存个档,然后接着玩,如果失败了,赶紧从上次的存档中恢复,然后接着玩。<br>
<strong>在实际开发中,可能会遇到这样的情况：如要对集群进行停机维护/扩容</strong><br>
那么这时候需要执行一次Savepoint也就是执行一次手动的Checkpoint/也就是手动的发一个barrier栅栏，那么这样的话，程序的所有状态都会被执行快照并保存，当维护/扩容完毕之后,可以从上一次Savepoint的目录中进行恢复！</p>
<h3 id="4-2-Savepoint-VS-Checkpoint">4.2 Savepoint VS Checkpoint</h3>
<p><img src="https://img-blog.csdnimg.cn/20210519143126143.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<ul>
<li><strong>目标</strong>
<ul>
<li>从概念上讲，Savepoints和Checkpoints的不同之处类似于传统数据库中备份和恢复日志的不同。Checkpoints的作用是确保程序有潜在失败可能的情况下（如网络暂时异常），可以正常恢复。相反，Savepoints的作用是让用户手动触发备份后，通过重启来恢复程序</li>
</ul>
</li>
<li><strong>实现</strong>
<ul>
<li>Checkpoints和Savepoints在实现上有所不同。Checkpoints轻量并且快速，它可以利用底层状态存储的各种特性，来实现快速备份和恢复。例如，以RocksDB作为状态存储，状态将会以RocksDB的格式持久化而不是Flink原生的格式，同时利用RocksDB的特性实现了增量Checkpoints。这个特性加速了checkpointing的过程，也是Checkpointing机制中第一个更轻量的实现。相反，Savepoints更注重数据的可移植性，并且支持任何对任务的修改，同时这也让Savepoints的备份和恢复成本相对更高。</li>
</ul>
</li>
<li><strong>生命周期</strong>
<ul>
<li>Checkpoints本身是定时自动触发的。它们的维护、创建和删除都由Flink自身来操作，不需要任何用户的干预。相反，Savepoints的触发、删除和管理等操作都需要用户手动触发。</li>
</ul>
</li>
</ul>
<table>
	<tr align=center>
		<th>维度</th>
		<th>Checkpoints</th>
		<th>Savepoints</th>
	</tr>
	<tr align=center>
		<td>目标</td>
		<td>任务失败的恢复/故障转移机制</td>
		<td>手动备份/重启/恢复任务</td>
	</tr>
	<tr align=center>
		<td>实现</td>
		<td>轻量快速</td>
		<td>注重可移植性，成本较高</td>
	</tr>
	<tr align=center>
		<td>生命周期</td>
		<td>Flink自身控制</td>
		<td>用户手动控制</td>
	</tr>
</table>
<h3 id="4-3-Savepoint演示">4.3 Savepoint演示</h3>
<ol>
<li>启动yarn session<br>
/export/server/flink/bin/yarn-session.sh -n 2 -tm 800 -s 1 -d</li>
<li>运行job-会自动执行Checkpoint<br>
/export/server/flink/bin/flink run --class cn.itcast.checkpoint.CheckpointDemo01 /root/ckp.jar</li>
<li>手动创建savepoint–相当于手动做了一次Checkpoint<br>
/export/server/flink/bin/flink savepoint	702b872ef80f08854c946a544f2ee1a5 hdfs://node1:8020/flink-checkpoint/savepoint/</li>
<li>停止job<br>
/export/server/flink/bin/flink cancel 702b872ef80f08854c946a544f2ee1a5</li>
<li>重新启动job,手动加载savepoint数据<br>
/export/server/flink/bin/flink run -s hdfs://node1:8020/flink-checkpoint/savepoint/savepoint-702b87-0a11b997fa70 --class cn.itcast.checkpoint.CheckpointDemo01 /root/ckp.jar</li>
<li>停止yarn session<br>
yarn application -kill application_1607782486484_0014</li>
</ol>
<h2 id="5-关于并行度">5. 关于并行度</h2>
<p>一个Flink程序由多个Operator组成(source、transformation和 sink)。<br>
一个Operator由多个并行的Task(线程)来执行，一个Operator的并行Task(线程)数目就被称为该Operator(任务)的并行度(Parallel)<br>
并行度可以有如下几种指定方式：</p>
<ol>
<li>Operator Level（算子级别）(可以使用)<br>
一个算子、数据源和sink的并行度可以通过调用 setParallelism()方法来指定<br>
<img src="https://img-blog.csdnimg.cn/20210519145653361.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li>Execution Environment Level（Env级别）(可以使用)<br>
执行环境(任务)的默认并行度可以通过调用setParallelism()方法指定。为了以并行度3来执行所有的算子、数据源和data sink， 可以通过如下的方式设置执行环境的并行度：<br>
执行环境的并行度可以通过显式设置算子的并行度而被重写<br>
<img src="https://img-blog.csdnimg.cn/20210519145823848.png" alt="在这里插入图片描述"></li>
<li>Client Level（客户端级别,推荐使用）（可以使用）<br>
并行度可以在客户端将job提交到Flink时设定<br>
对于CLI客户端，可以通过-p参数指定并行度：<br>
./bin/flink run -p 10 WordCount-java.jar</li>
<li>System Level（系统默认级别,尽量不使用）<br>
在系统级可以通过设置flink-conf.yaml文件中的parallelism.default属性来指定所有执行环境的默认并行度<br>
<img src="https://img-blog.csdnimg.cn/20210519150030632.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210519150138860.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ol>
<ul>
<li>Example1：
<ul>
<li>在fink-conf.yaml中 taskmanager.numberOfTaskSlots 默认值为1，即每个Task Manager上只有一个Slot ，此处是3</li>
<li>Example1中，WordCount程序设置了并行度为1，意味着程序 Source、Reduce、Sink在一个Slot中，占用一个Slot</li>
</ul>
</li>
<li>Example2：
<ul>
<li>通过设置并行度为2后，将占用2个Slot</li>
</ul>
</li>
<li>Example3：
<ul>
<li>通过设置并行度为9，将占用9个Slot</li>
</ul>
</li>
<li>Example4：
<ul>
<li>通过设置并行度为9，并且设置sink的并行度为1，则Source、Reduce将占用9个Slot，但是Sink只占用1个Slot</li>
</ul>
</li>
</ul>
<p><strong><font color=red>注：</font></strong></p>
<ul>
<li>并行度的优先级：算子级别 &gt; env级别 &gt; Client级别 &gt; 系统默认级别  (越靠前具体的代码并行度的优先级越高)</li>
<li>如果source不可以被并行执行，即使指定了并行度为多个，也不会生效</li>
<li>在实际生产中，我们推荐在算子级别显示指定各自的并行度，方便进行显示和精确的资源控制</li>
<li>slot是静态的概念，是指taskmanager具有的并发执行能力; parallelism是动态的概念，是指程序运行时实际使用的并发能力</li>
</ul>
<h1 id="十、Table-API-与SQL">十、Table API 与SQL</h1>
<p>Table API 是流处理和批处理通用的关系型 API，Table API 可以基于流输入或者批输入来运行而不需要进行任何修改。Table API 是 SQL 语言的超集并专门为 Apache Flink 设计的，Table API 是 Scala 和 Java 语言集成式的 API。与常规 SQL 语言中将查询指定为字符串不同，Table API 查询是以 Java 或 Scala 中的语言嵌入样式来定义的， 具有 IDE 支持如:自动完成和语法检测。</p>
<h2 id="1-Table-API-SQL的特点">1. Table API &amp; SQL的特点</h2>
<p>Flink之所以选择将 Table API &amp; SQL 作为未来的核心 API，是因为其具有一些非常重要的特点：<br>
<img src="https://img-blog.csdnimg.cn/20210519152722120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<ol>
<li>声明式:属于设定式语言，用户只要表达清楚需求即可，不需要了解底层执行</li>
<li>高性能:可优化，内置多种查询优化器，这些查询优化器可为 SQL 翻译出最优执行计划</li>
<li>简单易学:易于理解，不同行业和领域的人都懂，学习成本较低</li>
<li>标准稳定:语义遵循SQL标准，非常稳定，在数据库 30 多年的历史中，SQL 本身变化较少</li>
<li>流批统一:可以做到API层面上流与批的统一，相同的SQL逻辑，既可流模式运行，也可批模式运行，Flink底层Runtime本身就是一个流与批统一的引擎</li>
</ol>
<h2 id="2-Table-API-SQL发展历程">2. Table API &amp; SQL发展历程</h2>
<h3 id="2-1-架构升级">2.1 架构升级</h3>
<p>自 2015 年开始，阿里巴巴开始调研开源流计算引擎，最终决定基于 Flink 打造新一代计算引擎，针对 Flink 存在的不足进行优化和改进，并且在 2019 年初将最终代码开源，也就是Blink。Blink 在原来的 Flink 基础上最显著的一个贡献就是 Flink SQL 的实现。随着版本的不断更新，API 也出现了很多不兼容的地方。<br>
在 Flink 1.9 中，Table 模块迎来了核心架构的升级，引入了阿里巴巴Blink团队贡献的诸多功能<br>
<img src="https://img-blog.csdnimg.cn/20210519153027176.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
在Flink 1.9 之前，Flink API 层 一直分为DataStream API 和 DataSet API，Table API &amp; SQL 位于 DataStream API 和 DataSet API 之上。可以看处流处理和批处理有各自独立的api (流处理DataStream，批处理DataSet)。而且有不同的执行计划解析过程，codegen过程也完全不一样，完全没有流批一体的概念，面向用户不太友好。<br>
在Flink1.9之后新的架构中，有两个查询处理器：Flink Query Processor，也称作Old Planner和Blink Query Processor，也称作Blink Planner。为了兼容老版本Table及SQL模块，插件化实现了Planner，Flink原有的Flink Planner不变，后期版本会被移除。新增加了Blink Planner，新的代码及特性会在Blink planner模块上实现。批或者流都是通过解析为Stream Transformation来实现的，不像Flink Planner，批是基于Dataset，流是基于DataStream。</p>
<h3 id="2-2-查询处理器的选择">2.2 查询处理器的选择</h3>
<p>查询处理器是 Planner 的具体实现，通过parser、optimizer、codegen(代码生成技术)等流程将 Table API &amp; SQL作业转换成 Flink Runtime 可识别的 Transformation DAG，最终由 Flink Runtime 进行作业的调度和执行。<br>
Flink Query Processor查询处理器针对流计算和批处理作业有不同的分支处理，流计算作业底层的 API 是 DataStream API， 批处理作业底层的 API 是 DataSet API<br>
Blink Query Processor查询处理器则实现流批作业接口的统一，底层的 API 都是Transformation，这就意味着我们和Dataset完全没有关系了<br>
Flink1.11之后Blink Query Processor查询处理器已经是默认的了</p>
<ul>
<li>了解-Blink planner和Flink Planner具体区别如下：
<ul>
<li>Blink将批处理作业视为流式处理的特殊情况。因此，表和数据集之间的转换也不受支持，批处理作业不会转换为数据集程序，而是转换为数据流程序，与流作业相同</li>
<li>Blink planner不支持Batch TableSource</li>
<li>old planner和Blink planner的FilterableSource实现不兼容</li>
<li>基于字符串的键值配置选项仅用于Blink planner</li>
<li>PlannerConfig在两个planners中的实现（CalciteConfig）是不同的</li>
<li>Blink planner将在TableEnvironment和StreamTableEnvironment上将多个汇优化为一个DAG。旧的计划者总是将每个水槽优化为一个新的DAG，其中所有DAG彼此独立</li>
<li>旧的计划期现在不支持目录统计，而Blink计划器支持</li>
</ul>
</li>
</ul>
<h3 id="2-3-注意">2.3 注意</h3>
<ul>
<li><strong>API 稳定性</strong><br>
<img src="https://img-blog.csdnimg.cn/20210519154342961.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li><strong>性能对比</strong>
<ul>
<li>目前FlinkSQL性能不如SparkSQL，未来FlinkSQL可能会越来越好（下图是Hive、Spark、Flink的SQL执行速度对比）<br>
<img src="https://img-blog.csdnimg.cn/20210519154428147.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
</li>
</ul>
<h2 id="3-需要引入的pom依赖">3. 需要引入的pom依赖</h2>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-scala-bridge_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="4-简单了解TableAPI">4. 简单了解TableAPI</h2>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment env.setParallelism(<span class="number">1</span>)</span><br><span class="line">	<span class="keyword">val</span> inputStream = env.readTextFile(<span class="string">&quot;..\\sensor.txt&quot;</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> dataStream = inputStream</span><br><span class="line">		.map( data =&gt; &#123;</span><br><span class="line">			<span class="keyword">val</span> dataArray = data.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">			<span class="type">SensorReading</span>(dataArray(<span class="number">0</span>).trim, dataArray(<span class="number">1</span>).trim.toLong, dataArray(<span class="number">2</span>).trim.toDouble)</span><br><span class="line">	&#125;)</span><br><span class="line">	<span class="comment">// 基于env 创建 tableEnv</span></span><br><span class="line">	<span class="keyword">val</span> settings: <span class="type">EnvironmentSettings</span> = <span class="type">EnvironmentSettings</span>.newInstance().useOldPlanner().inStreamingMode().build()</span><br><span class="line">	<span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 从一条流创建一张表</span></span><br><span class="line">	<span class="keyword">val</span> dataTable: <span class="type">Table</span> = tableEnv.fromDataStream(dataStream)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 从表里选取特定的数据</span></span><br><span class="line">	<span class="keyword">val</span> selectedTable: <span class="type">Table</span> = dataTable.select(&#x27;id, &#x27;temperature).filter(<span class="string">&quot;id = &#x27;sensor_1&#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">val</span> selectedStream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = selectedTable.toAppendStream[(<span class="type">String</span>, <span class="type">Double</span>)] </span><br><span class="line">	</span><br><span class="line">	selectedStream.print()</span><br><span class="line"></span><br><span class="line">	env.execute(<span class="string">&quot;table test&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-1-动态表">4.1 动态表</h3>
<p>如果流中的数据类型是 case class 可以直接根据 case class 的结构生成 table</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.fromDataStream(dataStream)</span><br></pre></td></tr></table></figure>
<p>或者根据字段顺序单独命名</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.fromDataStream(dataStream,’id,’timestamp........)</span><br></pre></td></tr></table></figure>
<p>最后的动态表可以转换为流进行输出</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table.toAppendStream[(<span class="type">String</span>,<span class="type">String</span>)]</span><br></pre></td></tr></table></figure>
<h3 id="4-2-字段">4.2 字段</h3>
<p>用一个单引放到字段前面来标识字段名, 如 ‘name , ‘id ,’amount 等。</p>
<h2 id="5-TableAPI-的窗口聚合操作">5. TableAPI 的窗口聚合操作</h2>
<h3 id="5-1-案例演示">5.1 案例演示</h3>
<ul>
<li>统计每10 秒中每个传感器温度值的个数</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 统计每10 秒中每个传感器温度值的个数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	<span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">	env.setParallelism(<span class="number">1</span>)</span><br><span class="line">	env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> inputStream = env.readTextFile(<span class="string">&quot;..\\sensor.txt&quot;</span>)</span><br><span class="line">	<span class="keyword">val</span> dataStream = inputStream</span><br><span class="line">		.map( data =&gt; &#123;</span><br><span class="line">			<span class="keyword">val</span> dataArray = data.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">			<span class="type">SensorReading</span>(dataArray(<span class="number">0</span>).trim, dataArray(<span class="number">1</span>).trim.toLong, dataArray(<span class="number">2</span>).trim.toDouble)</span><br><span class="line">		&#125;)</span><br><span class="line">		.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">SensorReading</span>](<span class="type">Time</span>.seconds(<span class="number">1</span>)) &#123;</span><br><span class="line">			<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">SensorReading</span>): <span class="type">Long</span> = element.timestamp * <span class="number">1000</span>L</span><br><span class="line">		&#125;)</span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 基于env 创建 tableEnv</span></span><br><span class="line">	<span class="keyword">val</span> settings: <span class="type">EnvironmentSettings</span> = <span class="type">EnvironmentSettings</span>.newInstance().useOldPlanner().inStreamingMode().build()</span><br><span class="line">	<span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 从一条流创建一张表，按照字段去定义，并指定事件时间的时间字段</span></span><br><span class="line">	<span class="keyword">val</span> dataTable: <span class="type">Table</span> = tableEnv.fromDataStream(dataStream, &#x27;id, &#x27;temperature, &#x27;ts.rowtime)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 按照时间开窗聚合统计</span></span><br><span class="line">	<span class="keyword">val</span> resultTable: <span class="type">Table</span> = dataTable</span><br><span class="line">		.window( <span class="type">Tumble</span> over <span class="number">10.</span>seconds on &#x27;ts as &#x27;tw )</span><br><span class="line">		.groupBy(&#x27;id, &#x27;tw)</span><br><span class="line">		.select(&#x27;id, &#x27;id.count)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">val</span> selectedStream: <span class="type">DataStream</span>[(<span class="type">Boolean</span>, (<span class="type">String</span>, <span class="type">Long</span>))] = resultTable.toRetractStream[(<span class="type">String</span>, <span class="type">Long</span>)]</span><br><span class="line"></span><br><span class="line">	selectedStream.print()</span><br><span class="line">	env.execute(<span class="string">&quot;table window test&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="5-2-关于-group-by">5.2 关于 group by</h3>
<ol>
<li>如果了使用 groupby， table 转换为流的时候只能用 toRetractDstream</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[(<span class="type">Boolean</span>, (<span class="type">String</span>, <span class="type">Long</span>))] = table</span><br><span class="line">	.toRetractStream[(<span class="type">String</span>,<span class="type">Long</span>)]</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>toRetractDstream 得到的第一个 boolean 型字段标识 true 就是最新的数据(Insert)， false 表示过期老数据(Delete)</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[(<span class="type">Boolean</span>, (<span class="type">String</span>, <span class="type">Long</span>))] = table</span><br><span class="line">	.toRetractStream[(<span class="type">String</span>,<span class="type">Long</span>)]</span><br><span class="line">dataStream.filter(_._1).print()</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>如果使用的 api 包括时间窗口， 那么窗口的字段必须出现在 groupBy 中</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resultTable: <span class="type">Table</span> = dataTable</span><br><span class="line">	.window( <span class="type">Tumble</span> over <span class="number">10.</span>seconds on &#x27;ts as &#x27;tw )</span><br><span class="line">	.groupBy(&#x27;id, &#x27;tw)</span><br><span class="line">	.select(&#x27;id, &#x27;id.count)</span><br></pre></td></tr></table></figure>
<h3 id="5-3-关于时间窗口">5.3 关于时间窗口</h3>
<ol>
<li>用到时间窗口， 必须提前声明时间字段， 如果是 processTime 直接在创建动态表时进行追加就可以</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span>	dataTable: <span class="type">Table</span> = tableEnv.fromDataStream(dataStream, &#x27;id, &#x27;temperature, &#x27;ps.proctime)</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>如果是 EventTime 要在创建动态表时声明</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span>	dataTable: <span class="type">Table</span> = tableEnv.fromDataStream(dataStream, &#x27;id, &#x27;temperature, &#x27;ts.rowtime)</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>滚动窗口可以使用 Tumble over 10000.millis on 来表示</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resultTable: <span class="type">Table</span> = dataTable</span><br><span class="line">	.window( <span class="type">Tumble</span> over <span class="number">10.</span>seconds on &#x27;ts as &#x27;tw )</span><br><span class="line">	.groupBy(&#x27;id, &#x27;tw)</span><br><span class="line">	.select(&#x27;id, &#x27;id.count)</span><br></pre></td></tr></table></figure>
<h2 id="6-SQL-如何编写">6. SQL 如何编写</h2>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 统计每10 秒中每个传感器温度值的个数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment env.setParallelism(<span class="number">1</span>)</span><br><span class="line">	env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">val</span> inputStream = env.readTextFile(<span class="string">&quot;..\\sensor.txt&quot;</span>)</span><br><span class="line">	<span class="keyword">val</span> dataStream = inputStream</span><br><span class="line">		.map( data =&gt; &#123;</span><br><span class="line">			<span class="keyword">val</span> dataArray = data.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">			<span class="type">SensorReading</span>(dataArray(<span class="number">0</span>).trim, dataArray(<span class="number">1</span>).trim.toLong, dataArray(<span class="number">2</span>).trim.toDouble)</span><br><span class="line">		&#125;)</span><br><span class="line">		.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">SensorReading</span>](<span class="type">Time</span>.seconds(<span class="number">1</span>)) &#123;</span><br><span class="line">			<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">SensorReading</span>): <span class="type">Long</span> = element.timestamp * <span class="number">1000</span>L</span><br><span class="line">		&#125;)</span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 基于env 创建 tableEnv</span></span><br><span class="line">	<span class="keyword">val</span> settings: <span class="type">EnvironmentSettings</span> = <span class="type">EnvironmentSettings</span>.newInstance().useOldPlanner().inStreamingMode().build()</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 从一条流创建一张表，按照字段去定义，并指定事件时间的时间字段</span></span><br><span class="line">	<span class="keyword">val</span>	dataTable:	<span class="type">Table</span>	=	tableEnv.fromDataStream(dataStream,	&#x27;id, &#x27;temperature, &#x27;ts.rowtime)</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 直接写sql 完成开窗统计</span></span><br><span class="line">	<span class="keyword">val</span> resultSqlTable: <span class="type">Table</span> = tableEnv.sqlQuery(<span class="string">&quot;select id, count(id) from &quot;</span> + dataTable + <span class="string">&quot; group by id, tumble(ts, interval &#x27;15&#x27; second)&quot;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">val</span> selectedStream: <span class="type">DataStream</span>[(<span class="type">Boolean</span>, (<span class="type">String</span>, <span class="type">Long</span>))] = resultSqlTable.toRetractStream[(<span class="type">String</span>, <span class="type">Long</span>)]</span><br><span class="line"></span><br><span class="line">	selectedStream.print()</span><br><span class="line">	env.execute(<span class="string">&quot;table window test&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="7-相关概念">7. 相关概念</h2>
<h3 id="7-1-Dynamic-Tables-Continuous-Queries">7.1 Dynamic Tables &amp; Continuous Queries</h3>
<p>在Flink中，它把针对无界流的表称之为Dynamic Table（动态表），是Flink Table API和SQL的核心概念。顾名思义，它表示了Table是不断变化的。<br>
我们可以这样来理解，当我们用Flink的API，建立一个表，其实把它理解为建立一个逻辑结构，这个逻辑结构需要映射到数据上去。Flink source源源不断的流入数据，就好比每次都往表上新增一条数据。表中有了数据，我们就可以使用SQL去查询了。要注意一下，流处理中的数据是只有新增的，所以看起来数据会源源不断地添加到表中。<br>
动态表也是一种表，既然是表，就应该能够被查询。我们来回想一下原先我们查询表的场景：</p>
<ol>
<li>将SQL语句放入到mysql的终端执行</li>
<li>查看结果</li>
<li>再编写一条SQL语句</li>
<li>再放入到终端执行</li>
<li>再查看结果<br>
…如此反复</li>
</ol>
<p>而针对动态表，Flink的source端肯定是源源不断地会有数据流入，然后我们基于这个数据流建立了一张表，再编写SQL语句查询数据，进行处理。这个SQL语句一定是不断地执行的。而不是只执行一次。注意：针对流处理的SQL绝对不会像批式处理一样，执行一次拿到结果就完了。而是会不停地执行，不断地查询获取结果处理。所以，官方给这种查询方式取了一个名字，叫Continuous Query，中文翻译过来叫连续查询。而且每一次查询出来的数据也是不断变化的。<br>
<img src="https://img-blog.csdnimg.cn/20210519162930496.png#pic_center" alt="在这里插入图片描述"><br>
这是一个非常简单的示意图。该示意图描述了：我们通过建立动态表和连续查询来实现在无界流中的SQL操作。大家也可以看到，在Continuous上面有一个State，表示查询出来的结果会存储在State中，再下来Flink最终还是使用流来进行处理。<br>
所以，我们可以理解为Flink的Table API和SQL，是一个逻辑模型，通过该逻辑模型可以让我们的数据处理变得更加简单。<br>
<img src="https://img-blog.csdnimg.cn/20210519163021191.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210519163033602.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h3 id="7-2-Table-to-Stream-Conversion">7.2 Table to Stream Conversion</h3>
<h4 id="7-2-1-表中的Update和Delete">7.2.1 表中的Update和Delete</h4>
<p>我们前面提到的表示不断地Append，表的数据是一直累加的，因为表示对接Source的，Source是不会有update的。但如果我们编写了一个SQL。这个SQL看起来是这样的：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">user</span>, <span class="built_in">sum</span>(money) <span class="keyword">FROM</span> <span class="keyword">order</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">user</span>;</span><br></pre></td></tr></table></figure>
<p>当执行一条SQL语句之后，这条语句的结果还是一个表，因为在Flink中执行的SQL是Continuous Query，这个表的数据是不断变化的。新创建的表存在Update的情况。仔细看下下面的示例，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">第一条数据，张三,2000，执行这条SQL语句的结果是，张三,2000</span><br><span class="line">第二条数据，李四,1500，继续执行这条SQL语句，结果是，张三,2000 | 李四,1500</span><br><span class="line">第三条数据，张三,300，继续执行这条SQL语句，结果是，张三,2300 | 李四,1500</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>大家发现了吗，现在数据结果是有Update的。张三一开始是2000，但后面变成了2300。<br>
那还有删除的情况吗？有的。看一下下面这条SQL语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> t1.`<span class="keyword">user</span>`, <span class="built_in">SUM</span>(t1.`money`) <span class="keyword">FROM</span> t_order t1</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line"><span class="keyword">NOT</span> <span class="keyword">EXISTS</span> (<span class="keyword">SELECT</span> T2.`<span class="keyword">user</span>`<span class="keyword">AS</span> TOTAL_MONEY <span class="keyword">FROM</span> t_order t2 <span class="keyword">WHERE</span> T2.`<span class="keyword">user</span>` <span class="operator">=</span> T1.`<span class="keyword">user</span>`</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> t2.`<span class="keyword">user</span>` <span class="keyword">HAVING</span> <span class="built_in">SUM</span>(T2.`money`) <span class="operator">&gt;</span> <span class="number">3000</span>)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> t1.`<span class="keyword">user</span>`<span class="keyword">GROUP</span> <span class="keyword">BY</span> t1.`<span class="keyword">user</span>`</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">第一条数据，张三,2000，执行这条SQL语句的结果是，张三,2000</span><br><span class="line">第二条数据，李四,1500，继续执行这条SQL语句，结果是，张三,2000 | 李四,1500</span><br><span class="line">第三条数据，张三,300，继续执行这条SQL语句，结果是，张三,2300 | 李四,1500</span><br><span class="line">第四条数据，张三,800，继续执行这条SQL语句，结果是，李四,1500</span><br></pre></td></tr></table></figure>
<p>因为张三的消费的金额已经超过了3000，所以SQL执行完后，张三是被处理掉了。从数据的角度来看，它不就是被删除了吗？</p>
<p>通过上面的两个示例，给大家演示了，在Flink SQL中，对接Source的表都是Append-only的，不断地增加。执行一些SQL生成的表，这个表可能是要UPDATE的、也可能是要INSERT的。</p>
<h4 id="7-2-2-对表的编码操作">7.2.2 对表的编码操作</h4>
<p>我们前面说到过，表是一种逻辑结构。而Flink中的核心还是Stream。所以，Table最终还是会以Stream方式来继续处理。如果是以Stream方式处理，最终Stream中的数据有可能会写入到其他的外部系统中，例如：将Stream中的数据写入到MySQL中。<br>
我们前面也看到了，表是有可能会UPDATE和DELETE的。那么如果是输出到MySQL中，就要执行UPDATE和DELETE语句了。而DataStream我们在学习Flink的时候就学习过了，DataStream是不能更新、删除事件的。<br>
如果对表的操作是INSERT，这很好办，直接转换输出就好，因为DataStream数据也是不断递增的。但如果一个TABLE中的数据被UPDATE了、或者被DELETE了，如果用流来表达呢？因为流不可变的特征，我们肯定要对这种能够进行UPDATE/DELETE的TABLE做特殊操作。<br>
我们可以针对每一种操作，INSERT/UPDATE/DELETE都用一个或多个经过编码的事件来表示。<br>
例如：针对UPDATE，我们用两个操作来表达，[DELETE] 数据+  [INSERT]数据。也就是先把之前的数据删除，然后再插入一条新的数据。针对DELETE，我们也可以对流中的数据进行编码，[DELETE]数据。</p>
<p>总体来说，我们通过对流数据进行编码，也可以告诉DataStream的下游，[DELETE]表示发出MySQL的DELETE操作，将数据删除。用 [INSERT]表示插入新的数据。</p>
<h4 id="7-2-3-将表转换为三种不同编码方式的流">7.2.3 将表转换为三种不同编码方式的流</h4>
<p>Flink中的Table API或者SQL支持三种不同的编码方式。分别是：</p>
<ul>
<li>
<p>Append-only流</p>
<ul>
<li>跟INSERT操作对应。这种编码类型的流针对的是只会不断新增的Dynamic Table。这种方式好处理，不需要进行特殊处理，源源不断地往流中发送事件即可</li>
</ul>
</li>
<li>
<p>Retract流</p>
<ul>
<li>这种流就和Append-only不太一样。上面的只能处理INSERT，如果表会发生DELETE或者UPDATE，Append-only编码方式的流就不合适了。Retract流有几种类型的事件类型：
<ul>
<li>ADD MESSAGE：这种消息对应的就是INSERT操作</li>
<li>RETRACT MESSAGE：直译过来叫取消消息。这种消息对应的就是DELETE操作</li>
</ul>
</li>
<li>我们可以看到通过ADD MESSAGE和RETRACT MESSAGE可以很好的向外部系统表达删除和插入操作。那如何进行UPDATE呢？好办！RETRACT MESSAGE + ADD MESSAGE即可。先把之前的数据进行删除，然后插入一条新的。完美~<br>
<img src="https://img-blog.csdnimg.cn/20210519163916523.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
</li>
<li>
<p>Upsert流</p>
<ul>
<li>前面我们看到的RETRACT编码方式的流，实现UPDATE是使用DELETE + INSERT模式的。大家想一下：在MySQL中我们更新数据的时候，肯定不会先DELETE掉一条数据，然后再插入一条数据，肯定是直接发出UPDATE语句执行更新。而Upsert编码方式的流，是能够支持Update的，这种效率更高。它同样有两种类型的消息：
<ul>
<li>UPSERT MESSAGE：这种消息可以表示要对外部系统进行Update或者INSERT操作</li>
<li>DELETE MESSAGE：这种消息表示DELETE操作</li>
</ul>
</li>
<li>Upsert流是要求必须指定Primary Key的，因为Upsert操作是要有Key的。Upsert流针对UPDATE操作用一个UPSERT MESSAGE就可以描述，所以效率会更高<br>
<img src="https://img-blog.csdnimg.cn/20210519164013878.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
</li>
</ul>
<h1 id="十一、Flink-CEP-简介">十一、Flink CEP 简介</h1>
<h2 id="1-什么是复杂事件处理CEP">1.  什么是复杂事件处理CEP</h2>
<p>一个或多个由简单事件构成的事件流通过一定的规则匹配， 然后输出用户想得到的数据， 满足规则的复杂事件。<br>
<strong>特征：</strong></p>
<ul>
<li>目标
<ul>
<li>从有序的简单事件流中发现一些高阶特征</li>
</ul>
</li>
<li>输入
<ul>
<li>一个或多个由简单事件构成的事件流</li>
</ul>
</li>
<li>处理
<ul>
<li>识别简单事件之间的内在联系， 多个符合一定规则的简单事件构成复杂事件</li>
</ul>
</li>
<li>输出
<ul>
<li>满足规则的复杂事件</li>
</ul>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20210519165155380.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
CEP 用于分析低延迟、频繁产生的不同来源的事件流。CEP 可以帮助在复杂的、不相关的事件流中找出有意义的模式和复杂的关系，以接近实时或准实时的获得通知并阻止一些行为。<br>
CEP 支持在流上进行模式匹配， 根据模式的条件不同， 分为连续的条件或不连续的条件；模式的条件允许有时间的限制，当在条件范围内没有达到满足的条件时，  会导致模式匹配超时。<br>
看起来很简单， 但是它有很多不同的功能：</p>
<ul>
<li>输入的流数据， 尽快产生结果</li>
<li>在 2 个 event 流上， 基于时间进行聚合类的计算</li>
<li>提供实时/准实时的警告和通知</li>
<li>在多样的数据源中产生关联并分析模式</li>
<li>高吞吐、低延迟的处理</li>
</ul>
<p>市场上有多种 CEP 的解决方案， 例如 Spark、Samza、Beam 等， 但他们都没有提供专门的 library 支持。但是 Flink 提供了专门的 CEP library。</p>
<h2 id="2-Flink-CEP">2. Flink CEP</h2>
<p>Flink 为 CEP 提供了专门的 Flink CEP library， 它包含如下组件：</p>
<ul>
<li>Event Stream</li>
<li>pattern 定义</li>
<li>pattern 检测</li>
<li>生成 Alert</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20210519165438959.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc1ODg3Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>
首先， 开发人员要在 DataStream 流上定义出模式条件， 之后 Flink CEP 引擎进行模式检测， 必要时生成告警。<br>
为了使用 Flink CEP， 我们需要导入依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-cep_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-1-Event-Streams">2.1 Event Streams</h3>
<p>以登陆事件流为例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">LoginEvent</span>(<span class="params">userId: <span class="type">String</span>, ip: <span class="type">String</span>, eventType: <span class="type">String</span>, eventTi me: <span class="type">String</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">env.setParallelism(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> loginEventStream = env.fromCollection(<span class="type">List</span>(</span><br><span class="line">	<span class="type">LoginEvent</span>(<span class="string">&quot;1&quot;</span>, <span class="string">&quot;192.168.0.1&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="string">&quot;1558430842&quot;</span>),</span><br><span class="line">	<span class="type">LoginEvent</span>(<span class="string">&quot;1&quot;</span>, <span class="string">&quot;192.168.0.2&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="string">&quot;1558430843&quot;</span>),</span><br><span class="line">	<span class="type">LoginEvent</span>(<span class="string">&quot;1&quot;</span>, <span class="string">&quot;192.168.0.3&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="string">&quot;1558430844&quot;</span>),</span><br><span class="line">	<span class="type">LoginEvent</span>(<span class="string">&quot;2&quot;</span>, <span class="string">&quot;192.168.10.10&quot;</span>, <span class="string">&quot;success&quot;</span>, <span class="string">&quot;1558430845&quot;</span>)</span><br><span class="line">)).assignAscendingTimestamps(_.eventTime.toLong)</span><br></pre></td></tr></table></figure>
<h3 id="2-2-Pattern-API">2.2 Pattern API</h3>
<p>每个 Pattern 都应该包含几个步骤，或者叫做 state。从一个 state 到另一个 state， 通常我们需要定义一些条件， 例如下列的代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> loginFailPattern = <span class="type">Pattern</span>.begin[<span class="type">LoginEvent</span>](<span class="string">&quot;begin&quot;</span>)</span><br><span class="line">	.where(_.eventType.equals(<span class="string">&quot;fail&quot;</span>))</span><br><span class="line">	.next(<span class="string">&quot;next&quot;</span>)</span><br><span class="line">	.where(_.eventType.equals(<span class="string">&quot;fail&quot;</span>))</span><br><span class="line">	.within(<span class="type">Time</span>.seconds(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>每个 state 都应该有一个标示： 例如.begin<a href="%22begin%22">LoginEvent</a>中的&quot;begin&quot;<br>
每个 state 都需要有一个唯一的名字， 而且需要一个 filter 来过滤条件， 这个过滤条件定义事件需要符合的条件， 例如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.where(_.eventType.equals(<span class="string">&quot;fail&quot;</span>))</span><br></pre></td></tr></table></figure>
<p>我们也可以通过 subtype 来限制 event 的子类型：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start.subtype(<span class="type">SubEvent</span>.<span class="keyword">class</span>).where(...);</span><br></pre></td></tr></table></figure>
<p>事实上，你可以多次调用 subtype 和 where 方法；而且如果 where 条件是不相关 的，你可以通过 or 来指定一个单独的 filter 函数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pattern.where(...).or(...);</span><br></pre></td></tr></table></figure>
<p>之后，我们可以在此条件基础上，通过 next 或者 followedBy 方法切换到下一个 state，next 的意思是说上一步符合条件的元素之后紧挨着的元素；而 followedBy 并 不要求一定是挨着的元素。这两者分别称为严格近邻和非严格近邻。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> strictNext = start.next(<span class="string">&quot;middle&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> nonStrictNext = start.followedBy(<span class="string">&quot;middle&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>最后，我们可以将所有的 Pattern 的条件限定在一定的时间范围内：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">next.within(<span class="type">Time</span>.seconds(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>这个时间可以是 Processing Time，也可以是 Event Time。</p>
<h3 id="2-3-Pattern-检测">2.3 Pattern 检测</h3>
<p>通过一个 input DataStream 以及刚刚我们定义的 Pattern， 我们可以创建一个PatternStream：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> input = ...</span><br><span class="line"><span class="keyword">val</span> pattern = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> patternStream = <span class="type">CEP</span>.pattern(input, pattern)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> patternStream = <span class="type">CEP</span>.pattern(loginEventStream.keyBy(_.userId), loginFail <span class="type">Pattern</span>)</span><br></pre></td></tr></table></figure>
<p>一旦获得 PatternStream，我们就可以通过 select 或 flatSelect，从一个 Map 序列 找到我们需要的警告信息。</p>
<h3 id="2-4-select">2.4 select</h3>
<p>select 方法需要实现一个 PatternSelectFunction， 通过 select 方法来输出需要的警告。它接受一个 Map 对，包含 string/event，其中 key 为 state 的名字， event 则为真实的 Event。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> loginFailDataStream = patternStream</span><br><span class="line">	.select((pattern: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">LoginEvent</span>]]) =&gt; &#123;</span><br><span class="line">		<span class="keyword">val</span> first = pattern.getOrElse(<span class="string">&quot;begin&quot;</span>, <span class="literal">null</span>).iterator.next()</span><br><span class="line">		<span class="keyword">val</span> second = pattern.getOrElse(<span class="string">&quot;next&quot;</span>, <span class="literal">null</span>).iterator.next()</span><br><span class="line">		<span class="type">Warning</span>(first.userId, first.eventTime, second.eventTime, <span class="string">&quot;warning&quot;</span>)</span><br><span class="line">	&#125;)</span><br></pre></td></tr></table></figure>
<p>其返回值仅为 1 条记录。</p>
<h3 id="2-5-flatSelect">2.5 flatSelect</h3>
<p>通过实现 PatternFlatSelectFunction，实现与 select 相似的功能。唯一的区别就 是 flatSelect 方法可以返回多条记录，它通过一个 Collector[OUT]类型的参数来将要 输出的数据传递到下游。</p>
<h3 id="2-6-超时事件的处理">2.6 超时事件的处理</h3>
<p>通过 within 方法，我们的 parttern 规则将匹配的事件限定在一定的窗口范围内。 当有超过窗口时间之后到达的 event，我们可以通过在 select 或 flatSelect 中，实现 PatternTimeoutFunction 和 PatternFlatTimeoutFunction 来处理这种情况。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> patternStream: <span class="type">PatternStream</span>[<span class="type">Event</span>] = <span class="type">CEP</span>.pattern(input, pattern)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> outputTag = <span class="type">OutputTag</span>[<span class="type">String</span>](<span class="string">&quot;side-output&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">SingleOutputStreamOperator</span>[<span class="type">ComplexEvent</span>] = patternStream.select(outputTag)&#123;</span><br><span class="line">	(pattern: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Event</span>]], timestamp: <span class="type">Long</span>) =&gt; <span class="type">TimeoutEvent</span>()</span><br><span class="line">&#125; &#123;</span><br><span class="line">	pattern: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Event</span>]] =&gt; <span class="type">ComplexEvent</span>()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> timeoutResult: <span class="type">DataStream</span>&lt;<span class="type">TimeoutEvent</span>&gt; = result.getSideOutput(outputTa g)</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://program-park.github.io/">一位木带感情的码农</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://program-park.github.io/2021/05/19/flink_1/">https://program-park.github.io/2021/05/19/flink_1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">此文章版权归 <a href=https://program-park.github.io/>程序园</a> 所有，如有转载，请注明来自原作者。</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Flink/">Flink</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="post_share"><div class="social-share" data-image="/img/flink/1.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/05/21/flink_2/"><img class="prev-cover" src="/img/flink/1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Flink 接收 kafka source 数据</div></div></a></div><div class="next-post pull-right"><a href="/2021/04/10/scala_1/"><img class="next-cover" src="/img/scala/1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">大数据开发必备技能，Scala 零基础笔记</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/05/21/flink_2/" title="Flink 接收 kafka source 数据"><img class="cover" src="/img/flink/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-21</div><div class="title">Flink 接收 kafka source 数据</div></div></a></div><div><a href="/2021/06/09/flink_11/" title="Flink 1.12.4 RocksDBStateBackend 优化"><img class="cover" src="/img/flink/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-06-09</div><div class="title">Flink 1.12.4 RocksDBStateBackend 优化</div></div></a></div><div><a href="/2021/06/09/flink_12/" title="Flink1.12.4 配置文件详解"><img class="cover" src="/img/flink/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-06-09</div><div class="title">Flink1.12.4 配置文件详解</div></div></a></div><div><a href="/2021/06/06/flink_10/" title="Flink 1.12.4 TaskManager 的内存调优（standalone 模式）"><img class="cover" src="/img/flink/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-06-06</div><div class="title">Flink 1.12.4 TaskManager 的内存调优（standalone 模式）</div></div></a></div><div><a href="/2021/05/21/flink_3/" title="Flink 发送数据到 Kafka sink"><img class="cover" src="/img/flink/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-21</div><div class="title">Flink 发送数据到 Kafka sink</div></div></a></div><div><a href="/2021/06/01/flink_5/" title="Flink 提交任务的两种方式（非 flink on yarn）"><img class="cover" src="/img/flink/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-06-01</div><div class="title">Flink 提交任务的两种方式（非 flink on yarn）</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatat_img.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">一位木带感情的码农</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">38</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/program-park"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/program-park" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:lkm869666@126.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/weixin_44758876" target="_blank" title="CSDN"><i class="fa-solid fa-c"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这里是程序园，如有任何意见和疑问，请反馈到我的邮箱。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81Flink%E7%AE%80%E4%BB%8B"><span class="toc-text">一、Flink简介</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Flink%E7%BB%84%E4%BB%B6%E6%A0%88"><span class="toc-text">1. Flink组件栈</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Flink%E5%9F%BA%E7%9F%B3"><span class="toc-text">2. Flink基石</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Fink%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">3. Fink的应用场景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Event-driven-Applications%E3%80%90%E4%BA%8B%E4%BB%B6%E9%A9%B1%E5%8A%A8%E3%80%91"><span class="toc-text">3.1 Event-driven Applications【事件驱动】</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Data-Analytics-Applications%E3%80%90%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%91"><span class="toc-text">3.2 Data Analytics Applications【数据分析】</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Data-Pipeline-Applications%E3%80%90%E6%95%B0%E6%8D%AE%E7%AE%A1%E9%81%93%E3%80%91"><span class="toc-text">3.3 Data Pipeline Applications【数据管道】</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Flink%E7%9A%84%E4%BC%98%E7%82%B9"><span class="toc-text">4. Flink的优点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%B5%81%E5%A4%84%E7%90%86-%E6%89%B9%E5%A4%84%E7%90%86"><span class="toc-text">5. 流处理&amp;批处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E6%B5%81%E6%89%B9%E7%BB%9F%E4%B8%80"><span class="toc-text">6. 流批统一</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Flink%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2"><span class="toc-text">二、Flink安装部署</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Local%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F"><span class="toc-text">1. Local本地模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E5%8E%9F%E7%90%86"><span class="toc-text">1.1 原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E6%93%8D%E4%BD%9C"><span class="toc-text">1.2 操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E6%B5%8B%E8%AF%95"><span class="toc-text">1.3 测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Standalone%E7%8B%AC%E7%AB%8B%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F"><span class="toc-text">2. Standalone独立集群模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%8E%9F%E7%90%86"><span class="toc-text">2.1 原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%93%8D%E4%BD%9C"><span class="toc-text">2.2 操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%B5%8B%E8%AF%95"><span class="toc-text">2.3 测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Standalone-HA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F"><span class="toc-text">3. Standalone-HA高可用集群模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E5%8E%9F%E7%90%86"><span class="toc-text">3.1 原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%93%8D%E4%BD%9C"><span class="toc-text">3.2 操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E6%B5%8B%E8%AF%95"><span class="toc-text">3.3 测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Flink-On-Yarn%E6%A8%A1%E5%BC%8F"><span class="toc-text">4. Flink On Yarn模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%8E%9F%E7%90%86"><span class="toc-text">4.1 原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1-Flink%E5%A6%82%E4%BD%95%E5%92%8CYarn%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92"><span class="toc-text">4.1.1 Flink如何和Yarn进行交互</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2-%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F"><span class="toc-text">4.1.2 两种方式</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-1-2-1-Session%E6%A8%A1%E5%BC%8F"><span class="toc-text">4.1.2.1 Session模式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-1-2-2-Per-Job%E6%A8%A1%E5%BC%8F"><span class="toc-text">4.1.2.2 Per-Job模式</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%93%8D%E4%BD%9C"><span class="toc-text">4.2 操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%B5%8B%E8%AF%95"><span class="toc-text">4.3 测试</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1-Session%E6%A8%A1%E5%BC%8F"><span class="toc-text">4.3.1 Session模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-2-Per-Job%E5%88%86%E7%A6%BB%E6%A8%A1%E5%BC%8F"><span class="toc-text">4.3.2 Per-Job分离模式</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81Flink%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B"><span class="toc-text">三、Flink入门案例</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%89%8D%E7%BD%AE%E8%AF%B4%E6%98%8E"><span class="toc-text">1. 前置说明</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-API"><span class="toc-text">1.1 API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-text">1.2 编程模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-text">2. 准备工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-pom%E6%96%87%E4%BB%B6"><span class="toc-text">2.1 pom文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-log4j-properties"><span class="toc-text">2.2 log4j.properties</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Flink%E5%AE%9E%E7%8E%B0WordCount"><span class="toc-text">3. Flink实现WordCount</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E7%BC%96%E7%A0%81%E6%AD%A5%E9%AA%A4"><span class="toc-text">3.1 编码步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">3.2 代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-%E6%89%B9%E5%A4%84%E7%90%86wordcount"><span class="toc-text">3.2.1 批处理wordcount</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-%E6%B5%81%E5%A4%84%E7%90%86wordcount"><span class="toc-text">3.2.2 流处理wordcount</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81Flink%E5%8E%9F%E7%90%86"><span class="toc-text">四、Flink原理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E8%A7%92%E8%89%B2%E5%88%86%E5%B7%A5"><span class="toc-text">1. 角色分工</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-text">2. 执行流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Standalone%E7%89%88"><span class="toc-text">2.1 Standalone版</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-On-Yarn%E7%89%88"><span class="toc-text">2.2 On Yarn版</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Flink-Streaming-Dataflow"><span class="toc-text">3. Flink Streaming Dataflow</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Dataflow%E3%80%81Operator%E3%80%81Partition%E3%80%81SubTask%E3%80%81Parallelism"><span class="toc-text">3.1 Dataflow、Operator、Partition、SubTask、Parallelism</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Operator%E4%BC%A0%E9%80%92%E6%A8%A1%E5%BC%8F"><span class="toc-text">3.2 Operator传递模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Operator-Chain"><span class="toc-text">3.3 Operator Chain</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-TaskSlot-And-Slot-Sharing"><span class="toc-text">3.4 TaskSlot And Slot Sharing</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Flink%E8%BF%90%E8%A1%8C%E6%97%B6%E7%9A%84%E7%BB%84%E4%BB%B6"><span class="toc-text">4. Flink运行时的组件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Flink%E6%89%A7%E8%A1%8C%E5%9B%BE%EF%BC%88ExecutionGraph%EF%BC%89"><span class="toc-text">5. Flink执行图（ExecutionGraph）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%B5%81%E6%89%B9%E4%B8%80%E4%BD%93API"><span class="toc-text">五、流批一体API</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-DataStream-API"><span class="toc-text">1. DataStream API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Source"><span class="toc-text">2. Source</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E9%A2%84%E5%AE%9A%E4%B9%89Source"><span class="toc-text">2.1 预定义Source</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-1-%E5%9F%BA%E4%BA%8E%E9%9B%86%E5%90%88%E7%9A%84Source"><span class="toc-text">2.1.1 基于集合的Source</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-2-%E5%9F%BA%E4%BA%8E%E6%96%87%E4%BB%B6%E7%9A%84Source"><span class="toc-text">2.1.2 基于文件的Source</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-3-%E5%9F%BA%E4%BA%8ESocket%E7%9A%84Source"><span class="toc-text">2.1.3 基于Socket的Source</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-4-Kafka-Source"><span class="toc-text">2.1.4 Kafka Source</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E8%87%AA%E5%AE%9A%E4%B9%89Source"><span class="toc-text">2.2 自定义Source</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Transformation"><span class="toc-text">3. Transformation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-map"><span class="toc-text">3.1 map</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-flatMap"><span class="toc-text">3.2 flatMap</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-filter"><span class="toc-text">3.3 filter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-keyBy"><span class="toc-text">3.4 keyBy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E6%BB%9A%E5%8A%A8%E8%81%9A%E5%90%88%E7%AE%97%E5%AD%90%EF%BC%88Rolling-Aggregation%EF%BC%89"><span class="toc-text">3.5 滚动聚合算子（Rolling Aggregation）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-Reduce"><span class="toc-text">3.6 Reduce</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-7-%E5%90%88%E5%B9%B6-%E6%8B%86%E5%88%86"><span class="toc-text">3.7 合并&amp;拆分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-7-1-Split-%E5%92%8C-Select"><span class="toc-text">3.7.1 Split 和 Select</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-7-2-Connect-%E5%92%8C-CoMap"><span class="toc-text">3.7.2 Connect 和 CoMap</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-7-3-Union"><span class="toc-text">3.7.3 Union</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-8-%E5%88%86%E5%8C%BA"><span class="toc-text">3.8 分区</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-8-1-rebalance%E9%87%8D%E5%B9%B3%E8%A1%A1%E5%88%86%E5%8C%BA"><span class="toc-text">3.8.1 rebalance重平衡分区</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-8-2-%E5%85%B6%E4%BB%96%E5%88%86%E5%8C%BA"><span class="toc-text">3.8.2 其他分区</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-9-%E6%94%AF%E6%8C%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-text">3.9 支持的数据类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-9-1-%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-text">3.9.1 基础数据类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-9-2-Java-%E5%92%8CScala-%E5%85%83%E7%BB%84%EF%BC%88Tuples%EF%BC%89"><span class="toc-text">3.9.2 Java 和Scala 元组（Tuples）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-9-3-Scala-%E6%A0%B7%E4%BE%8B%E7%B1%BB%EF%BC%88case-classes%EF%BC%89"><span class="toc-text">3.9.3 Scala 样例类（case classes）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-9-4-Java-%E7%AE%80%E5%8D%95%E5%AF%B9%E8%B1%A1%EF%BC%88POJOs%EF%BC%89"><span class="toc-text">3.9.4 Java 简单对象（POJOs）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-9-5-%E5%85%B6%E5%AE%83%EF%BC%88Arrays-Lists-Maps-Enums-%E7%AD%89%E7%AD%89%EF%BC%89"><span class="toc-text">3.9.5 其它（Arrays, Lists, Maps, Enums, 等等）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Sink"><span class="toc-text">4. Sink</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Kafka"><span class="toc-text">4.1 Kafka</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Redis"><span class="toc-text">4.2 Redis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Elasticsearch"><span class="toc-text">4.3 Elasticsearch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-JDBC%E8%87%AA%E5%AE%9A%E4%B9%89sink"><span class="toc-text">4.4 JDBC自定义sink</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81Flink%E4%B8%AD%E7%9A%84Window"><span class="toc-text">六、Flink中的Window</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Window%E6%A6%82%E8%BF%B0"><span class="toc-text">1. Window概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Window%E7%9A%84%E5%88%86%E7%B1%BB"><span class="toc-text">2. Window的分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%8C%89%E7%85%A7time%E5%92%8Ccount%E5%88%86%E7%B1%BB"><span class="toc-text">2.1 按照time和count分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%8C%89%E7%85%A7side%E5%92%8Csize%E5%88%86%E7%B1%BB"><span class="toc-text">2.2 按照side和size分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%80%BB%E7%BB%93"><span class="toc-text">2.3 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Window-API"><span class="toc-text">3. Window API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-TimeWindow"><span class="toc-text">3.1 TimeWindow</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-%E6%BB%9A%E5%8A%A8%E7%AA%97%E5%8F%A3"><span class="toc-text">3.1.1 滚动窗口</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3"><span class="toc-text">3.1.2 滑动窗口</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-CountWindow"><span class="toc-text">3.2 CountWindow</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-%E6%BB%9A%E5%8A%A8%E7%AA%97%E5%8F%A3"><span class="toc-text">3.2.1 滚动窗口</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3"><span class="toc-text">3.2.2 滑动窗口</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-window-function"><span class="toc-text">3.3 window function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%85%B6%E4%BB%96API"><span class="toc-text">3.4 其他API</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E4%B8%8EWartermark"><span class="toc-text">七、时间语义与Wartermark</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Flink%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89"><span class="toc-text">1. Flink中的时间语义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-EventTime%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-text">2. EventTime的重要性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Watermark"><span class="toc-text">3. Watermark</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-text">3.1 基本概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Watermark-%E7%9A%84%E5%BC%95%E5%85%A5"><span class="toc-text">3.2 Watermark 的引入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Assigner-with-periodic-watermarks"><span class="toc-text">Assigner with periodic watermarks</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Assigner-with-punctuated-watermarks"><span class="toc-text">Assigner with punctuated watermarks</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-EventTime-%E5%9C%A8window-%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">4. EventTime 在window 中的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%BB%9A%E5%8A%A8%E7%AA%97%E5%8F%A3%EF%BC%88TumblingEventTimeWindows%EF%BC%89"><span class="toc-text">4.1 滚动窗口（TumblingEventTimeWindows）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%EF%BC%88SlidingEventTimeWindows%EF%BC%89"><span class="toc-text">4.2 滑动窗口（SlidingEventTimeWindows）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E4%BC%9A%E8%AF%9D%E7%AA%97%E5%8F%A3%EF%BC%88EventTimeSessionWindows%EF%BC%89"><span class="toc-text">4.3 会话窗口（EventTimeSessionWindows）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AB%E3%80%81ProcessFunction-API%EF%BC%88%E5%BA%95%E5%B1%82-API%EF%BC%89"><span class="toc-text">八、ProcessFunction API（底层 API）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-KeyedProcessFunction"><span class="toc-text">1. KeyedProcessFunction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-TimerService-%E5%92%8C-%E5%AE%9A%E6%97%B6%E5%99%A8%EF%BC%88Timers%EF%BC%89"><span class="toc-text">2. TimerService 和 定时器（Timers）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E4%BE%A7%E8%BE%93%E5%87%BA%E6%B5%81%EF%BC%88SideOutput%EF%BC%89"><span class="toc-text">3. 侧输出流（SideOutput）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-CoProcessFunction"><span class="toc-text">4. CoProcessFunction</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B9%9D%E3%80%81%E7%8A%B6%E6%80%81%E7%BC%96%E7%A8%8B%E5%92%8C%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6"><span class="toc-text">九、状态编程和容错机制</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%9C%89%E7%8A%B6%E6%80%81%E7%9A%84%E7%AE%97%E5%AD%90%E5%92%8C%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F"><span class="toc-text">1. 有状态的算子和应用程序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E7%AE%97%E5%AD%90%E7%8A%B6%E6%80%81%EF%BC%88operator-state%EF%BC%89"><span class="toc-text">1.1 算子状态（operator state）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E9%94%AE%E6%8E%A7%E7%8A%B6%E6%80%81%EF%BC%88keyed-state%EF%BC%89"><span class="toc-text">1.2 键控状态（keyed state）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7"><span class="toc-text">2. 状态一致性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E4%B8%80%E8%87%B4%E6%80%A7%E7%BA%A7%E5%88%AB"><span class="toc-text">2.1 一致性级别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E7%AB%AF%E5%88%B0%E7%AB%AF%EF%BC%88end-to-end%EF%BC%89%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7"><span class="toc-text">2.2 端到端（end-to-end）状态一致性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%A3%80%E6%9F%A5%E7%82%B9%EF%BC%88checkpoint%EF%BC%89"><span class="toc-text">3. 检查点（checkpoint）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Flink%E7%9A%84%E6%A3%80%E6%9F%A5%E7%82%B9%E7%AE%97%E6%B3%95"><span class="toc-text">3.1 Flink的检查点算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Flink-Kafka-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84-exactly-once-%E8%AF%AD%E4%B9%89"><span class="toc-text">3.2 Flink+Kafka 如何实现端到端的 exactly-once 语义</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E9%80%89%E6%8B%A9%E4%B8%80%E4%B8%AA%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF%EF%BC%88state-backend%EF%BC%89"><span class="toc-text">4. 选择一个状态后端（state backend）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Savepoint"><span class="toc-text">4. Savepoint</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Savepoint%E4%BB%8B%E7%BB%8D"><span class="toc-text">4.1 Savepoint介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Savepoint-VS-Checkpoint"><span class="toc-text">4.2 Savepoint VS Checkpoint</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Savepoint%E6%BC%94%E7%A4%BA"><span class="toc-text">4.3 Savepoint演示</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%85%B3%E4%BA%8E%E5%B9%B6%E8%A1%8C%E5%BA%A6"><span class="toc-text">5. 关于并行度</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E3%80%81Table-API-%E4%B8%8ESQL"><span class="toc-text">十、Table API 与SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Table-API-SQL%E7%9A%84%E7%89%B9%E7%82%B9"><span class="toc-text">1. Table API &amp; SQL的特点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Table-API-SQL%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B"><span class="toc-text">2. Table API &amp; SQL发展历程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%9E%B6%E6%9E%84%E5%8D%87%E7%BA%A7"><span class="toc-text">2.1 架构升级</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E5%99%A8%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-text">2.2 查询处理器的选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%B3%A8%E6%84%8F"><span class="toc-text">2.3 注意</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E9%9C%80%E8%A6%81%E5%BC%95%E5%85%A5%E7%9A%84pom%E4%BE%9D%E8%B5%96"><span class="toc-text">3. 需要引入的pom依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E7%AE%80%E5%8D%95%E4%BA%86%E8%A7%A3TableAPI"><span class="toc-text">4. 简单了解TableAPI</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%8A%A8%E6%80%81%E8%A1%A8"><span class="toc-text">4.1 动态表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%AD%97%E6%AE%B5"><span class="toc-text">4.2 字段</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-TableAPI-%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E6%93%8D%E4%BD%9C"><span class="toc-text">5. TableAPI 的窗口聚合操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA"><span class="toc-text">5.1 案例演示</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E5%85%B3%E4%BA%8E-group-by"><span class="toc-text">5.2 关于 group by</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E5%85%B3%E4%BA%8E%E6%97%B6%E9%97%B4%E7%AA%97%E5%8F%A3"><span class="toc-text">5.3 关于时间窗口</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-SQL-%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99"><span class="toc-text">6. SQL 如何编写</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5"><span class="toc-text">7. 相关概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-Dynamic-Tables-Continuous-Queries"><span class="toc-text">7.1 Dynamic Tables &amp; Continuous Queries</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-Table-to-Stream-Conversion"><span class="toc-text">7.2 Table to Stream Conversion</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-1-%E8%A1%A8%E4%B8%AD%E7%9A%84Update%E5%92%8CDelete"><span class="toc-text">7.2.1 表中的Update和Delete</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-2-%E5%AF%B9%E8%A1%A8%E7%9A%84%E7%BC%96%E7%A0%81%E6%93%8D%E4%BD%9C"><span class="toc-text">7.2.2 对表的编码操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-3-%E5%B0%86%E8%A1%A8%E8%BD%AC%E6%8D%A2%E4%B8%BA%E4%B8%89%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%BC%96%E7%A0%81%E6%96%B9%E5%BC%8F%E7%9A%84%E6%B5%81"><span class="toc-text">7.2.3 将表转换为三种不同编码方式的流</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E4%B8%80%E3%80%81Flink-CEP-%E7%AE%80%E4%BB%8B"><span class="toc-text">十一、Flink CEP 简介</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%8D%E6%9D%82%E4%BA%8B%E4%BB%B6%E5%A4%84%E7%90%86CEP"><span class="toc-text">1.  什么是复杂事件处理CEP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Flink-CEP"><span class="toc-text">2. Flink CEP</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Event-Streams"><span class="toc-text">2.1 Event Streams</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Pattern-API"><span class="toc-text">2.2 Pattern API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Pattern-%E6%A3%80%E6%B5%8B"><span class="toc-text">2.3 Pattern 检测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-select"><span class="toc-text">2.4 select</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-flatSelect"><span class="toc-text">2.5 flatSelect</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-%E8%B6%85%E6%97%B6%E4%BA%8B%E4%BB%B6%E7%9A%84%E5%A4%84%E7%90%86"><span class="toc-text">2.6 超时事件的处理</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/08/30/reptile_1/" title="Python 爬虫基础之 urllib 库的深入使用详解"><img src="/img/reptile/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python 爬虫基础之 urllib 库的深入使用详解"/></a><div class="content"><a class="title" href="/2022/08/30/reptile_1/" title="Python 爬虫基础之 urllib 库的深入使用详解">Python 爬虫基础之 urllib 库的深入使用详解</a><time datetime="2022-08-30T09:15:02.000Z" title="发表于 2022-08-30 17:15:02">2022-08-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/24/redis_6/" title="Redis 命令详解"><img src="/img/redis/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Redis 命令详解"/></a><div class="content"><a class="title" href="/2021/06/24/redis_6/" title="Redis 命令详解">Redis 命令详解</a><time datetime="2021-06-24T10:23:02.000Z" title="发表于 2021-06-24 18:23:02">2021-06-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/24/redis_5/" title="redis-benchmark 性能测试详解"><img src="/img/redis/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="redis-benchmark 性能测试详解"/></a><div class="content"><a class="title" href="/2021/06/24/redis_5/" title="redis-benchmark 性能测试详解">redis-benchmark 性能测试详解</a><time datetime="2021-06-24T08:41:09.000Z" title="发表于 2021-06-24 16:41:09">2021-06-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/24/python_7/" title="python3 pip安装第三方库失败，WARNING: You are using pip version 20.2.3； however, version 21.1.2 is available."><img src="/img/python/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="python3 pip安装第三方库失败，WARNING: You are using pip version 20.2.3； however, version 21.1.2 is available."/></a><div class="content"><a class="title" href="/2021/06/24/python_7/" title="python3 pip安装第三方库失败，WARNING: You are using pip version 20.2.3； however, version 21.1.2 is available.">python3 pip安装第三方库失败，WARNING: You are using pip version 20.2.3； however, version 21.1.2 is available.</a><time datetime="2021-06-24T04:36:36.000Z" title="发表于 2021-06-24 12:36:36">2021-06-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/21/redis_4/" title="Redis 的简单学习"><img src="/img/redis/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Redis 的简单学习"/></a><div class="content"><a class="title" href="/2021/06/21/redis_4/" title="Redis 的简单学习">Redis 的简单学习</a><time datetime="2021-06-21T07:45:12.000Z" title="发表于 2021-06-21 15:45:12">2021-06-21</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/flink/1.png')"><div id="footer-wrap"><div class="copyright">&copy;2022 By 一位木带感情的码农</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'https://program-park.github.io/2021/05/19/flink_1/'
    this.page.identifier = '/2021/05/19/flink_1/'
    this.page.title = 'Flink：从入门到放弃'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }

  document.getElementById('darkmode').addEventListener('click', () => {
    setTimeout(() => window.disqusReset(), 200)
  })
}

if ('Valine' === 'Disqus' || !false) {
  if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>